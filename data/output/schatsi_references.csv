filename;raw reference string
05_56.pdf;"
references 
amodei., d., olah, c., steinhardt, j., christiano, p., schulman, j., mane, d. (2016). concrete 
problems in ai safety. ai, 1-29. 
bostrom, n. (2014). superintelligence: paths, dangers, strategies. oxford university press, inc. 
new york, ny, usa 
bostrom, n. (2017). strategic implications of openness in ai development. global policy, 8:2, 
135-148. 
bonabeau e. (2002). agent-based modeling: methods and techniques for simulating human 
systems. proc natl acad sci, 99:3, 7280-87. 
ergonomics & human factors 2021, eds r charles & d golightly, ciehf 
 
dallat, c., salmon, p. m., & goode, n. (2018). identifying risks and emergent risks across 
sociotechnical systems: the networked hazard analysis and risk management system 
(net-harms). theoretical issues in ergonomics science, 19(4), 456-482. 
everitt, t., lea, g., hutter, m. (2018). agi safety literature review. ijcai. arxiv: 1805.01109. 
gareth, j. (2013). an introduction to statistical learning. springer: new york. 
gkikas, n. (2019) allocation of function in the era of artificial intelligence: a 60 year old 
paradigm challenged. contemporary ergonomics 2019. taylor and francis: london.  
goodfelow, i., bengio, y., and courville, a,. (2016). mit press: boston, ma. 
gurkaynak, g., yilmaz, i., haksever, g. (2016). stifling ai: human perils. computer law and 
security review, 32:5, 749-758 
kaplan, a., haenlein, m. (2018). siri, siri, in my hand: who’s the fairest in the land? on the 
interpretations, illustrations, and implications of artificial intelligence. business horizons, 
62:1, 15-25 
leveson, n. g. (2004). a new accident model for engineering safer systems. safety science, 42:4, 
pp. 237—270. 
müller, v. c., bostrom, n. (2016), ‘future progress in artificial intelligence: a survey of expert 
opinion’, in vincent c. müller (ed.), fundamental issues of artificial intelligence (synthese 
library; berlin: springer), 553-571. 
omohundro, s. (2014) autonomous technology and the greater human good, journal of 
experimental & theoretical artificial intelligence, 26:3, 303-315. 
pennachin, c., goertzel, b. (2007). contemporary approaches to artificial general intelligence. in 
b. goertzel & c. pennachin (eds.), artificial general intelligence, springer. 
steinhardt, j. (2015). long-term and short-term challenges to ensuring the safety of ai systems. 
https://jsteinhardt.wordpress.com/2015/06/24/long-term-and-short-term-challenges-to-
ensuring-the-safety-of-ai-systems/ 
 
 
"
1-s2.0-S1566253521002414-main.pdf;"
references
[1] m. gaur, k. faldu, a. sheth, semantics of the black-box: can knowledge graphs
help make deep learning systems more interpretable and explainable? 2020,
arxiv preprint arxiv:2010.08660.
[2] c. panigutti, a. perotti, d. pedreschi, doctor xai: an ontology-based approach
to black-box sequential data classification explanations, in: proceedings of the
2020 conference on fairness, accountability, and transparency, 2020, pp.
629–639.
[3] j.m. rožanec, b. kažič, m. škrjanc, b. fortuna, d. mladenić, automotive
oem demand forecasting: a comparative study of forecasting algorithms and
strategies, appl. sci. 11 (2021) 6787.
[4] f. lolli, r. gamberini, a. regattieri, e. balugani, t. gatos, s. gucci, single-
hidden layer neural networks for forecasting intermittent demand, int. j. prod.
econ. 183 (2017) 116–128.
[5] j.j. bergman, j.s. noble, r.g. mcgarvey, r.l. bradley, a bayesian approach
to demand forecasting for new equipment programs, robot. comput.-integr.
manuf. 47 (2017) 17–21.
[6] m. babai, a. tsadiras, c. papadopoulos, on the empirical performance of
some new neural network methods for forecasting intermittent demand, ima
j. manag. math. (2020).
[7] t. benbarrad, m. salhaoui, s.b. kenitar, m. arioua, intelligent machine vision
model for defective product inspection based on machine learning, j. sensor
actuator netw. 10 (2021) 7.
[8] r. rajkumar, i. lee, l. sha, j. stankovic, cyber-physical systems: the next
computing revolution, in: design automation conference, ieee, 2010, pp.
731–736.
[9] m.w. grieves, virtually intelligent product systems: digital and physical twins,
2019.
[10] p. tubaro, a.a. casilli, micro-work, artificial intelligence and the automotive
industry, j. ind. bus. econ. 46 (2019) 333–345.
[11] r.d. raut, a. gotmare, b.e. narkhede, u.h. govindarajan, s.u. bokade, en-
abling technologies for industry 4.0 manufacturing and supply chain: concepts,
current status, and adoption challenges, ieee eng. manag. rev. 48 (2020)
83–102.
[12] f. tao, q. qi, a. liu, a. kusiak, data-driven smart manufacturing, j. manuf.
syst. 48 (2018) 157–169.
[13] b. brühl, m. hülsmann, d. borscheid, c.m. friedrich, d. reith, a sales forecast
model for the german automobile market based on time series analysis and data
mining methods, in: industrial conference on data mining, springer, 2009, pp.
146–160.
[14] f.-k. wang, k.-k. chang, c.-w. tzeng, using adaptive network-based fuzzy
inference system to forecast automobile sales, expert syst. appl. 38 (2011)
10587–10593.
[15] a. vahabi, s.s. hosseininia, m. alborzi, a sales forecasting model in automo-
tive industry using adaptive neuro-fuzzy inference system (anfis) and genetic
algorithm (ga), management 1 (2016) 2.
[16] j. gao, y. xie, x. cui, h. yu, f. gu, chinese automobile sales forecast-
ing using economic indicators and typical domestic brand automobile sales
data: a method based on econometric model, adv. mech. eng. 10 (2018)
1687814017749325.
[17] n.z. ubaidillah, a study of car demand and its interdependency in sarawak,
int. j. bus. soc. 21 (2020).
[18] t. williams, stock control with sporadic and slow-moving demand, j. oper.
res. soc. 35 (1984) 939–948.
[19] f. johnston, j.e. boylan, forecasting for items with intermittent demand, j.
oper. res. soc. 47 (1996) 113–121.
[20] a.a. syntetos, j.e. boylan, j. croston, on the categorization of demand patterns,
j. oper. res. soc. 56 (2005) 495–503.
[21] a.a. syntetos, m.z. babai, n. altay, on the demand distributions of spare parts,
int. j. prod. res. 50 (2012) 2101–2117.
information fusion 81 (2022) 91–102
101
j.m. rožanec et al.
[22] d. lengu, a.a. syntetos, m.z. babai, spare parts management: linking distribu-
tional assumptions to demand classification, european j. oper. res. 235 (2014)
624–635.
[23] r. saluja, a. malhi, s. knapič, k. främling, c. cavdar, towards a rigorous
evaluation of explainability for multivariate time series, 2021, arxiv preprint
arxiv:2104.04075.
[24] a. dwivedi, m. niranjan, k. sahu, a business intelligence technique for
forecasting the automobile sales using adaptive intelligent systems (anfis and
ann), int. j. comput. appl. 74 (2013).
[25] x. wang, d. zeng, h. dai, y. zhu, making the right business decision:
forecasting the binary npd strategy in chinese automotive industry with
machine learning methods, technol. forecast. soc. change 155 (2020) 120032.
[26] d.s. farahani, m. momeni, n.s. amiri, car sales forecasting using artificial
neural networks and analytical hierarchy process, data analytics 2016
(2016) 69.
[27] k.k. chandriah, r.v. naraganahalli, rnn/lstm with modified adam optimizer
in deep learning approach for automobile spare parts demand forecasting,
multimedia tools appl. (2021) 1–15.
[28] r. fildes, p. goodwin, stability in the inefficient use of forecasting systems: a
case study in a supply chain company, int. j. forecast. 37 (2021) 1031–1046.
[29] p. corredor, e. ferrer, r. santamaria, is cognitive bias really present in analyst
forecasts? the role of investor sentiment, int. bus. rev. 23 (2014) 824–837.
[30] r.m. hogarth, s. makridakis, forecasting and planning: an evaluation, manage.
sci. 27 (1981) 115–138.
[31] j.h. barnes jr., cognitive biases and their impact on strategic planning, strat.
manag. j. 5 (1984) 129–137.
[32] a.b. arrieta, n. díaz-rodríguez, j. del ser, a. bennetot, s. tabik, a. barbado,
s. garcía, s. gil-lópez, d. molina, r. benjamins, et al., explainable artificial
intelligence (xai): concepts, taxonomies, opportunities and challenges toward
responsible ai, inf. fusion 58 (2020) 82–115.
[33] o. biran, c. cotton, explanation and justification in machine learning: a survey,
in: ijcai-17 workshop on explainable ai (xai), vol. 8, 2017, pp. 8–13.
[34] a. malhi, s. knapic, k. främling, explainable agents for less bias in human-
agent decision making, in: international workshop on explainable, transparent
autonomous agents and multi-agent systems, springer, 2020, pp. 129–146.
[35] x. wang, m. yin, are explanations helpful? a comparative study of the effects of
explanations in ai-assisted decision-making, in: 26th international conference
on intelligent user interfaces, 2021, pp. 318–328.
[36] j.j. ferreira, m. monteiro, the human-ai relationship in decision-making: ai
explanation to support people on justifying their decisions, 2021, arxiv preprint
arxiv:2102.05460.
[37] o. biran, k.r. mckeown, human-centric justification of machine learning
predictions, in: ijcai, vol. 2017, 2017, pp. 1461–1467.
[38] a. adadi, m. berrada, peeking inside the black-box: a survey on explainable
artificial intelligence (xai), ieee access 6 (2018) 52138–52160.
[39] o. loyola-gonzalez, black-box vs. white-box: understanding their advantages
and weaknesses from a practical point of view, ieee access 7 (2019)
154096–154113.
[40] a. das, p. rad, opportunities and challenges in explainable artificial intelligence
(xai): a survey, 2020, arxiv preprint arxiv:2006.11371.
[41] q.v. liao, d. gruen, s. miller, questioning the ai: informing design practices
for explainable ai user experiences, in: proceedings of the 2020 chi conference
on human factors in computing systems, 2020, pp. 1–15.
[42] p.p. angelov, e.a. soares, r. jiang, n.i. arnold, p.m. atkinson, explainable
artificial intelligence: an analytical review, wiley interdisciplinary reviews:
data mining and knowledge discovery 11 (2021) e1424.
[43] t. rojat, r. puget, d. filliat, j. del ser, r. gelin, n. díaz-rodríguez, explainable
artificial intelligence (xai) on timeseries data: a survey, 2021, arxiv preprint
arxiv:2104.00950.
[44] s.a. siddiqui, d. mercier, m. munir, a. dengel, s. ahmed, tsviz: demystifi-
cation of deep learning models for time-series analysis, ieee access 7 (2019)
67027–67040.
[45] k. kashiparekh, j. narwariya, p. malhotra, l. vig, g. shroff, convtimenet: a
pre-trained deep convolutional neural network for time series classification, in:
2019 international joint conference on neural networks (ijcnn), ieee, 2019,
pp. 1–8.
[46] s. tonekaboni, s. joshi, d. duvenaud, a. goldenberg, explaining time series
by counterfactuals, 2019.
[47] o. ozyegen, i. ilic, m. cevik, evaluation of local explanation methods for
multivariate time series forecasting, 2020, arxiv preprint arxiv:2009.09092.
[48] d. mercier, a. dengel, s. ahmed, p2exnet: patch-based prototype explana-
tion network, in: international conference on neural information processing,
springer, 2020, pp. 318–330.
[49] b. lim, s.o. arık, n. loeff, t. pfister, temporal fusion transformers for
interpretable multi-horizon time series forecasting, int. j. forecast. (2021).
[50] m.t. ribeiro, s. singh, c. guestrin, ‘‘why should i trust you?"" explaining
the predictions of any classifier, in: proceedings of the 22nd acm sigkdd
international conference on knowledge discovery and data mining, 2016, pp.
1135–1144.
[51] p. hall, n. gill, m. kurka, w. phan, machine learning interpretability with
h2o driverless ai, 2017, h2o. ai. url: http://docs.h2o.ai/driverless-ai/latest-
stable/docs/booklets/mlibooklet.pdf.
[52] m.r. zafar, n.m. khan, dlime: a deterministic local interpretable model-
agnostic explanations approach for computer-aided diagnosis systems, 2019,
arxiv preprint arxiv:1906.10263.
[53] k. sokol, p. flach, limetree: interactively customisable explanations based
on local surrogate multi-output regression trees, 2020, arxiv preprint arxiv:
2005.01427.
[54] m.t. ribeiro, s. singh, c. guestrin, anchors: high-precision model-agnostic
explanations, in: aaai, 18, 2018, pp. 1527–1535.
[55] j. van der waa, m. robeer, j. van diggelen, m. brinkhuis, m. neerincx,
contrastive explanations with local foil trees, 2018, arxiv preprint arxiv:1806.
07470.
[56] r. guidotti, a. monreale, s. ruggieri, d. pedreschi, f. turini, f. giannotti, local
rule-based explanations of black box decision systems, 2018, arxiv preprint
arxiv:1805.10820.
[57] j. rožanec, e. trajkova, k. kenda, b. fortuna, d. mladenić, explaining bad
forecasts in global time series models, 2021.
[58] w. samek, k.-r. müller, towards explainable artificial intelligence, in: explain-
able ai: interpreting, explaining and visualizing deep learning, springer, 2019,
pp. 5–22.
[59] r.
srinivasan,
a.
chander,
explanation
perspectives
from
the
cognitive
sciences-a survey, in: ijcai, 2020, pp. 4812–4818.
[60] d. pedreschi, f. giannotti, r. guidotti, a. monreale, l. pappalardo, s. ruggieri,
f. turini, open the black box data-driven explanation of black box decision
systems, 2018, arxiv preprint arxiv:1806.09936.
[61] s. verma, j. dickerson, k. hines, counterfactual explanations for machine
learning: a review, 2020, arxiv preprint arxiv:2010.10596.
[62] d. doran, s. schulz, t.r. besold, what does explainable ai really mean? a new
conceptualization of perspectives, 2017, arxiv preprint arxiv:1710.00794.
[63] m. chromik, human-centric explanation facilities, (ph.d. thesis), lmu, 2021.
[64] m. el-assady, w. jentner, r. kehlbeck, u. schlegel, r. sevastjanova, f. sperrle,
t. spinner, d. keim, towards xai: structuring the processes of explanations,
in: acm workshop on human-centered machine learning, 2019.
[65] m.t. keane, b. smyth, good counterfactuals and where to find them: a
case-based technique for generating counterfactuals for explainable ai (xai),
in: international conference on case-based reasoning, springer, 2020, pp.
163–178.
[66] m.t. keane, e.m. kenny, e. delaney, b. smyth, if only we had better
counterfactual explanations: five key deficits to rectify in the evaluation of
counterfactual xai techniques, 2021, arxiv preprint arxiv:2103.01035.
[67] f. ameri, d. dutta, an upper ontology for manufacturing service description,
in: international design engineering technical conferences and computers and
information in engineering conference, vol. 42578, 2006, pp. 651–661.
[68] s. lemaignan, a. siadat, j.-y. dantan, a. semenenko, mason: a proposal for an
ontology of manufacturing domain, in: ieee workshop on distributed intelligent
systems: collective intelligence and its applications (dis’06), ieee, 2006, pp.
195–200.
[69] m. courtot, f. gibson, a.l. lister, j. malone, d. schober, r.r. brinkman,
a. ruttenberg, mireot: the minimum information to reference an external
ontology term, appl. ontol. 6 (2011) 23–33.
[70] m. cannataro, c. comito, a data mining ontology for grid programming, in:
proc. 1st int. workshop on semantics in peer-to-peer and grid computing,
citeseer, 2003, pp. 113–134.
[71] p. panov, s. džeroski, l. soldatova, ontodm: an ontology of data mining, in:
2008 ieee international conference on data mining workshops, ieee, 2008,
pp. 752–760.
[72] p. panov, l. soldatova, s. džeroski, ontology of core data mining entities, data
min. knowl. discov. 28 (2014) 1222–1265.
[73] c. diamantini, d. potena, e. storti, kddonto: an ontology for discovery and
composition of kdd algorithms, in: third generation data mining: towards
service-oriented knowledge discovery (sokd’09), citeseer, 2009, pp. 13–24.
[74] v. ermolayev, n. keberle, w.-e. matzke, an ontology of environments, events,
and happenings, in: 2008 32nd annual ieee international computer software
and applications conference, ieee, 2008, pp. 539–546.
[75] s. gottschalk, e. demidova, eventkg: a multilingual event-centric temporal
knowledge graph, in: european semantic web conference, springer, 2018, pp.
272–287.
[76] r. mizoguchi, yamato: yet another more advanced top-level ontology, in:
proceedings of the sixth australasian ontology workshop, 2010, pp. 1–16.
[77] m. uschold, building ontologies: towards a uni ed methodology, in: proceedings
of 16th annual conference of the british computer society specialists group
on expert systems, citeseer, 1996.
[78] m. uschold, m. king, towards a methodology for building ontologies, citeseer,
1995.
[79] m.
fernández-lópez,
a.
gómez-pérez,
n.
juristo,
methontology:
from
ontological art towards ontological engineering, 1997.
information fusion 81 (2022) 91–102
102
j.m. rožanec et al.
[80] h.m. kim, m.s. fox, m. gruninger, an ontology of quality for enterprise
modelling, in: proceedings 4th ieee workshop on enabling technologies:
infrastructure for collaborative enterprises (wet ice’95), ieee, 1995, pp.
105–116.
[81] f. ameri, c. urbanovsky, c. mcarthur, a systematic approach to developing
ontologies for manufacturing service modeling, in: proceedings of the workshop
on ontology and semantic web for manufacturing, vol. 14, 2012.
[82] x. chang, r. rai, j. terpenny, development and utilization of ontologies in
design for manufacturing, j. mech. des. 132 (2010).
[83] r. weber, m. shrestha, a.j. johs, knowledge-based xai through cbr: there
is more to explanations than models can tell, 2021, arxiv preprint arxiv:
2108.10363.
[84] s.r. islama, w. eberleb, implications of combining domain knowledge in
explainable artificial intelligence, 2021.
[85] r. sharma, a.k. sinha, sales forecast of an automobile industry, int. j. comput.
appl. 53 (2012).
[86] d. salinas, v. flunkert, j. gasthaus, t. januschowski, deepar: probabilistic
forecasting with autoregressive recurrent networks, int. j. forecast. 36 (2020)
1181–1191.
[87] m. nentwig, m. hartung, a.-c. ngonga ngomo, e. rahm, a survey of current
link discovery frameworks, semantic web 8 (2017) 419–436.
[88] m.a. khoudja, m. fareh, h. bouarfa, ontology matching using neural networks:
survey and analysis, in: 2018 international conference on applied smart
systems (icass), ieee, 2018, pp. 1–6.
[89] j.p. mccrae, p. buitelaar, linking datasets using semantic textual similarity,
cybernet. inform. technol. 18 (2018) 109–123.
[90] e. thiéblin, o. haemmerlé, n. hernandez, c. trojahn, survey on complex
ontology matching, semantic web 11 (2020) 689–727.
[91] l.-w. chen, extending wikification: nominal discovery, nominal linking, and
the grounding of nouns, 2017.
[92] j. brank, g. leban, m. grobelnik, annotating documents with relevant
wikipedia concepts, in: proceedings of sikdd.
[93] r.c. fernandez, e. mansour, a.a. qahtan, a. elmagarmid, i. ilyas, s. madden,
m. ouzzani, m. stonebraker, n. tang, seeping semantics: linking datasets
using word embeddings for data discovery, in: 2018 ieee 34th international
conference on data engineering (icde), ieee, 2018, pp. 989–1000.
[94] r.j. miller, open data integration, proc. vldb endow. 11 (2018) 2130–2139.
[95] x.l. dong, t. rekatsinas, data integration and machine learning: a natural
synergy, in: proceedings of the 2018 international conference on management
of data, 2018, pp. 1645–1650.
[96] e. demidova, a. zaveri, e. simperl, matching domain and top-level ontologies
exploring word sense disambiguation and word embedding, in: emerging topics
in semantic technologies: iswc 2018 satellite events, vol. 36, ios press, 2018,
p. 27.
[97] a. assi, h. mcheick, a. karawash, w. dhifli, context-aware instance matching
through graph embedding in lexical semantic space, knowl.-based syst. 186
(2019) 104925.
[98] j.z.m. roˇ zanec, d. mladenić, ontology for contextualized demand forecasting
explanations, 2020, http://dx.doi.org/10.7910/dvn/gireay.
[99] p. leitão, f. restivo, adacor: a holonic architecture for agile and adaptive
manufacturing control, comput. ind. 57 (2006) 121–130.
[100] s. borgo, p. leitão, foundations for a core ontology of manufacturing, in:
ontologies, springer, 2007, pp. 751–775.
[101] g. kourtis, e. kavakli, r. sakellariou, a rule-based approach founded on
description logics for industry 4.0 smart factories, ieee trans. ind. inf. 15
(2019) 4888–4899.
[102] j.s. armstrong, v.g. morwitz, v. kumar, sales forecasts for existing consumer
products and services: do purchase intentions contribute to accuracy? int. j.
forecast. 16 (2000) 383–397.
[103] v.g. morwitz, j.h. steckel, a. gupta, when do purchase intentions predict
sales? int. j. forecast. 23 (2007) 347–364.
[104] h. drucker, c.j. burges, l. kaufman, a.j. smola, v. vapnik, support vector
regression machines, in: advances in neural information processing systems,
1997, pp. 155–161.
[105] m. stone, cross-validatory choice and assessment of statistical predictions, j.
r. stat. soc. ser. b stat. methodol. 36 (1974) 111–133.
[106] r.j. hyndman, et al., another look at forecast-accuracy metrics for intermittent
demand, int. j. appl. forecast. 4 (2006) 43–46.
[107] g. leban, b. fortuna, j. brank, m. grobelnik, event registry: learning about
world events from news, in: proceedings of the 23rd international conference
on world wide web, 2014, pp. 107–110.
[108] p.o. of the european union, eu open data portal: the official portal for
european data, 2020, https://data.europa.eu (accessed 15 december 2020).
[109] t. mikolov, i. sutskever, k. chen, g.s. corrado, j. dean, distributed represen-
tations of words and phrases and their compositionality, in: advances in neural
information processing systems, 2013, pp. 3111–3119.
[110] google, word2vec: an efficient implementation of the continuous bag-of-words
and skip-gram architectures for computing vector representations of words,
2015, https://code.google.com/archive/p/word2vec (accessed 15 december
2020).
[111] m. kusner, y. sun, n. kolkin, k. weinberger, from word embeddings to
document distances, in: international conference on machine learning, 2015,
pp. 957–966.
[112] n.f. noy, m. crubézy, r.w. fergerson, h. knublauch, s.w. tu, j. vendetti, m.a.
musen, protégé-2000: an open-source ontology-development and knowledge-
acquisition environment, in: amia... annual symposium proceedings. amia
symposium, 2003, p. 953.
[113] neo4j, neo4j: the graph database platform for today’s intelligent applications,
2020, https://neo4j.com (accessed 15 december 2020).
[114] j.z.m. roˇ zanec, d. mladenić, data for contextualized demand forecasting
explanations, 2020, http://dx.doi.org/10.7910/dvn/fpxycm.
[115] a. gudi, recognizing semantic features in faces using deep learning, 2015, arxiv
preprint arxiv:1512.00743.
[116] f.d. de souza, s. sarkar, g. cámara-chávez, building semantic understanding
beyond deep learning from sound and vision, in: 2016 23rd international
conference on pattern recognition (icpr), ieee, 2016, pp. 2097–2102.
[117] x. huang, c. zanni-merk, b. crémilleux, enhancing deep learning with seman-
tics: an application to manufacturing time series analysis, procedia comput. sci.
159 (2019) 437–446.
[118] e. coviello, a.b. chan, g. lanckriet, time series models for semantic music
annotation, ieee trans. audio speech lang. process. 19 (2010) 1343–1359.
[119] y. dong, h. su, j. zhu, b. zhang, improving interpretability of deep neural
networks with semantic information, in: proceedings of the ieee conference
on computer vision and pattern recognition, 2017, pp. 4306–4314.
"
1-s2.0-S2214317316301287-main.pdf;"
references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
1.
introduction
various initiatives in the domain of smart farming have been
documented; examples include smartagrifood,1 the dutch
smart dairy framing project,2 eu precision livestock farming
(eu-plf),3 and cow of the future.4 though particular objec-
tives vary, the overriding objective is that of efﬁciency. infor-
mation and communication technologies (icts) offers great
potential for improving efﬁciency, effectiveness and produc-
tivity; nonetheless, they remain underutilised in agriculture
[1]. small changes in production or efﬁciency can have a
major impact on proﬁtability [2]; from a sustainability per-
spective, this may, counter-intuitively perhaps, result in a
reduction in output. fundamental to efﬁciency is the effective
capture, processing, management and visualisation of many
heterogeneous
sources
of
information
so
as
to
enable
economically-viable
and
environment-friendly
decision-
making. enabling such decision-making is problematic, not
only as a consequence of the variety of information available
but also due to the dynamic nature of the many variables nec-
essary for strategic planning and optimal decision making.
ongoing developments in ictoffers signiﬁcant potential to
manage information at the farm level. sensing technologies,
at least in principle, offer farmers the ability to monitor their
farms with an unprecedented level of detail, in a multiplicity
of dimensions and in near real-time. this offers an intriguing
possibility of developing farm-speciﬁc models that the individ-
ual farmer can use to plan their activities in response to
changing circumstances, thus enabling the exploration of
the various trade-offs inherent in any decision-making pro-
cess whilst managing the information overload problem. for
the remainder of this paper, developments in modelling are
explored, and the technologies necessary to enable the con-
struction of farm-speciﬁc models considered.
2.
sustainable intensiﬁcation
reconciling sustainability with productivity, economic factors,
and environmental impact is a formidable challenge; nonethe-
less, maintaining current agricultural practices will have nega-
tive effects on global food production [3]. three theoretical
limits within which agriculture must operate include [4]:
1. quantity of food that can be produced within a given
climate;
2. quantity of food demanded by a growing economically
changing population, and
3. impact of food production on the environment.
ultimately, environmental impact depends on how global
agriculture expands in response to rising demand [5]. agri-
cultural intensiﬁcation has reduced the carbon footprint per
agricultural product; this process is expected to continue
[6]. sustainable agriculture seeks to maximize the net bene-
ﬁts
that
society
receives
from
agricultural
production,
demanding amongst others, major changes in livestock pro-
duction practices [7]. sustainable intensiﬁcation, a more
recent and ﬂuid construct, seeks to increase food production
while minimizing pressure on the environment, and is a
speciﬁc policy goal for certain institutions [8]. transition
towards more productive livestock production, in combina-
tion with other climate policies, for example, represents,
potentially, an effective mechanism for delivering desirable
climate and food-availability outcomes [9]. thus, from a prac-
tical livestock farming perspective, identiﬁcation of the most
efﬁcient animals and feed systems is a prerequisite for sus-
tainable livestock intensiﬁcation programs; system modelling
is viewed as a key enabling tool [10].
2.1.
agricultural domain modelling
modelling techniques have been harnessed in a wide variety
of
agricultural
domains
(fig.
1);
these
include
high-
resolution ﬁeld maps of soil properties [11], pasture growth
rate [12], greenhouse gas emissions [13] amongst others. for
animals, basic activity patterns can be quickly derived
through the use of gps-enabled collars [14,15]. more sophisti-
cated models by which to infer behaviour may then be con-
structed using a variety of machine learning techniques
fig. 1 – models have been developed for many dimensions
of the agricultural enterprise. incorporating pertinent
models whilst managing the trade-offs between complexity
and usability is a key challenge for enabling a smart farm.
1 http://smartagrifood.com/.
2 http://www.smartdairyfarming.nl/nl/.
3 http://www.eu-plf.eu/.
4 http://www.usdairy.com/sustainability/for-farmers.
180
i n f o r m a t i o n p r o c e s s i n g i n a g r i c u l t u r e 4 ( 2 0 1 7 ) 1 7 9 –1 8 7
[16,17]. such an approach has also been adopted in the case of
feeding behaviour [18]. multivariate continuous sensing for
behaviour and performance monitoring has enabled the con-
struction of models for detecting lameness [19]. in the dairy
sector, models have been developed for milk production fore-
casting [20], predictive feed intake both for total mixed ration
(tmr) [21] and for lactating dairy cows [22,23].
3.
models for the individual farm
research in farm simulation and modelling has evolved since
the mid-1990s to cover more than one on-farm enterprise;
examples include, gpfarm [24], grazplan [25], and ecomod
[26] (table 1). in the case of animal models, several important
gaps have been highlighted including the need for a more
mechanistic representation of the control of feed intake [27].
yet despite the beneﬁts that should accrue from harnessing
sophisticated simulation models, their efﬁcacy for everyday
farm management has proven to be somewhat limited [28].
reasons for this include, but are not limited to, complexity,
lack of time and a concern that there will be no increase in
proﬁt relative to the effort expended. when considering the
lack of take-up of modelling solutions on the farm, the case
of apsim [29] is probably archetypical. apsim is a well-
established platform and has been extensively documented
in the literature; yet it was necessary to simplify it for consul-
tants and farmers, resulting in a product called yield prophet
r.5 in practice, farmers tend to expect a ready-made solution.
however, models do not lend themselves to this; rather they
oblige the farmer, and others, to consider alternate production
strategies. this raises issues of model perception, understand-
ing and interaction, while always seeking to eliminate the
black box effect. as using models may be regarded as an exer-
cise in decision support or as an innovation process, it is nec-
essary that the three main components of such a process in
agricultural production systems, namely, biological processes,
farm management and advisory services, be facilitated [30].
though the concept of simulation models in agriculture
remains compelling, their uptake nonetheless remains disap-
pointing [31]. traditionally, models almost invariably focus on
one particular sub-domain; this may limit their effectiveness.
feed intake models in particular are most valuable when used
in
conjunction
with
other
models
that
predict
animal
responses in terms of milk yield, body weight change, nutri-
ent use efﬁciency and gaseous emissions [23].
4.
modelling feed intake
correlations between animal feed characteristics and intake
have been studied for decades. intake and digestibility have
been shown to account for the most variation in the produc-
tivity of dairy cows; as such, feed intake models have, and con-
tinue to be, the focus of intense research efforts. nrc
(national research council), norfor (nordic feed evaluation
system), and tdmi (total dmi index) are amongst the best
known for predicting dry matter intake (dmi) in dairy cows
fed via tmr; see [21] for an evaluation of each. relatively sim-
ple models such as nrc that only consider animal character-
istics and milk yield can be surprisingly effective when
compared to more sophisticated models; however, their
potential to incorporate additional factors such as diet charac-
teristics is limited [21]. low rumen ph has a detrimental effect
on dairy cows; it is inﬂuenced by diet, amongst other factors.
biopara-milk, is a whole cow model that simulates digestive
processes, predicting performance and circadian ph dynamics
[32]. halachmi et al. [33] have developed a feed intake model
for individual cows (it can also be used at a macro/herd level),
that incorporates feed behaviour in addition to milk yield and
live weight; however, it presupposes the availability of low-
cost feeding-behaviour sensors. clearly it is difﬁcult to effec-
tively and accurately sense feeding behaviour in a manner
that is not cost prohibitive and that is tolerant of a demanding
sensing context. comparing models is difﬁcult; tedeschi et al.
[34] have concluded that not all models are suitable for pre-
dicting milk production and that simpler systems may be
more tolerant of variation. furthermore, it was concluded that
the development of mathematical nutrition models is a pre-
requisite to correctly estimating the contribution of ruminants
to greenhouse gas emissions (ghg) emissions.
4.1.
reducing methane emissions
decreasing methane (ch4) emissions is necessary both envi-
ronmentally, as ch4 has a strong greenhouse gas effect, and
nutritionally, as ch4 represents a loss of feed energy [35].
the eu currently encourages voluntary compliance with
table 1 – exemplar models for farming enterprises.
model
geographic region
domain
gpfarm (great plains framework for
agricultural resource management)
north america
whole farm
grazplan
australia
grazing enterprises
ecomod
australia &
new zealand
pasture management
apsim (agricultural production systems simulator)
australia
crop modelling
nrc (national research council)
north america
nutrition (animal factors)
norfor (nordic feed evaluation system)
scandinavia
nutrition (animal and feed factors)
tdmi (total dmi index)
finland
nutrition – dry matter intake (animal
and feed factors)
biopara-milk
united kingdom
impact of feed on rumen ph in dairy cows
karoline
scandinavia
whole dairy cow models for nutrition,
milk production, digestion, and ch4 emissions
5 http://www.yieldprophet.com.au/yp/home.aspx
i n f o r m a t i o n p r o c e s s i n g i n a g r i c u l t u r e 4 ( 2 0 1 7 ) 1 7 9 –1 8 7
181
methane emission levels in the farm sector but moves are
afoot to include cattle livestock within the industrial emis-
sions directive (ied). within the eu there are approximately
90 million cattle and they collectively contribute some 41 per-
cent of eu ammonia emissions and 2 percent of methane
emissions. many factors inﬂuence ruminant ch4 production,
including level of intake, diet composition, quality of feeds,
energy consumption, animal size, growth rate, level of produc-
tion, and environmental parameters. broucek [36] suggests
that new approaches for measuring emissions from agricul-
ture are needed so as to establish typical emission ranges for
dairy (and beef) farms, and to measure the effect of manage-
ment factors on these emissions. it should be noted that when
dairy cows are fed the same diet at the same intake, variation
between cows in ch4 emissions can be substantial [37]. efforts
at modelling methane production of farms have been under-
taken. karoline is a whole dairy cow mechanistic, dynamic
model predicting nutrient supply and milk production; this
model has been recently revised in terms of its digestion and
ch4 emissions modules [38,35]. a model that predicts ch4
emissions from an on-farm database, based on intake,
digestibility and ndf, has been proposed in [39]; this model
improves the estimation of the methane emission factor from
both beef and dairy systems. grainger and beauchemin [40]
have demonstrated that dietary and farm management
options can be implemented to reduce ch4 emissions from
beef and dairy cattle without lowering production.
5.
technology-driven models for the smart
farm
a broad spectrum of computational intelligence techniques
has been harnessed for model development in agriculture.
in the case of cattle behaviour inference, hidden markov
models [41], regression trees [42] and support vector machi-
nes (svms) [43] are some examples. markov decision pro-
cesses (mdps) are frequently used for decision support [44].
machine learning algorithms have been harnessed for pre-
dicting conception success in dairy cows [45]. models based
on fuzzy logic have proven effective in detecting general
abnormal situations and giving forewarning of abnormalities
[46]. a key difﬁculty with many approaches to inferring beha-
viour activities is the need for a training dataset. mixture
models supporting unsupervised learning using probability
density functions have demonstrated the viability of real-
time and automatic monitoring of behaviour with high spa-
tial and temporal resolution [17]. in the dairy domain, a com-
parison
of
modelling
techniques
for
milk
production
forecasting identiﬁed a non-linear auto-regressive model as
being more accurate than conventional regression modelling
techniques [20]. though the subject of intense research effort,
nonetheless, there remains a need for analytic models that
are more accurate, robust, and crucially, more reusable [47].
feeding, in addition to standing and lying behaviour, is an
important indicator of the comfort and psychological status
of cows [48]; it has been demonstrated to be an important
indicator of the onset of oestrus [49]. rumination time is a
promising predictor of calving [50] and risk of disease in early
lactation
[51].
acoustic
monitoring
[52],
jaw
movement
augmented with sensor data [53], bite counters [54], noseband
pressure sensors [55] and electromyography (emg) [56] have
all been demonstrated as potential enablers of feed intake
measurement. in housing situations, computer vision-based
systems enable the automatic detection of feeding and stand-
ing behaviours [57]. a precision feeding system (concentrates)
for singular cows, using passive transponders for cow identi-
ﬁcation and rfid readers, has been developed [58]. though
viable, the cost effectiveness of such a system for the average
commercial dairy farm is questionable. cattle require opti-
mum nutrition and management during their life time,
demanding a coordinated approach between all stakeholders.
yet despite considerable research into the management and
nutrition of dairy cows, there remains much on-farm variabil-
ity in its application [59].
5.1.
sensing for the smart farm
animal modelling, machine learning and feed monitoring are
invariably underpinned by sensors and sensor networks.
sensing technologies has proven fundamental to precision
agriculture since its earliest days, evolving from ground-
based sensor deployments for speciality crops, for example,
citrus fruit production [60] to uavs for vineyards [61] to har-
nessing multi-purpose satellite systems to aid cotton farming
[62]. in the case of cattle health, helwatkar et al. [63] have
identiﬁed a number of common diseases in dairy cattle that
can be identiﬁed through the use of non-invasive, cheap sen-
sor technologies. more complex sensor platforms exist; cam-
era systems to detect back posture [64], and ingestible pills for
heart rate determination [65] for instance. however, rfid is
one of the most generic and popular technologies in multiple
agri-food domains, including dairy farms [66]. both caja et al.
[67] and rutten et al. [68] have considered the literature in
terms of the documented use of sensors for managing the
health of dairy herds. sensors that measure arbitrary aspects
of a cow, or summarise sensor data to provide information
(e.g., estrus), are predominant. cases where sensor informa-
tion is augmented with data from other sources so as to
enable decision-making were non-existent; this reinforces
observations made by other researchers. pesonen et al. [69]
observed that though there was (and indeed still is) much
information available from sensors and as a result of manual
record keeping, this information is not used as the time
incurred often outweighs the economic beneﬁts.
sensor networks, particularly wireless sensor networks
(wsns), have been widely deployed in agriculture [70,71]
and the food industry [72,73]. application domains include
crop management [74], phenotype measurement [75], rustling
prevention [76] and greenhouse management [77]. wireless
sensor
and
actuator
networks
(wsans)
are
receiving
increased attention in domains such as irrigation control
[78,79]. mobile sensing [80], usually realised through drones,
is still very much in its infancy. moosense [81] is a wsn that
incorporates both ground-based and animal-mounted sen-
sors that manages multiple animal parameters including
ambient environment parameters, and nutrient intake (cus-
tomised food auger and ﬂuid kiosk). gonza´lez et al. [14]
demonstrated the viability of a heterogeneous wsn to pro-
182
i n f o r m a t i o n p r o c e s s i n g i n a g r i c u l t u r e 4 ( 2 0 1 7 ) 1 7 9 –1 8 7
vide data in real-time so as to aid in the understanding of ani-
mal behaviour and enable effective herd management.
5.2.
enabling the smart farm
the smart farm [82] is predicated on large-scale heteroge-
neous sensing. heterogeneous sensors can measure a range
of modalities, and be instantiated in practice using platforms
from different manufacturers comprising different hardware,
protocols and algorithms. though of great potential, manag-
ing heterogeneity is challenging. a common approach is that
of middleware which offers sufﬁcient levels of abstractions
such that the heterogeneity in its various dimensions can
be mitigated and effectively managed [83–85]. sixth [86], a
distributed, intelligent middleware solution is an exemplar
of this genre of sensor network. global sensor network, an
open-source sensor middleware, has been demonstrated as
an enabler of the smart farm [87]. to ensure interoperability
and scalability, standards such as sensor web enablement
(swe),6 an open geospatial consortium (ogc)7 initiative,
must be adopted; this has been prototyped successfully in
multiple domains including precision farming [88]. such stan-
dards must co-exist with pre-existing agricultural standardis-
ation
activities
including
agroxml8
and
isoagrinet.9
deploying technologies on a farm can be problematic [89].
in the case of a cattle monitoring wsn, speciﬁc challenges
must be overcome [90]. these include the unforgiving nature
of the environment, namely animals in close proximity and,
in a rural context, mobility, radio interference caused by the
animal itself, data storage limitations and data transmission
difﬁculties. energy limitations are an omnipresent problem
[91] often thwarting network longevity; likewise cost becomes
a factor in remote monitoring scenarios [92].
5.3.
the internet of things (iots)
internet of things (iots) points to the promise of a framework
through which diverse data from the farm, including sensor
networks, can be captured and managed [93–96]. likewise, a
web of things (wots) approach has been demonstrated in an
experimental smart farm in australia [97]. both approaches,
iots and wots, mirror the relationship between the www
and the internet within the context of real-world objects.
developments in the iots and wots space cannot be trea-
ted in isolation from that of the internet itself. future internet
(fi) is seen as a mechanism through which a diverse series of
systems and services can be seamlessly integrated within an
arbitrary domain, including that of agriculture [98]. a prereq-
uisite for such systems will be an ability to:
1. gracefully handle ever-increasing and diverse real-time
data streams;
2. handle noisy, incomplete and sometimes contradictory
data;
3. capture, correlate and conﬂate data in real-time;
4. dynamically affect network behaviour to opportunistically
alter data capture, data routing or data recording regimes;
5. facilitate orchestrated sensing activity through the ability
of individual network nodes to reason, operate and collab-
orate within a collective, recognising the coexistence of
both individual and collective goals. this demands the
embracing of distributed intelligence and multi-agent
approaches [99–102].
though iots will be an indispensable technology for the
smart farm itself, their use in conjunction with fi offers a basis
for a new generation of farm management information sys-
tems [103–105] enabling smart farms become active nodes in
business to business (b2b) solutions and agricultural value
chains.
6.
future developments
though many of the technologies for delivering the smart farm
exist, their adoption by individual farmers and agricultural
enterprises depends on a number of additional factors. fore-
most amongst these are the issues of usability and the identi-
ﬁcation of best practice; such issues are also common to the
adoption of smart technologies in other domains. both farm
and farmer-centric approaches are needed; only in this way
will the smart farm concept prove sustainable going forward.
6.1.
user centricity
user-centered design envisages the end-user being integral to
the design, development and evaluation of innovative tech-
nologies. the rational for this is irrefutable; however, ensur-
ing this happens in practice can prove challenging, resulting
in products that fail to meet needs, and may even be deﬁcient
in terms of usability and functionality. technologies for the
smart farm are not immune to these kind of problems; effec-
tive solutions to such problems demands a detailed under-
standing of farmers’ needs, interactions and operational
contexts. this difﬁculty is accentuated when it is considered
that technologies fundamental to the smart farm may be
regarded as disruptive by the farming community; thus con-
ventional acceptance models, for example the technology
acceptance model (tam) may need revision. overcoming
such perceptions may indeed pose a grand challenge not just
for those actively involved in farming, but all stakeholders
including agribusiness and government.
given the indispensable nature of iots to the smart farm
enterprise, issues of business cases, sustainability and inno-
vation must be considered if acceptance is to be achieved.
one approach to this may be that of the living lab. such labs
seek to integrate research and innovation processes into real
communities, resulting in open, user-centered innovative
systems. they are characterised by co-creation, active user
involvement, real-life setting, multi-stakeholder participation
and multi-method approaches.10 uptake in the agri-food
6 http://www.opengeospatial.org/ogc/markets-technologies/
swe.
7 http://www.opengeospatial.org.
8 http://www.agroxml.de.
9 http://www.isoagrinet.org.
10 http://www.openlivinglabs.eu/.
i n f o r m a t i o n p r o c e s s i n g i n a g r i c u l t u r e 4 ( 2 0 1 7 ) 1 7 9 –1 8 7
183
domain has been limited to date; however, wolfert et al. have
utilised the construct in the dutch arable farming sector [106].
6.2.
sustainable decision-making
a key determinant in motivating the adoption of modelling
techniques as an indispensable tool for the management of
the smart farm will be the efﬁcacy of the resultant decision-
making. it has been suggested that the volume and hetero-
geneity of the data will demand a cloud-based analytics
approach to derive meaningful information [107]. web 2.0
and cloud computing have been successfully harnessed for
livestock management [108]; however, to gain a deeper under-
standing of the contextual relationships between the diverse
elements, an ontological approach is required [109]. this
necessitates an open standards-based approach such as that
proposed by chen et al. [110], for example. in so far as the
smart farm is viewed as a cyber-physical infrastructure for
precision farming, opportunities for autonomous decision-
making exist. however, to fully exploit the potential of new
technologies, it is necessary to design for human-in-the-
loop.
many
technologies
claim
to
be
human-centric;
nonetheless, in the case of control and decision making, the
human is often regarded as an external element. reconciling
the desire to reduce the burden on the user, in this case the
farmer, whilst ensuring the effectiveness of any necessary
decision-making present particular challenges. establishing
best practice principles, through the utilisation of individual
farms as living labs for example, offer one avenue through
which these challenges can be addressed going forward.
7.
conclusion
signiﬁcant research effort has been expended in the develop-
ment of models in the broad agricultural domain. applying
models on individual farms has been for the most part spo-
radic despite the potential advantages that could ensue.
why this is the case may be debated; however, the monolithic
nature of many models, lack of business cases for adopting
such models, as well as the difﬁculties that arise for individ-
ual farmers in applying such models in practice all contribute.
beneﬁts that could potentially accrue from smart farming are
multiple; how these might be realised within the dimensions
of productivity, proﬁtability and sustainability remains less
than clear. technologies underpinning smart farms offer an
opportunity for enabling the construction and application of
farm-speciﬁc models. this objective is attainable, and offers a
radical innovation for farming management practice.
r e f e r e n c e s
[1] milovanovic s. the role and potential of information
technology in agricultural improvement. ekonomika
poljoprivrede 2014;61(2):471.
[2] bewley jm, russell ra, dolecheck ka, borchers mr.
precision dairy monitoring: what have we learned? in:
halachmi i, editor. precision livestock farming applications -
making sense of sensors to support farm
management. wageningen: wageningen academic
publishers; 2015. p. 13–24.
[3] liu y, pan x, li j. current agricultural practices threaten
future global food production. j agric environ ethics 2015;28
(2):203–16.
[4] beddington jr, asaduzzaman m, clark me, bremauntz af,
guillou md, howlett dj, et al. what next for agriculture after
durban? science 2012;335(6066):289–90.
[5] tilman d, balzer c, hill j, befort bl. global food demand and
the sustainable intensiﬁcation of agriculture. proc natl acad
sci 2011;108(50):20260–4.
[6] bennetzen eh, smith p, porter jr. decoupling of greenhouse
gas emissions from global agricultural production:
1970–2050. glob change biol 2016;22(2):763–81.
[7] tilman d, cassman kg, matson pa, naylor r, polasky s.
agricultural sustainability and intensive production
practices. nature 2002;418(6898):671–7.
[8] garnett t, appleby mc, balmford a, bateman ij, benton tg,
bloomer p, et al. sustainable intensiﬁcation in
agriculture: premises and policies. science 2013;341
(6141):33–4.
[9] havlı´k p, valin h, herrero m, obersteiner m, schmid e,
ruﬁno mc, et al. climate change mitigation through
livestock system transitions. proc natl acad sci 2014;111
(10):3709–14.
[10] tedeschi lo, muir jp, riley dg, fox dg. the role of
ruminant animals in sustainable livestock intensiﬁcation
programs. int j sustain develop world ecol 2015;22
(5):452–65.
[11] knadel m, thomsen a, schelde k, greve mh. soil organic
carbon and particle sizes mapping using vis–nir, ec and
temperature mobile sensor platform. comp electron agric
2015;114:134–44.
[12] rahman mm, lamb dw, stanley jn, trotter mg. use of
proximal sensors to evaluate at the sub-paddock scale a
pasture growth-rate model based on light-use efﬁciency.
crop and pasture sci 2014;65(4):400–9.
[13] salcedo g. dairycant: a model for the reduction of dairy
farm greenhouse gas emissions. adv animal biosci 2015;6
(1):26.
[14] gonza´lez la, bishop-hurley g, henry d, charmley e.
wireless sensor networks to study, monitor and manage
cattle in grazing systems. anim prod sci 2014;54
(10):1687–93.
[15] umemura k. technical note: monitoring grazing bites and
walking activity with pedometers. j dairy sci 2013;96
(2):1090–3.
[16] smith d, dutta r, hellicar a, bishop-hurley g, rawnsley r,
henry d, et al. bag of class posteriors, a new multivariate
time series classiﬁer applied to animal behaviour
identiﬁcation. expert syst appl 2015;42(7):3774–84.
[17] gonza´lez la, bishop-hurley gj, handcock rn, crossman c.
behavioral classiﬁcation of data from collars containing
motion sensors in grazing cattle. comp electron agric
2015;110:91–102.
[18] bishop-hurley g, henry d, smith d, dutta r, hills j,
rawnsley r, et al. an investigation of cow feeding behavior
using motion sensors. in: proc. i2 mtc proceedings of the
2014 ieee international instrumentation and measurement
technology conference. montevideo, uruguay; 2014. p.
1285-90.
[19] van hertem t, maltz e, antler a, romanini ce, viazzi s, bahr
c, et al. lameness detection based on multivariate
continuous sensing of milk yield, rumination, and neck
activity. j dairy sci 2013;96(7):4286–98.
[20] murphy md, o’mahony mj, shalloo l, french p, upton j.
comparison of modelling techniques for milk-production
forecasting. j dairy sci 2014;97(6):3352–63.
184
i n f o r m a t i o n p r o c e s s i n g i n a g r i c u l t u r e 4 ( 2 0 1 7 ) 1 7 9 –1 8 7
[21] jensen lm, nielsen ni, nadeau e, markussen b, nørgaard p.
evaluation of ﬁve models predicting feed intake by
dairy cows fed total mixed rations. livestock sci
2015;176:91–103.
[22] krizsan sj, sairanen a, ho¨jer a, huhtanen p. evaluation of
different feed intake models for dairy cows. j dairy sci
2014;97(4):2387–97.
[23] zom rl, andre´ g, van vuuren am. development of a model
for the prediction of feed intake by dairy cows: 1. prediction
of feed intake. livestock science 2012;143(1):43–57.
[24] shaffer mj, bartling pn, ascough jc. object-oriented
simulation of integrated whole farms: gpfarm framework.
comp electron agric 2000;28(1):29–49.
[25] moore ad, donnelly jr, freer m. grazplan: decision
support systems for australian grazing enterprises. iii.
pasture growth and soil moisture submodels, and the
grassgro dss. agric syst 1997;55(4):535–82.
[26] johnson ir, chapman df, snow vo, eckard rj, parsons aj,
lambert mg, et al. dairymod and ecomod: biophysical
pasture-simulation models for australia and new zealand.
anim prod sci 2008;48(5):621–31.
[27] black jl. brief history and future of animal simulation
models for science and application. anim prod sci 2014;54
(12):1883–95.
[28] sempore aw, andrieu n, nacro hb, sedogo mp, le gal py.
relevancy and role of whole-farm models in supporting
smallholder farmers in planning their agricultural season.
environ modelling softw 2015;68:147–55.
[29] holzworth dp, huth ni, zurcher ej, herrmann ni, mclean g,
chenu k, et al. apsim–evolution towards a new generation
of agricultural systems simulation. environ modelling softw
2014;62:327–50.
[30] le gal py, dugue´ p, faure g, novak s. how does research
address the design of innovative agricultural production
systems at the farm level? a rev agric syst 2011;104
(9):714–28.
[31] mccown rl, carberry ps, hochman z, dalgliesh np, foale
ma. re-inventing model-based decision support with
australian dryland farmers. 1. changing intervention
concepts during 17 years of action research. crop and
pasture. science 2009;60(11):1017–30.
[32] ambriz-vilchis v, fawcett rh, shaw dj, macrae ai, jessop
ns. biopara-milk: a whole cow simulation model for the
prediction of rumen ph. in: halachmi i, editor. precision
livestock farming applications - making sense of sensors to
support farm management. wageningen: wageningen
academic publishers; 2015. p. 299–306.
[33] halachmi i, meir yb, miron j, maltz e. feeding behavior
improves prediction of dairy cow voluntary feed intake but
cannot serve as the sole indicator. animal 2016;10
(09):1501–6.
[34] tedeschi lo, herrero m, thornton pk. an overview of dairy
cattle models for predicting milk production: their
evolution, evaluation, and application for the agricultural
model intercomparison and improvement project (agmip)
for livestock. link: <https://cgspace.cgiar.org/rest/
bitstreams/40330/retrieve>.
[35] huhtanen p, ramin m, ude´n p. nordic dairy cow model
karoline in predicting methane emissions: 1. model
description and sensitivity analysis. livestock sci
2015;178:71–80.
[36] broucek j. production of methane emissions from
ruminant husbandry: a review. j environ protection 2014;5
(15):1482.
[37] bell mj, potterton sl, craigon j, saunders n, wilcox rh,
hunter m, et al. variation in enteric methane emissions
among cows on commercial dairy farms. animal 2014;8
(09):1540–6.
[38] ramin m, huhtanen p. nordic dairy cow model karoline in
predicting methane emissions: 2. model eval livestock sci
2015;178:81–93.
[39] jaurena g, cantet jm, arroquy ji, palladino ra,
wawrzkiewicz m, colombatto d. prediction of the ym factor
for livestock from on-farm accessible data. livestock sci
2015;177:52–62.
[40] grainger c, beauchemin ka. can enteric methane
emissions from ruminants be lowered without lowering
their production? anim feed sci technol 2011;166:308–20.
[41] guo y, poulton g, corke p, bishop-hurley gj, wark t, swain
dl. using accelerometer, high sample rate gps and
magnetometer data to develop a cattle movement and
behaviour model. ecol model 2009;220(17):2068–75.
[42] ungar ed, henkin z, gutman m, dolev a, genizi a,
ganskopp d. inference of animal activity from gps collar
data on free-ranging cattle. rangeland ecol manag 2005;58
(3):256–66.
[43] martiskainen p, ja¨rvinen m, sko¨n jp, tiirikainen j,
kolehmainen m, mononen j. cow behaviour pattern
recognition using a three-dimensional accelerometer and
support vector machines. appl anim behav sci 2009;119
(1):32–8.
[44] nielsen lr, kristensen ar. markov decision processes to
model livestock systems. in: pla`-aragone´s lm, editor.
handbook of operations research in agriculture and the
agri-food industry. new york: springer; 2015. p. 419–54.
[45] hempstalk k, mcparland s, berry dp. machine learning
algorithms for the prediction of conception success to a
given insemination in lactating dairy cows. j dairy sci
2015;98(8):5262–73.
[46] liberati p, zappavigna p. improving the automated
monitoring of dairy cows by integrating various data
acquisition systems. comp electron agric 2009;68(1):62–7.
[47] dutta r, smith d, rawnsley r, bishop-hurley g, hills j,
timms g, et al. dynamic cattle behavioural classiﬁcation
using supervised ensemble classiﬁers. comp electron agric
2015;111:18–28.
[48] bava l, tamburini a, penati c, riva e, mattachini g, provolo
g, et al. effects of feeding frequency and environmental
conditions on dry matter intake, milk yield and behaviour of
dairy cows milked in conventional or automatic milking
systems. italian j anim sci 2012;11(3):e42.
[49] halli k, koch c, romberg fj, hoy s. investigations on
automatically measured feed intake amount in dairy cows
during the oestrus period. archiv fuer tierzucht 2015;58
(1):93.
[50] clark ce, lyons na, millapan l, talukder s, cronin gm,
kerrisk kl, et al. rumination and activity levels as
predictors of calving for dairy cows. animal 2015;9
(04):691–5.
[51] calamari lu, soriani n, panella g, petrera f, minuti a, trevisi
er. rumination time around calving: an early signal to
detect cows at greater risk of disease. j dairy sci 2014;97
(6):3635–47.
[52] andriamandroso a, lebeau f, bindelle j. accurate
monitoring of the rumination behaviour of cattle using imu
signals from a mobile device. in: hopkins a, collins rp,
fraser md, king vr, lloyed dc, moorby jm, robson prh,
editors. grassland science in europe. wales: gomer press;
2014. p. 631–4.
[53] tani y, yokota y, yayota m, ohtani s. automatic recognition
and classiﬁcation of cattle chewing activity by an acoustic
monitoring method with a single-axis acceleration sensor.
comp electron agric 2013;92:54–65.
[54] umemura k, wanaka t, ueno t. technical note: estimation
of feed intake while grazing using a wireless system
requiring no halter. j dairy sci 2009;92(3):996–1000.
i n f o r m a t i o n p r o c e s s i n g i n a g r i c u l t u r e 4 ( 2 0 1 7 ) 1 7 9 –1 8 7
185
[55] braun u, zu¨ rcher s, ha¨ssig m. eating and rumination
activity in 10 cows over 10 days. res vet sci 2015;101:196–8.
[56] bu¨ chel s, sundrum a. technical note: evaluation of a new
system for measuring feeding behavior of dairy cows. comp
electron agric 2014;108:12–6.
[57] porto sm, arcidiacono c, anguzza u, cascone g. the
automatic detection of dairy cow feeding and standing
behaviours in free-stall barns by a computer vision-based
system. biosys eng 2015;133:46–55.
[58] yan y, wang r, song z, yan s. study on intelligent multi-
concentrates feeding system for dairy cow. in: li d, zhao c,
editors. int conf comp comput technol
agric. beijing: china; 2009. p. 275–82.
[59] logue dn, mayne cs. welfare-positive management and
nutrition for the dairy herd: a european perspective. vet j
2014;199(1):31–8.
[60] lee ws, ehsani r. sensing systems for precision agriculture
in florida. comp electron agric 2015;112:2–9.
[61] candiago s, remondino f, de giglio m, dubbini m, gattelli
m. evaluating multispectral images and vegetation indices
for precision farming applications from uav images.
remote sens 2015;7(4):4026–47.
[62] huang y, thomson sj. remote sensing for cotton farming.
cotton 2015;57:439–64.
[63] helwatkar ma, riordan d, walsh j. sensor technology for
animal health monitoring. in: icst 2014 proceedings of the
8th international conference on sensor technology.
liverpool, uk. p. 266–71.
[64] viazzi s, bahr c, van hertem t, schlageter-tello a, romanini
ce, halachmi i, et al. comparison of a three-dimensional
and two-dimensional camera system for automated
measurement of back posture in dairy cows. comp electron
agric 2014;100:139–47.
[65] warren s, martinez a, sobering t, andresen d.
electrocardiographic pill for cattle heart rate determination.
in: 30th annual international conference of the ieee
engineering in medicine and biology society. bc: vancouver;
2008. p. 4852–5.
[66] trevarthen a, michael k. the rfid-enabled dairy farm:
towards total farm management. in: proc. icmb’08
proceedings of the 7th international conference on mobile
business. spain: barcelona; 2008. p. 241–50.
[67] caja g, castro-costa a, knight ch. engineering to support
wellbeing of dairy animals. j dairy res 2016;83(02):136–47.
[68] rutten cj, velthuis ag, steeneveld w, hogeveen h. invited
review: sensors to support health management on dairy
farms. j dairy sci 2013;96(4):1928–52.
[69] pesonen l, sørensen cag, nikkila¨ r, rydberg a, green o.
information and communication technology in agriculture.
link: https://www.researchgate.net/publication/252326254_
njf_seminar_411_information_and_communication_
technology_in_agriculture.
[70] ojha t, misra s, raghuwanshi ns. wireless sensor networks
for agriculture: the state-of-the-art in practice and future
challenges. comp electron agric 2015;118:66–84.
[71] abbasi az, islam n, shaikh za. a review of wireless sensors
and networks’ applications in agriculture. comp standards
interfaces 2014;36(2):263–70.
[72] wang j, wang h, he j, li l, shen m, tan x, et al. wireless
sensor network for real-time perishable food supply chain
management. comp electron agric 2015;110:196–207.
[73] ruiz-garcia l, lunadei l, barreiro p, robla i. a review of
wireless sensor technologies and applications in agriculture
and food industry: state of the art and current trends.
sensors 2009;9(6):4728–50.
[74] juul jp, green o, jacobsen rh. deployment of wireless sensor
networks in crop storages. wireless pers commun 2015;81
(4):1437–54.
[75] greenwood pl, valencia p, overs l, paull dr, purvis iw. new
ways of measuring intake, efﬁciency and behaviour of
grazing livestock. anim prod sci 2014;54(10):1796–804.
[76] nkwari pk, rimer s, paul bs. cattle monitoring system using
wireless sensor network in order to prevent cattle rustling.
in: proceedings of the ist-africa conference and exhibition.
mauritius. p. 1–10.
[77] srbinovska m, gavrovski c, dimcev v, krkoleva a, borozan v.
environmental parameters monitoring in precision
agriculture using wireless sensor networks. j cleaner prod
2015;88:297–307.
[78] nikolidakis sa, kandris d, vergados dd, douligeris c.
energy efﬁcient automated control of irrigation in
agriculture by using wireless sensor networks. comp
electron agric 2015;113:154–63.
[79] chikankar pb, mehetre d, das s. an automatic irrigation
system using zigbee in wireless sensor network. in: proc.
icpc proceedings of the 2015 international conference on
pervasive computing (icpc)maharashtra, india. p. 1–5.
[80] zecha cw, link j, claupein w. mobile sensor platforms:
categorisation and research applications in precision
farming. j sens sens syst 2013;2(1):51–72.
[81] sarangi s, bisht a, rao v, kar s, mohanty tk, ruhil ap.
development of a wireless sensor network for animal
management: experiences with moosense. in: proc. ants
proceedings of the 2014 ieee international conference on
advanced networks and telecommunications systems.
new delhi, india. p. 1–6.
[82] wark t, corke p, sikka p, klingbeil l, guo y, crossman c,
et al. transforming agriculture through pervasive
wireless sensor networks. ieee pervasive comput 2007;6
(2):50–7.
[83] bhuyan b, sarma hk, sarma n. a survey on middleware for
wireless sensor networks. j wireless network commun
2014;4(1):7–17.
[84] lee j, park gl, kang mj, kwak hy, lee sj, han j. middleware
integration for ubiquitous sensor networks in agriculture.
in: proc. iccsa’12 proceedings of the 12th international
conference on computational science and its applications -
volume part iii. salvador de bahia, brazil. p. 217–22.
[85] wang mm, cao jn, li j, dasi sk. middleware for wireless
sensor networks: a survey. j comp sci technol 2008;23
(3):305–26.
[86] o’hare gmp, muldoon c, o’grady mj, collier rw, murdoch o,
carr d. sensor web interaction. int j artif intell tools 2012;21
(02):1240006.
[87] gaire r, lefort l, compton m, falzon g, lamb d, taylor k.
demonstration: semantic web enabled smart farm with
gsn. link: http://ceur-ws.org/vol-1035/iswc2013_demo_11.
pdf..
[88] geipel j, jackenkroll m, weis m, claupein w. a sensor web-
enabled infrastructure for precision farming. isprs int j
geo-inf 2015;4(1):385–99.
[89] banhazi t, vranken e, berckmans d, rooijakkers l,
berckmans d. word of caution for technology providers;
practical problems associated with large scale deployment
of plf-technologies on commercial farms. in: halachmi i,
editor. making sense of sensors to support farm
management. wageningen: wageningen academic
publishers; 2015. p. 2–10.
[90] kwong kh, wu tt, goh hg, sasloglou k, stephen b, glover i,
et al. practical considerations for wireless sensor networks
in cattle monitoring applications. comp electron agric
2012;81:33–44.
[91] anisi mh, abdul-salaam g, abdullah ah. a survey of
wireless sensor network approaches and their energy
consumption for monitoring farm ﬁelds in precision
agriculture. precision agric 2015;16(2):216–38.
186
i n f o r m a t i o n p r o c e s s i n g i n a g r i c u l t u r e 4 ( 2 0 1 7 ) 1 7 9 –1 8 7
[92] xu x, liang w, xu z. remote monitoring cost minimization
for an unreliable sensor network with guaranteed network
throughput. inf proc agric 2014;1(2):83–94.
[93] jayaraman pp, palmer d, zaslavsky a, salehi a,
georgakopoulos d. addressing information processing
needs of digital agriculture with openiot platform. in:
podnar zˇ arko i, pripuzˇic´ k, serrano m, editors. proceedings
of the international workshop, fp7 openiot project, held in
conjunction with softcom. split: croatia; 2015. p. 137–52.
[94] ma j, zhou x, li s, li z. connecting agriculture to the
internet of things through sensor networks. in: proc.
ithings/cpscom proceedings of the 2011 international
conference on internet of things and 4th international
conference on cyber, physical and social computing.
dalian, china. p. 84–7.
[95] yan-e d. design of intelligent agriculture management
information system based on iot. in: proc. icicta
proceedings of the 4th international conference on
intelligent computation technology and automation.
shenzhen, china. p. 1045–9.
[96] ilapakurti a, vuppalapati c. building an iot framework for
connected dairy. in: proc. bigdataservice 2015 proceedings
of the first international conference on big data computing
service and applications.san francisco, usa. p. 275–85.
[97] taylor k, grifﬁth c, lefort l, gaire r, compton m, wark t,
et al. farming the web of things. ieee intell syst 2013;28
(6):12–9.
[98] lehmann rj, reiche r, schiefer g. future internet and the
agri-food sector: state-of-the-art in literature and research.
comp electron agric 2012;89:158–74.
[99] tynan r, o’hare gmp, marsh d, o’kane d. interpolation for
wireless sensor network coverage. in: proc. emnets-ii
proceedings of the second ieee workshop on embedded
networked sensors.sydney, australia. p. 123–31.
[100] tynan r, marsh d, o’kane d, o’hare, gmp. intelligent agents
for wireless sensor networks. in: proc. aamas’05
proceedings of the fourth international joint conference on
autonomous agents and multiagent systems. utrecht, the
netherlands. p. 1179–80.
[101] muldoon c, o’hare gmp, o’grady mj, tynan r. agent
migration and communication in wsns. in: proceedings of
the ninth international conference on parallel and
distributed computing, applications and technologies.
dunedin, new zealand. p. 425–30.
[102] marsh d, tynan r, o’kane d, o’hare gmp. autonomic
wireless sensor networks. eng appl artif intell 2004;17
(7):741–8.
[103] barmpounakis s, kaloxylos a, groumas a, katsikas l, sarris
v, dimtsa k, et al. management and control applications in
agriculture domain via a future internet business-to-
business platform. inf proc agric 2015;2(1):51–63.
[104] fountas s, carli g, sørensen cg, tsiropoulos z, cavalaris c,
vatsanidou a, et al. farm management information
systems: current situation and future perspectives. comp
electron agric 2015;115:40–50.
[105] kaloxylos a, eigenmann r, teye f, politopoulou z, wolfert s,
shrank c, et al. farm management systems and the future
internet era. comp electron agric 2012;89:130–44.
[106] wolfert j, verdouw cn, verloop cm, beulens aj. organizing
information integration in agri-food—a method based on a
service-oriented architecture and living lab approach. comp
electron agric 2010;70(2):389–405.
[107] waga d, rabah k. environmental conditions’ big data
management and cloud computing analytics for sustainable
agriculture. world j comp appl technol 2014;2(3):73–81.
[108] teng cc, brown k, caro c, nielsen w, wells j. a service
oriented livestock management system using occasionally
connected mobile-cloud architecture. in: proc. syscon 2012
proceedings of the 2012 ieee international systems
conference. vancouver, canada. p. 1–5.
[109] sivamani s, park j, shin c, cho k, park d, cho y. towards an
intelligent livestock farm management using owl-based
ontology model. int j smart home 2015;9(4):251–66.
[110] chen n, zhang x, wang c. integrated open geospatial web
service enabled cyber-physical information infrastructure
for precision agriculture monitoring. comp electron agric
2015;111:78–91.
i n f o r m a t i o n p r o c e s s i n g i n a g r i c u l t u r e 4 ( 2 0 1 7 ) 1 7 9 –1 8 7
187
"
1480083.1480099.pdf;"
references 
1 w h ware 
the ultimate 
computer 
ieee spectrum march 1972 p 84 
2 c g bell 
r chen 
s rege 
effect of technology on near term computer 
structures 
ieee computer march/april 1972 p 29 
3 d j f a r b e r 
k 
larson 
the structure of a distributed computing system 
software 
proceedings of xxii polytechnic institute of 
brooklyn 
symposium april 1972 
4 m j flynn 
a podvin 
shared resource 
multiprocessing 
ieee computer march 1972 p 20 
5 j h barnes 
r m brown 
m kato 
d j kuck 
d l slotnick 
r a stokes 
the illiac 
iv 
computer 
ieee 
transactions 
on computers 
c-17 vol 8 p 
746 
august 1968 
6 s a holland 
c j 
purcell 
the cdc star-100: 
a large scale network oriented 
computer 
system 
proceedings of the ieee computer conference 
september 
1971 p 55 
7 c g bell 
a newell 
the pms and isp descriptive systems for computer 
structures 
sjcc 1970 p 351 
8 c g bell 
a newell 
computer 
structures 
mcgraw-hill 1971 
9 c g bell 
p freeman et al 
c.ai: a computing environment for ai research 
computer science department carnegie-mellon 
university 
pittsburgh pennsylvania may 1971 
10 l 
roberts 
data processing technology forecast 
advanced research projects agency april 1969 
11 w a wulf 
c g bell 
c.mmp: a 
multiminiprocessor 
this volume 
12 m barbacci 
h goldberg 
m 
knudsen 
c.ai(p.lisp)—a 
lisp 
processor for c.ai 
computer science department carnegie-mellon 
university 
pittsburgh pennsylvania may 1971 
13 d m c c r a c k e n 
g r o b e r t s o n 
c.ai(p.l*)—an 
l* processor for c.ai 
computer science department carnegie-mellon 
university 
pittsburgh pennsylvania may 1971 
"
15_1_581.pdf;"
reference 
architecture. software architecture for big 
data and the cloud (2017) pp. 49-68. 
https://doi.org/10.1016/b978-0-12-805467-
3.00004-1 
[19] r. mitchell, l. pottier, s. jacobs, r. f. da 
silva, m. rynge, k. vahi, & e. deelman, 
exploration 
of 
workflow 
management 
systems emerging features from users 
perspectives. proceedings 
- 2019 ieee 
international conference on big data, big data 
2019. 
https://doi.org/10.1109/bigdata47090.2019.90
05494 
[20] f. rouzbeh, p. griffin, a. grama, m. 
adibuzzaman, collaborative cloud computing 
framework for health data with open source 
technologies. 
https://doi.org/10.1145/3388440.3412460 
[21] google, tensorflow [cited 2020-12-25]. 
https://www.tensorflow.org/ 
[22] m. abdar, w. książek, u. r. acharya, r. s. 
tan, v. makarenkov, p. pławiak, a new 
machine learning technique for an accurate 
diagnosis of coronary artery disease. computer 
methods and programs in biomedicine 179 
(2019) 104992. 
https://doi.org/10.1016/j.cmpb.2019.104992 
 
 
this article is an open access article distributed under the terms and conditions of the creative 
commons attribution noncommercial (cc by-nc 4.0) license. 
"
1869750.pdf;"
references 
[1] 
“sage,” merriam webster online dictionary. accessed june 12, 2021. https://www.merriam-
webster.com/dictionary/sage  
[2] 
microsoft documentation. “comparing the azure ecosystem: differences between global 
azure, azure stack hub, and azure stack hci.” accessed march 2, 2022. 
https://docs.microsoft.com/en-us/azure-stack/operator/compare-azure-azure-stack 
[3] 
microsoft documentation. “compare azure government and global azure.” 
https://docs.microsoft.com/en-us/azure/azure-government/compare-azure-government-
global-azure. accessed april 25, 2022. 
 
 
58 
 
 
this page left blank 
 
 
 
59 
 
appendix a. 
knowledge functional and reynold’s number 
to help understand the genesis of the knowledge functional and our effort to qualify the state of a 
system, it is important to recognize the long history of our exemplar, the z pulsed power facility 
(z). it started as the particle beam fusion accelerator ii (pbfa-ii) in 1985, was upgraded to 
pbfa-z in 1996, and upgraded again in 2007 to z refurbished (zr). due to being an incredibly 
complex system and a one-of-a-kind facility, it is difficult to integrate modern technologies into z 
without affecting normal operations, and as such it was during those times of renovation when the 
most substantial changes were made. with nearly four decades of history and over one decade since 
the last facility upgrade, it was time to set forth on unified solutions. the current data landscape of 
z can thusly be summarized as disparate data silos with esoteric solutions and few unifying 
principles. 
in applying information theory to the data landscape at the z pulsed power facility, it can be seen 
that even though it is a complex system-of-systems, much of the data generated can be considered 
deterministic. while the data is deterministic, the further downstream it moves, the more it gets 
coupled with other data sources, the more it gets transformed, and the more it gets replicated. ergo, 
the data becomes more chaotic, and the entropy of the system increases. with this application of 
information theory, it is not surprising that we ended up drawing parallels to our previous 
experiences with fluid mechanics, specifically laminar and turbulent flows, to qualify the state of a 
given system. 
in fluid mechanics, laminar flow is characterized by being deterministic and reversible, whereas 
turbulent flow is deterministic and irreversible. while turbulent flow may be irreversible, it is still 
possible to predict its average behavior by averaging the fluid flow with varying initial conditions. 
with these characterizations in mind, we were able to easily draw comparisons of the current data 
landscape of the pulsed power facilities to turbulent flow; both are deterministic, but their associated 
chaos and ever-increasing entropy make them difficult to model. we are also able to see that a 
steady-state solution, akin to laminar flow, is our desired state, but in order to achieve such a state, 
we must be able to determine how far from it we are and measure the effects of our ontological 
efforts. 
drawing from fluid mechanics again, there is a nondimensional value/equation that can be 
leveraged to estimate whether a given fluid flow is either laminar or turbulent: 
𝑅𝑒 = 𝜌𝑢𝐿
𝜇  
this value is known as reynold’s number (𝑅𝑒) and it is a nondimensional value that relates a fluid’s 
density (𝜌), velocity (𝑢), characteristic length (𝐿), and dynamic viscosity (𝜇) to essentially estimate 
the chaos of a given flow; higher values represent more chaotic and therefore more turbulent fluid 
flows. it is important to note that reynold’s number is system dependent and not exact; this is not 
surprising as it is used to apply qualitative labels – laminar and turbulent flow – to quantitative data 
where there is no distinct value that represents the transition from laminar to turbulent flow. 
using reynold’s number as a basis, we have created our own analogue to measure the chaos of our 
data to help qualitatively evaluate the current state of the systems and to measure the efficacy of the 
knowledge functional in driving those systems towards a steady-state data landscape: 
𝐾𝐹(𝑛) ≈ 𝑑𝑐𝑎
𝑢  
 
60 
 
where (𝐾𝐹(𝑛)) represents the state of the knowledge functional, (𝑑) represents data sources, (𝑐) 
represents data connections, (𝑎) represents applications, and (𝑢) represents users. in our 
representation, the numerator represents pressure for the systems to change because more data 
sources indicate data silos, more data connections indicate inefficient pipelines, and more 
applications indicate esoteric and/or duplicate solutions. the denominator represents the resistance 
to change of the systems because the users are biggest barrier to change; the users instantiate an 
environment to fit their needs, even if it results in more data sources, more connections, and more 
applications that add unnecessary complexity. circling back to the knowledge functional, the end 
goal of this effort can be considered as reducing chaos in the z system – reducing reynold’s 
number.  
 
61 
 
appendix b. 
cultural identity 
when applying devops applied to data, an important understanding to remember are the unique 
roles that enable its success for development and implementation, such as: devops engineer, ml 
engineer, data engineer, mlops and dataops engineer, data scientist, data analyst. 
the unique roles have skills necessary to employ advanced methodologies and technologies for 
devops and data solutions. by better understanding the unique roles, the workforce can be better 
supported to transform by: hiring qualified persons with preexisting skills; and training existing 
members of the workforce to learn the required skills for the unique roles; the latter offers 
enhancement of career development that could support worker retention. 
the siloed roles of the devops engineer, ml engineer, data engineer, mlops and dataops 
engineer, data scientist, data analyst serve to describe the common compartmentalization of 
responsibility and individual knowledge in the mlops and dataops workflow. what these siloed 
roles enable is the specialization of important data-related tasks so collaboration of the roles can 
empower the path to data insight and improve time to knowledge. 
in practice, however, the expected responsibilities and technological ability described by these roles 
will often be either more generalized or more specified than can be described in general. depending 
upon human resources and individual skillsets of workers, the size of an organization's data team 
will vary. larger teams may have the ability to designate distinct roles for each team member and 
delegate specific responsibilities scoped to each role. however, the converse usually takes place for 
smaller teams where team members are designated many of the roles, so delegation of 
responsibilities are lumped together as well.  
devops engineers, ml engineers, data engineers, mlops and dataops engineers empower the 
capabilities of the data scientists and data analysts by abstracting many of the infrastructural 
processes and technological knowledge required to serve end solutions. in a siloed workplace, the 
data engineer and dataops engineer place attention on the management of the data lifecycle and 
commands the necessary technological knowledge of the acquisition, management, storage, and 
dissemination of data. data engineers and dataops engineers work with the devops engineers to 
implement necessary data solutions within desired technological ecosystems. devops engineers 
look to make the technological ecosystem supporting the data workflows and solutions with 
advanced computing technologies and methodologies. data analysts and data scientist utilize their 
statistical training and various technological tools to research the data solutions, extract insight from 
the data solutions, and communicate insights in an accessible manner. 
the interplay between ml engineers and mlops engineers parallels that between data engineers 
and dataops engineers. ml engineers leverage data and machine learning techniques to develop 
models that automate data-driven decisions. mlops engineers manage the continuous integration 
and deployment of the ml engineers’ models in the organization’s technological ecosystem for 
data scientists to discern insight from data and develop data models. 
devops engineer, ml engineer, data engineer, mlops and dataops engineer, data scientist, 
data analyst roles synergistically interplay to efficiently employ devops on data to increase time to 
knowledge. devops engineering enables and enhances ml engineering and data engineering by 
promoting mlops and dataops engineering; and all five enhance data science and data analytics. 
 
62 
 
distribution 
 
 
 
email—internal 
name 
org. 
sandia email address 
 
 
 
 
 
 
 
 
 
 
 
 
technical library 
1911 
sanddocs@sandia.gov 
 
 
email—external  
name 
company email address 
company name 
 
 
 
 
 
 
 
 
hardcopy—internal 
number of 
copies 
name 
org. 
mailstop 
 
 
 
 
 
 
 
 
 
 
hardcopy—external 
number of 
copies 
name 
company name and 
company mailing address 
 
 
 
 
 
 
 
 
 
63 
 
 
this page left blank 
 
 
 
sandia national laboratories 
is a multimission laboratory 
managed and operated by 
national technology & 
engineering solutions of 
sandia llc, a wholly owned 
subsidiary of honeywell 
international inc. for the u.s. 
department of energy’s 
national nuclear security 
administration under contract 
de-na0003525. 
 
"
2006.11371.pdf;"
references
[1] a. d. torres, h. yan, a. h. aboutalebi, a. das, l. duan, and p. rad,
“patient facial emotion recognition and sentiment analysis using
secure cloud with hardware acceleration,” comput. intell. multimed.
big data cloud with eng. appl., pp. 61–89, 2018.
[2] s. m. lee, j. b. seo, j. yun, y.-h. cho, j. vogel-claussen, m. l.
schiebler, w. b. gefter, e. j. van beek, j. m. goo, k. s. lee
et al., “deep learning applications in chest radiography and computed
tomography,” pp. 75–85, 2019.
[3] r. chen, l. yang, s. goodison, and y. sun, “deep-learning approach
to identifying cancer subtypes using high-dimensional genomic data,”
bioinformatics, vol. 36, no. 5, pp. 1476–1483, 2020.
[4] r. sayres, a. taly, e. rahimy, k. blumer, d. coz, n. hammel, j. krause,
a. narayanaswamy, z. rastegar, d. wu et al., “using a deep learning
algorithm and integrated gradients explanation to assist grading for
diabetic retinopathy,” ophthalmology, vol. 126, no. 4, pp. 552–564,
2019.
[5] a. das, p. rad, k. k. r. choo, b. nouhi, j. lish, and j. martel,
“distributed machine learning cloud teleophthalmology iot for predict-
ing amd disease progression,” future generation computer systems,
vol. 93, pp. 486–498, 2019.
[6] j. son, j. y. shin, h. d. kim, k.-h. jung, k. h. park, and s. j. park,
“development and validation of deep learning models for screening
multiple abnormal ﬁndings in retinal fundus images,” ophthalmology,
vol. 127, no. 1, pp. 85–94, 2020.
[7] n. mohammadian rad, s. m. kia, c. zarbo, t. van laarhoven,
g. jurman, p. venuti, e. marchiori, and c. furlanello, “deep learning
for automatic stereotypical motor movement detection using wearable
sensors in autism spectrum disorders,” signal processing, vol. 144, pp.
180–191, mar 2018.
[8] a. s. heinsfeld, a. r. franco, r. c. craddock, a. buchweitz, and
f. meneguzzi, “identiﬁcation of autism spectrum disorder using deep
learning and the abide dataset,” neuroimage: clinical, vol. 17, pp.
16–23, 2018.
[9] s. h. silva, a. alaeddini, and p. najaﬁrad, “temporal graph traversals
using reinforcement learning with proximal policy optimization,” ieee
access, vol. 8, pp. 63 910–63 922, 2020.
[10] c. you, j. lu, d. filev, and p. tsiotras, “advanced planning for
autonomous vehicles using reinforcement learning and deep inverse
reinforcement learning,” robotics and autonomous systems, vol. 114,
pp. 1–18, apr 2019.
[11] s. grigorescu, b. trasnea, t. cocias, and g. macesanu, “a survey of
deep learning techniques for autonomous driving,” journal of field
robotics, vol. 37, no. 3, pp. 362–386, 2020.
[12] d. feng, c. haase-schutz, l. rosenbaum, h. hertlein, c. glaser,
f. timm, w. wiesbeck, and k. dietmayer, “deep multi-modal
object detection and semantic segmentation for autonomous driving:
datasets, methods, and challenges,” ieee transactions on intelligent
transportation systems, pp. 1–20, 2020.
[13] a. sahba, a. das, p. rad, and m. jamshidi, “image graph production
by dense captioning,” in 2018 world autom. congr., vol. 2018-june.
ieee, jun 2018, pp. 1–5.
[14] n. bendre, n. ebadi, j. j. prevost, and p. najaﬁrad, “human action
performance using deep neuro-fuzzy recurrent attention model,” ieee
access, vol. 8, pp. 57 749–57 761, 2020.
[15] a. boles and p. rad, “voice biometrics: deep learning-based voiceprint
authentication system,” in 2017 12th system of systems engineering
conference (sose).
ieee, 2017, pp. 1–6.
[16] s. panwar, a. das, m. roopaei, and p. rad, “a deep learning approach
for mapping music genres,” in 2017 12th system of systems engineering
conference (sose).
ieee, 2017, pp. 1–5.
[17] g. d. l. t. parra, p. rad, k.-k. r. choo, and n. beebe, “detecting
internet of things attacks using distributed deep learning,” journal of
network and computer applications, p. 102662, 2020.
[18] h. chacon, s. silva, and p. rad, “deep learning poison data attack
detection,” in 2019 ieee 31st international conference on tools with
artiﬁcial intelligence (ictai).
ieee, 2019, pp. 971–978.
[19] a. kwasniewska, m. szankin, m. ozga, j. wolfe, a. das, a. zajac,
j. ruminski, and p. rad, “deep learning optimization for edge devices:
analysis of training quantization parameters,” iecon 2019 - 45th
annu. conf. ieee ind. electron. soc., pp. 96–101, oct 2019.
[20] c. zhang, p. patras, and h. haddadi, “deep learning in mobile and
wireless networking: a survey,” ieee communications surveys &
tutorials, vol. 21, no. 3, pp. 2224–2287, 2019.
[21] high level independent group on artiﬁcial intelligence (ai hleg),
“ethics guidelines for trustworthy ai,” euorpean comm., 2019.
22
[22] c. cath, s. wachter, b. mittelstadt, m. taddeo, and l. floridi, “artiﬁcial
intelligence and the good society: the us, eu, and uk approach,” science
and engineering ethics, vol. 24, no. 2, pp. 505–528, 2018.
[23] k. h. keskinbora, “medical ethics considerations on artiﬁcial intelli-
gence,” journal of clinical neuroscience, vol. 64, pp. 277–282, jun
2019.
[24] a. etzioni and o. etzioni, “incorporating ethics into artiﬁcial in-
telligence,” the journal of ethics, vol. 21, no. 4, pp. 403–418, dec
2017.
[25] n. bostrom and e. yudkowsky, “the ethics of artiﬁcial intelligence,”
in the cambridge handbook of artiﬁcial intelligence, k. frankish and
w. m. ramsey, eds.
cambridge: cambridge university press, 2014,
pp. 316–334.
[26] b. c. stahl and d. wright, “ethics and privacy in ai and big data:
implementing responsible research and innovation,” ieee security &
privacy, vol. 16, no. 3, pp. 26–33, 2018.
[27] d. s. weld and g. bansal, “the challenge of crafting intelligible
intelligence,” communications of the acm, vol. 62, no. 6, pp. 70–79,
may 2019.
[28] a. lui and g. w. lamb, “artiﬁcial intelligence and augmented
intelligence collaboration: regaining trust and conﬁdence in the ﬁnancial
sector,” information & communications technology law, vol. 27, no. 3,
pp. 267–283, sep 2018.
[29] m. hengstler, e. enkel, and s. duelli, “applied artiﬁcial intelligence and
trustthe case of autonomous vehicles and medical assistance devices,”
technological forecasting and social change, vol. 105, pp. 105–120,
apr 2016.
[30] l. chen, a. cruz, s. ramsey, c. j. dickson, j. s. duca, v. hornak,
d. r. koes, and t. kurtzman, “hidden bias in the dud-e dataset leads
to misleading performance of deep learning in structure-based virtual
screening,” plos one, vol. 14, no. 8, p. e0220113, 2019.
[31] r. challen, j. denny, m. pitt, l. gompels, t. edwards, and k. tsaneva-
atanasova, “artiﬁcial intelligence, bias and clinical safety,” bmj quality
& safety, vol. 28, no. 3, pp. 231–237, mar 2019.
[32] f. h. sinz, x. pitkow, j. reimer, m. bethge, and a. s. tolias,
“engineering a less artiﬁcial intelligence,” neuron, vol. 103, no. 6, pp.
967–979, sep 2019.
[33] o. osoba and w. welser, an intelligence in our image: the risks of
bias and errors in artiﬁcial intelligence.
rand corporation, 2017.
[34] a. kurakin, i. goodfellow, and s. bengio, “adversarial machine learn-
ing at scale,” 5th international conference on learning representations,
iclr 2017 - conference track proceedings, nov 2016.
[35] i. goodfellow, j. shlens, and c. szegedy, “explaining and harnessing
adversarial examples,” in 3rd international conference on learning
representations, iclr 2015 - conference track proceedings, 2015.
[36] j. su, d. v. vargas, and k. sakurai, “one pixel attack for fooling deep
neural networks,” ieee transactions on evolutionary computation,
vol. 23, no. 5, pp. 828–841, oct 2019.
[37] s. huang, n. papernot, i. goodfellow, y. duan, and p. abbeel,
“adversarial attacks on neural network policies,” 5th international
conference on learning representations, iclr 2017 - workshop track
proceedings, feb 2017.
[38] t. miller, “explanation in artiﬁcial intelligence: insights from the social
sciences,” pp. 1–38, 2019.
[39] k. sokol and p. flach, “explainability fact sheets: a framework for
systematic assessment of explainable approaches,” in fat* 2020 -
proceedings of the 2020 conference on fairness, accountability, and
transparency, 2020, pp. 56–67.
[40] c. rudin, “stop explaining black box machine learning models for high
stakes decisions and use interpretable models instead,” nature machine
intelligence, vol. 1, no. 5, pp. 206–215, may 2019.
[41] d. doran, s. schulz, and t. r. besold, “what does explainable ai really
mean? a new conceptualization of perspectives,” in ceur workshop
proceedings, 2018.
[42] d. castelvecchi, “can we open the black box of ai?” nature news, vol.
538, no. 7623, p. 20, 2016.
[43] a. adadi and m. berrada, “peeking inside the black-box: a survey
on explainable artiﬁcial intelligence (xai),” ieee access, vol. 6, pp.
52 138–52 160, 2018.
[44] f. k. dosilovic, m. brcic, and n. hlupic, “explainable artiﬁcial intelli-
gence: a survey,” in 2018 41st international convention on information
and communication technology, electronics and microelectronics,
mipro 2018 - proceedings, 2018.
[45] s. chakraborty, r. tomsett, r. raghavendra, d. harborne, m. alzantot,
f. cerutti, m. srivastava, a. preece, s. julier, r. m. rao, t. d.
kelley, d. braines, m. sensoy, c. j. willis, and p. gurram, “inter-
pretability of deep learning models: a survey of results,” in 2017
ieee smartworld, ubiquitous intelligence & computing, advanced &
trusted computed, scalable computing & communications, cloud &
big data computing, internet of people and smart city innovation
(smartworld/scalcom/uic/atc/cbdcom/iop/sci).
ieee, aug
2017, pp. 1–6.
[46] z. zhu, e. albadawy, a. saha, j. zhang, m. r. harowicz, and m. a.
mazurowski, “deep learning for identifying radiogenomic associations
in breast cancer,” computers in biology and medicine, vol. 109, pp.
85–90, 2019.
[47] d. van thiel and w. f. f. van raaij, “artiﬁcial intelligent credit risk
prediction: an empirical study of analytical artiﬁcial intelligence
tools for credit risk prediction in a digital era,” journal of accounting
and finance, vol. 19, no. 8, dec 2019.
[48] j. turiel and t. aste, “peer-to-peer loan acceptance and default
prediction with artiﬁcial intelligence,” royal society open science,
vol. 7, no. 6, p. 191649, 2020.
[49] s. lapuschkin, s. w¨aldchen, a. binder, g. montavon, w. samek, and
k.-r. m¨uller, “unmasking clever hans predictors and assessing what
machines really learn,” nature communications, vol. 10, no. 1, p. 1096,
dec 2019.
[50] r. agarwal, n. frosst, x. zhang, r. caruana, and g. e. hinton, “neural
additive models: interpretable machine learning with neural nets,” arxiv
preprint arxiv:2004.13912, 2020.
[51] y. goyal, u. shalit, and b. kim, “explaining classiﬁers with causal
concept effect (cace),” arxiv preprint arxiv:1907.07165, 2019.
[52] a. ghorbani, j. wexler, j. y. zou, and b. kim, “towards automatic
concept-based explanations,” in advances in neural information pro-
cessing systems, 2019, pp. 9273–9282.
[53] m. ibrahim, m. louie, c. modarres, and j. paisley, “global explana-
tions of neural networks: mapping the landscape of predictions,” in
proceedings of the 2019 aaai/acm conference on ai, ethics, and
society, 2019, pp. 279–287.
[54] h. li, y. tian, k. mueller, and x. chen, “beyond saliency: under-
standing convolutional neural networks from saliency prediction on
layer-wise relevance propagation,” image and vision computing, vol.
83-84, pp. 70–86, mar 2019.
[55] c. burns, j. thomason, and w. tansey, “interpreting black box models
via hypothesis testing,” arxiv preprint arxiv:1904.00045, mar 2019.
[56] a. chattopadhay, a. sarkar, p. howlader, and v. n. balasubramanian,
“grad-cam++: generalized gradient-based visual explanations for
deep convolutional networks,” in 2018 ieee winter conference on
applications of computer vision (wacv).
ieee, mar 2018, pp. 839–
847.
[57] v. petsiuk, a. das, and k. saenko, “rise: randomized input sam-
pling for explanation of black-box models,” british machine vision
conference 2018, bmvc 2018, jun 2018.
[58] b. kim, m. wattenberg, j. gilmer, c. cai, j. wexler, f. viegas, and
r. sayres, “interpretability beyond feature attribution: quantitative
testing with concept activation vectors (tcav),” in 35th international
conference on machine learning, icml 2018, 2018.
[59] m. sundararajan, a. taly, and q. yan, “axiomatic attribution for deep
networks,” in 34th international conference on machine learning,
icml 2017, 2017.
[60] m. ancona, e. ceolini, c. ¨oztireli, and m. gross, “towards better
understanding of gradient-based attribution methods for deep neural
networks,” 6th international conference on learning representations,
iclr 2018 - conference track proceedings, 2018.
[61] g. montavon, s. lapuschkin, a. binder, w. samek, and k.-r.
m¨uller, “explaining nonlinear classiﬁcation decisions with deep taylor
decomposition,” pattern recognition, vol. 65, pp. 211–222, 2017.
[62] l. m. zintgraf, t. s. cohen, t. adel, and m. welling, “visualizing
deep neural network decisions: prediction difference analysis,” 5th
international conference on learning representations, iclr 2017 -
conference track proceedings, feb 2017.
[63] r. r. selvaraju, m. cogswell, a. das, r. vedantam, d. parikh, and
d. batra, “grad-cam: visual explanations from deep networks via
gradient-based localization,” in proceedings of the ieee international
conference on computer vision, 2017.
[64] s. m. lundberg and s. i. lee, “a uniﬁed approach to interpreting model
predictions,” in advances in neural information processing systems,
2017, pp. 4765–4774.
[65] m. t. ribeiro, s. singh, and c. guestrin, “”why should i trust you?”,”
in proceedings of the 22nd acm sigkdd international conference
on knowledge discovery and data mining - kdd ’16.
new york,
new york, usa: acm press, 2016, pp. 1135–1144.
23
[66] b. zhou, a. khosla, a. lapedriza, a. oliva, and a. torralba, “learning
deep features for discriminative localization,” in 2016 ieee confer-
ence on computer vision and pattern recognition (cvpr).
ieee, jun
2016, pp. 2921–2929.
[67] j. t. springenberg, a. dosovitskiy, t. brox, and m. riedmiller,
“striving for simplicity: the all convolutional net,” in 3rd international
conference on learning representations, iclr 2015 - workshop track
proceedings, 2015.
[68] b. letham, c. rudin, t. h. mccormick, and d. madigan, “interpretable
classiﬁers using rules and bayesian analysis: building a better stroke
prediction model,” the annals of applied statistics, vol. 9, no. 3, pp.
1350–1371, sep 2015.
[69] s. bach, a. binder, g. montavon, f. klauschen, k.-r. m¨uller, and
w. samek, “on pixel-wise explanations for non-linear classiﬁer
decisions by layer-wise relevance propagation,” plos one, vol. 10,
no. 7, p. e0130140, jul 2015.
[70] r. caruana, y. lou, j. gehrke, p. koch, m. sturm, and n. elhadad,
“intelligible models for healthcare: predicting pneumonia risk and
hospital 30-day readmission,” in proceedings of the acm sigkdd
international conference on knowledge discovery and data mining,
2015, pp. 1721–1730.
[71] m. d. zeiler and r. fergus, “visualizing and understanding convolu-
tional networks,” in computer vision – eccv 2014, d. fleet, t. pajdla,
b. schiele, and t. tuytelaars, eds.
cham: springer international
publishing, 2014, pp. 818–833.
[72] b. kim, c. rudin, and j. shah, “the bayesian case model: a generative
approach for case-based reasoning and prototype classiﬁcation,” in
advances in neural information processing systems, 2014.
[73] k. simonyan, a. vedaldi, and a. zisserman, “deep inside convolutional
networks: visualising image classiﬁcation models and saliency maps,”
2nd international conference on learning representations, iclr 2014
- workshop track proceedings, dec 2013.
[74] d. erhan, a. courville, and y. bengio, “understanding representations
learned in deep architectures,” department dinformatique et recherche
operationnelle, university of montreal, qc, canada, tech. rep, vol.
1355, p. 1, 2010.
[75] l. grosenick, s. greer, and b. knutson, “interpretable classiﬁers for
fmri improve prediction of purchases,” ieee transactions on neural
systems and rehabilitation engineering, vol. 16, no. 6, pp. 539–548,
dec 2008.
[76] v. schetinin, j. e. fieldsend, d. partridge, t. j. coats, w. j. krzanowski,
r. m. everson, t. c. bailey, and a. hernandez, “conﬁdent interpretation
of bayesian decision tree ensembles for clinical applications,” ieee
transactions on information technology in biomedicine, 2007.
[77] f. rossi, “building trust in artiﬁcial intelligence,” journal of interna-
tional affairs, vol. 72, no. 1, pp. 127–134, 2018.
[78] t. c. king, n. aggarwal, m. taddeo, and l. floridi, “artiﬁcial
intelligence crime: an interdisciplinary analysis of foreseeable threats
and solutions,” science and engineering ethics, vol. 26, no. 1, pp. 89–
120, feb 2020.
[79] p. ˇcerka, j. grigien˙e, and g. sirbikyt˙e, “liability for damages caused
by artiﬁcial intelligence,” computer law & security review, vol. 31,
no. 3, pp. 376–389, 2015.
[80] a. b. arrieta, n. d´ıaz-rodr´ıguez, j. del ser, a. bennetot, s. tabik,
a. barbado, s. garc´ıa, s. gil-l´opez, d. molina, r. benjamins
et al., “explainable artiﬁcial intelligence (xai): concepts, taxonomies,
opportunities and challenges toward responsible ai,” information fusion,
vol. 58, pp. 82–115, 2020.
[81] j. zou and l. schiebinger, “ai can be sexist and racist it’s time to
make it fair,” nature, vol. 559, no. 7714, pp. 324–326, jul 2018.
[82] m. du, f. yang, n. zou, and x. hu, “fairness in deep learning: a
computational perspective,” arxiv preprint arxiv:1908.08843, 2019.
[83] s.-k. yeom, p. seegerer, s. lapuschkin, s. wiedemann, k.-r. m¨uller,
and w. samek, “pruning by explaining: a novel criterion for deep
neural network pruning,” arxiv preprint arxiv:1912.08881, 2019.
[84] s. mishra, b. l. sturm, and s. dixon, “local interpretable model-
agnostic explanations for music content analysis,” in proc. 18th int.
soc. music inf. retr. conf. ismir 2017, 2017, pp. 537–543.
[85] t. peltola, “local interpretable model-agnostic explanations of bayesian
predictive models via kullback-leibler projections,” arxiv preprint
arxiv:1810.02678, 2018.
[86] m. rehman zafar and n. mefraz khan, “dlime: a deterministic local
interpretable model-agnostic explanations approach for computer-aided
diagnosis systems,” arxiv preprint arxiv:1906.10263, 2019.
[87] s. bramhall, h. horn, m. tieu, and n. lohia, “qlime-a quadratic local
interpretable model-agnostic explanation approach,” smu data science
review, vol. 3, no. 1, p. 4, 2020.
[88] s. shi, x. zhang, and w. fan, “a modiﬁed perturbed sampling method
for local interpretable model-agnostic explanation,” arxiv preprint
arxiv:2002.07434, 2020.
[89] c. molnar, interpretable machine learning.
lulu. com, 2020.
[90] l. antwarg, b. shapira, and l. rokach, “explaining anomalies detected
by autoencoders using shap,” arxiv preprint arxiv:1903.02407, 2019.
[91] m. sundararajan and a. najmi, “the many shapley values for model
explanation,” arxiv preprint arxiv:1908.08474, 2019.
[92] k. aas, m. jullum, and a. løland, “explaining individual predictions
when features are dependent: more accurate approximations to shapley
values,” arxiv preprint arxiv:1903.10464, 2019.
[93] s. m. lundberg, g. erion, h. chen, a. degrave, j. m. prutkin,
b. nair, r. katz, j. himmelfarb, n. bansal, and s.-i. lee, “from
local explanations to global understanding with explainable ai for trees,”
nature machine intelligence, vol. 2, no. 1, pp. 2522–5839, 2020.
[94] m. vega garc´ıa and j. l. aznarte, “shapley additive explanations for
no2 forecasting,” ecol. inform., vol. 56, p. 101039, mar 2020.
[95] c.-k. yeh, b. kim, s. o. arik, c.-l. li, p. ravikumar, and t. pﬁster,
“on concept-based explanations in deep neural networks,” arxiv preprint
arxiv:1910.07969, 2019.
[96] a. mahendran and a. vedaldi, “salient deconvolutional networks,”
in lecture notes in computer science (including subseries lecture
notes in artiﬁcial intelligence and lecture notes in bioinformatics).
springer, 2016, pp. 120–135.
[97] p.-j. kindermans, k. t. sch¨utt, m. alber, k.-r. m¨uller, d. erhan,
b. kim, and s. d¨ahne, “learning how to explain neural networks:
patternnet and patternattribution,” 6th international conference on
learning representations, iclr 2018 - conference track proceedings,
jan 2018.
[98] a. shrikumar, p. greenside, and a. kundaje, “learning important
features through propagating activation differences,” in proceedings of
the 34th international conference on machine learning - volume 70,
ser. icml17.
jmlr.org, 2017, p. 31453153.
[99] g. erion, j. d. janizek, p. sturmfels, s. lundberg, and s.-i. lee,
“learning explainable models using attribution priors,” arxiv preprint
arxiv:1906.10670, jun 2019.
[100] h. yang, c. rudin, and m. seltzer, “scalable bayesian rule lists,” in
34th international conference on machine learning, icml 2017, 2017.
[101] f. doshi-velez, b. c. wallace, and r. adams, “graph-sparse lda: a
topic model with structured sparsity,” in proceedings of the twenty-
ninth aaai conference on artiﬁcial intelligence, ser. aaai15.
aaai
press, 2015, p. 25752581.
[102] f. doshi-velez and b. kim, “towards a rigorous science of interpretable
machine learning,” arxiv preprint arxiv:1702.08608, 2017.
[103] r. elshawi, y. sherif, m. al-mallah, and s. sakr, “interpretability in
healthcare a comparative study of local machine learning interpretability
techniques,” in proc. - ieee symp. comput. med. syst., 2019.
[104] a. holzinger, a. carrington, and h. m¨uller, “measuring the quality of
explanations: the system causability scale (scs),” ki - k¨unstliche
intelligenz, pp. 193–198, 2020.
[105] m. yang and b. kim, “benchmarking attribution methods with relative
feature importance,” corr, vol. abs/1907.09701, 2019.
[106] t.-y. lin, m. maire, s. belongie, j. hays, p. perona, d. ramanan,
p. doll´ar, and c. l. zitnick, “microsoft coco: common objects in
context,” in european conference on computer vision.
springer, 2014,
pp. 740–755.
[107] b. zhou, a. lapedriza, a. khosla, a. oliva, and a. torralba, “places:
a 10 million image database for scene recognition,” ieee trans.
pattern anal. mach. intell., vol. 40, no. 6, pp. 1452–1464, jun 2018.
[108] d. a. melis and t. jaakkola, “towards robust interpretability with
self-explaining neural networks,” in advances in neural information
processing systems, 2018, pp. 7775–7784.
[109] r. luss, p.-y. chen, a. dhurandhar, p. sattigeri, k. shanmugam, and
c.-c. tu, “generating contrastive explanations with monotonic attribute
functions,” arxiv preprint arxiv:1905.12698, 2019.
[110] s. mohseni and e. d. ragan, “a human-grounded evaluation bench-
mark for local explanations of machine learning,” arxiv preprint
arxiv:1801.05075, 2018.
[111] j. deng, w. dong, r. socher, l. li, kai li, and li fei-fei, “imagenet:
a large-scale hierarchical image database,” in 2009 ieee conference
on computer vision and pattern recognition, 2009, pp. 248–255.
[112] c. a. stewart, t. m. cockerill, i. foster, d. hancock, n. merchant,
e. skidmore, d. stanzione, j. taylor, s. tuecke, g. turner et al.,
“jetstream: a self-provisioned, scalable science and engineering cloud
environment,” in proceedings of the 2015 xsede conference: scientiﬁc
advancements enabled by enhanced cyberinfrastructure, 2015, pp. 1–8.
24
[113] c. molnar, g. casalicchio, and b. bischl, “iml: an r package for
interpretable machine learning,” journal of open source software,
vol. 3, no. 26, p. 786, 2018.
[114] a. ghorbani, a. abid, and j. zou, “interpretation of neural networks is
fragile,” in proceedings of the aaai conference on artiﬁcial intelligence,
vol. 33, 2019, pp. 3681–3688.
[115] h. j. weerts, w. van ipenburg, and m. pechenizkiy, “a human-
grounded evaluation of shap for alert processing,” arxiv preprint
arxiv:1907.03324, 2019.
[116] s. wang, t. zhou, and j. bilmes, “bias also matters: bias attribution
for deep neural network explanation,” in international conference on
machine learning, 2019, pp. 6659–6667.
[117] p.-j. kindermans, s. hooker, j. adebayo, m. alber, k. t. sch¨utt,
s. d¨ahne, d. erhan, and b. kim, “the (un) reliability of saliency
methods,” in explainable ai: interpreting, explaining and visualizing
deep learning.
springer, 2019, pp. 267–280.
[118] j. adebayo, j. gilmer, m. muelly, i. goodfellow, m. hardt, and b. kim,
“sanity checks for saliency maps,” in advances in neural information
processing systems, 2018, pp. 9505–9515.
[119] b. zhou, y. sun, d. bau, and a. torralba, “interpretable basis
decomposition for visual explanation,” in proceedings of the european
conference on computer vision (eccv), 2018, pp. 119–134.
arun das is currently a ph.d. student and research
fellow at the secure ai and autonomy lab and open
cloud institute of university of texas at san antonio
(utsa), san antonio, tx, usa. arun received the
bachelor of technology (b.tech.) degree in electrical
and electronics engineering from cochin university
of science and technology, kerala, india, in 2013 and
the m.s. degree in computer engineering from the
university of texas at san antonio, san antonio, tx,
usa in 2016. he was a recipient of the utsa brain
health consortium graduate student seed grant
in 2020 for his work in behavior analytics for children with neurotypical
disabilities. he is a member of the ieee, and ieee eta kappa nu honor
society. arun’s research interests are in the areas of artiﬁcial intelligence,
computer vision, distributed and parallel computing, cloud computing, and
computer architecture.
peyman najaﬁrad (paul rad) received the phd.
degree in electrical and computer engineering on
cyber analytics from the university of texas at san
antonio, san antonio, tx, usa. he is the founder
and director of the secure ai and autonomy lab,
and an associate professor with the information
systems and cyber security (iscs) from university
of texas at san antonio. he has received his ﬁrst
b.s. degree from sharif university of technology
in computer engineering in 1994, his 1st master in
artiﬁcial intelligence from the tehran polytechnic,
the 2nd master in computer science from the university of texas at san
antonio (magna cum laude) in december 1999, and the ph.d. in electrical
and computer engineering from the university of texas at san antonio. he
was a recipient of the most outstanding graduate student in the college of
engineering, 2016, achieving rackspace innovation mentor program award
for establishing rackspace patent community board structure and mentoring
employees, 2012, achieving dell corporation company excellence (ace)
award in austin for exceptional performance and innovative product research
and development contributions, 2007, and dell inventor milestone award,
top 3 dell inventor of the year, 2005.he holds 15 u.s. patents on cyber
infrastructure, cloud computing, and big data analytics with over 300 product
citations by top fortune 500 leading technology companies such as amazon,
microsoft, ibm, cisco, amazon technologies, hp, and vmware. he has
advised over 200 companies on cloud computing and data analytics with over
50 keynote presentations. he serves on the advisory board for several startups,
high performance cloud group chair at the cloud advisory council (cac),
openstack foundation member, the number 1 open source cloud software,
san antonio tech bloc founding member, childrens hospital of san antonio
foundation board member.
"
2013 FCR Chuan Fertilizer recommendation wheat China.pdf;"
references
buresh,
 r.j.,
 pampolino,
 m.f.,
 witt,
 c.,
 2010.
 field-speciﬁc
 potassium
 and
 phosphorus
balances
 and
 fertilizer
 requirements
 for
 irrigated
 rice-based
 cropping
 systems.
plant
 soil
 335,
 35–64.
buresh,
 r.j.,
 witt,
 c.,
 2007.
 site-speciﬁc
 nutrient
 management.
 in:
 proceedings
 of
 the
ifa
 international
 workshop
 on
 fertilizer
 best
 management
 practices,
 7–9
 march
2007,
 brussels,
 belgium.
 international
 fertilizer
 industry
 association,
 paris,
 pp.
47–55.
chen,
 x.p.,
 2003.
 optimization
 of
 the
 n
 fertilizer
 management
 of
 a winter
wheat/summer
 maize
 rotation
 system
 in
 the
 northern
 china
 plain.
 ph.d.
 dis-
sertation.
 university
 of
 hohenheim,
 stuttgart,
 germany.
chen,
 x.p.,
 zhang,
 f.s.,
 2006.
 theory
 and
 practices
 for
 integrated
 nutrient
 manage-
ment
 in
 wheat–maize
 rotation
 systems.
 china
 agricultural
 university
 press,
beijing
 (in
 chinese).
chuan,
 l.m.,
 he,
 p.,
 jin,
 j.y.,
 li,
 s.t.,
 grant,
 c.,
 zhou,
 w.,
 2012.
 estimating
 nutrient
uptake
 requirements
 for
 wheat
 in
 china.
 field
 crops
 res.,
 unpublished
 results.
cui,
 z.l.,
 2005.
 optimization
 of
 the
 nitrogen
 fertilizer
 management
 for
 a
 winter
wheat–summer
 maize
 rotation
 system
 in
 the
 north
 china
 plain—from
 ﬁeld
 to
regional
 scale.
 ph.d.
 dissertation.
 china
 agric.
 univ.,
 beijing,
 china
 (in
 chinese
with
 english
 abstract).
cui,
 z.l.,
 zhang,
 f.s.,
 chen,
 x.p.,
 miao,
 y.x.,
 li,
 j.l.,
 shi,
 l.w.,
 xu,
 j.f.,
 ye,
 y.l.,
 liu,
 c.s.,
yang,
 z.p.,
 zhang,
 q.,
 huang,
 s.m.,
 bao,
 d.j.,
 2008a.
 on-farm
 evaluation
 of
 an
 in-
season
 nitrogen
 management
 strategy
 based
 on
 soil
 nmin test.
 field
 crops
 res.
105,
 48–55.
cui,
 z.l.,
 zhang,
 f.s.,
 chen,
 x.p.,
 miao,
 y.x.,
 li,
 j.l.,
 shi,
 l.w.,
 xu,
 j.f.,
 ye,
 y.l.,
 liu,
c.s., yang,
 z.p.,
 zhang,
 q.,
 huang,
 s.m.,
 bao,
 d.j.,
 2008b.
 on-farm
 estimation
 of
indigenous
 nutrient
 supply
 for
 site-speciﬁc
 nitrogen
 management
 in
 the
 north
china
 plain.
 nutr.
 cycl.
 agroecosyst.
 81,
 37–47.
dobermann,
 a.,
 2007.
 nutrient
 use
 efﬁciency,
 measurement
 and
 management.
 in:
ifa  international
 workshop
 on
 fertilizer
 best
 management
 practices,
 7–9
 march
2007,
 brussels,
 belgium.
 international
 fertilizer
 industry
 association,
 paris.
dobermann,
 a.,
 cassman,
 k.g.,
 2002.
 plant
 nutrient
 management
 for
 enhanced
 pro-
ductivity
 in
 intensive
 grain
 production
 systems
 of
 the
 united
 states
 and
 asia.
plant
 soil
 247,
 153–175.
dobermann,
 a.,
 witt,
 c.,
 abdulrachman,
 s.,
 gines,
 h.c.,
 nagarajan,
 r.,
 son,
 t.t.,
 tan,
p.s.,  wang,
 g.h.,
 chien,
 n.v.,
 thoa,
 v.t.k.,
 phung,
 c.v.,
 stalin,
 p.,
 muthukrishnan,
p.,
 ravi,
 v.,
 babu,
 m.,
 simbahan,
 g.c.,
 adviento,
 m.a.a.,
 bartolome,
 v.,
 2003.
 esti-
mating
 indigenous
 nutrient
 supplies
 for
 site-speciﬁc
 nutrient
 management
 in
irrigated
 rice.
 agron.
 j.
 95,
 924–935.
dobermann,
 a.,
 witt,
 c.,
 dawe,
 d.,
 abdulrachman,
 s.,
 gines,
 h.c.,
 nagarajan,
 r.,
satawathananont,
 s.,
 son,
 t.t.,
 tan,
 p.s.,
 wang,
 g.h.,
 chien,
 n.v.,
 thoa,
 v.t.k.,
phung,
 c.v.,
 stalin,
 p.,
 muthukrishnan,
 p.,
 ravi,
 v.,
 babu,
 m.,
 chatuporn,
 s.,
kongchum,
 m.,
 sookthongsa,
 j.,
 sun,
 q.,
 fu,
 r.,
 simbahan,
 g.c.,
 adviento,
 m.a.a.,
2002.
 site-speciﬁc
 nutrient
 management
 for
 intensive
 rice
 cropping
 systems
 in
asia.
 field
 crops
 res.
 74,
 37–66.
gao,
 w.,
 jin,
 j.y.,
 he,
 p.,
 li,
 s.t.,
 2008.
 dynamics
 of
 maize
 nutrient
 uptake
 and
 accu-
mulation
 in
 different
 regions
 of
 northern
 china.
 plant
 nutr.
 fert.
 sci.
 14,
 623–629
(in
 chinese
 with
 english
 abstract).
gransee,
 a.,
 merbach,
 w.,
 2000.
 phosphorus
 dynamics
 in
 a
 long-term
 p fertilization
trial
 on
 luvic
 phaeozem
 at
 halle.
 j.
 plant
 nutr.
 soil
 sci.
 163,
 353–357.
he, c.e.,
 liu,
 x.j.,
 fangmeier,
 a.,
 zhang,
 f.s.,
 2007.
 quantifying
 the
 total
 airborne
nitrogen-input
 into
 agro
 ecosystems
 in
 the
 north
 china
 plain.
 agric.
 ecosyst.
environ.
 121,
 395–400.
he,
 p.,
 jin,
 j.y.,
 pampolino,
 m.f.,
 johnston,
 a.,
 2012.
 approach
 and
 decision
 support
system
 based
 on
 crop
 yield
 response
 and
 agronomic
 efﬁciency.
 plant
 nutr.
 fert.
sci.
 18,
 499–505
 (in
 chinese
 with
 english
 abstract).
he,
 p.,
 li,
 s.t.,
 jin,
 j.y.,
 wang,
 h.t.,
 li,
 c.j.,
 wang,
 y.l.,
 cui,
 r.z.,
 2009.
 performance
 of
an optimized
 nutrient
 management
 system
 for
 double-cropped
 wheat–maize
rotations
 in
 north-central
 china.
 agron.
 j.
 101,
 1489–1496.
hou,
 y.l.,
 guo,
 z.,
 ren,
 j.,
 2002.
 summarization
 of
 principles
 and
 models
 for
 semi-
quantitative
 fertilization
 without
 soil
 testing.
 chin.
 j.
 ecol.
 21,
 31–35
 (in
 chinese
with
 english
 abstract).
janssen,
 b.h.,
 guiking,
 f.c.t.,
 van
 der
 eijk,
 d.,
 smaling,
 e.m.a.,
 wolf,
 j.,
 van
 reuler,
h., 1990.
 a
 system
 for
 quantitative
 evaluation
 of
 the
 fertility
 of
 tropical
 soils
(quefts).
 geoderma
 46,
 299–318.
ju, x.t.,
 pan,
 j.r.,
 liu,
 x.j.,
 chen,
 x.p.,
 zhang,
 f.s.,
 mao,
 d.r.,
 2002.
 the
 fate
 of
 nitrogen
fertilizer
 in
 winter
 wheat
 growth
 season
 under
 high
 soil
 fertility
 condition.
 acta
agric.
 nucl.
 sin.
 16,
 397–402
 (in
 chinese
 with
 english
 abstract).
ju,
 x.t.,
 xing,
 g.x.,
 chen,
 x.p.,
 zhang,
 s.l.,
 zhang,
 l.j.,
 liu,
 x.j.,
 cui,
 z.l.,
 yin,
 b.,
 christie,
p.,
 zhu,
 z.l.,
 zhang,
 f.s.,
 2009.
 reducing
 environmental
 risk
 by
 improving
 n
 man-
agement
 in
 intensive
 chinese
 agricultural
 systems.
 proc.
 natl.
 acad.
 sci.
 u.s.a.
106, 3041–3046.
khurana,
 h.s.,
 singh,
 b.,
 dobermann,
 a.,
 philips,
 s.b.,
 sidhu,
 a.s.,
 singh,
 y.,
 2008.
 site-
speciﬁc
 nutrient
 management
 performance
 in
 a rice–wheat
 cropping
 system.
better
 crops
 int.
 92,
 26–28.
ladha,
 j.k.,
 pathak,
 h.,
 krupnik,
 t.j.,
 six,
 j.,
 van
 kessel,
 c.,
 2005.
 efﬁciency
 of
 fertil-
izer  nitrogen
 in
 cereal
 production:
 retrospects
 and
 prospects.
 adv.
 agron.
 87,
85–156.
liu,
 m.q.,
 yu,
 z.r.,
 liu,
 y.h.,
 konijn,
 n.t.,
 2006a.
 fertilizer
 requirements
 for
wheat
 and
 maize
 in
 china:
 the
 quefts
 approach.
 nutr.
 cycl.
 agroecosyst.
 74,
245–258.
liu,
 x.j.,
 ju,
 x.t.,
 zhang,
 y.,
 he,
 c.e.,
 kopsch,
 j.,
 zhang,
 f.s.,
 2006b.
 nitrogen
 deposi-
tion
 in
 agro
 ecosystems
 in
 the
 beijing
 area.
 agric.
 ecosyst.
 environ.
 113,
 370–
377.
lu, r.k.,
 1998.
 principles
 of
 soil–plant
 nutrition
 and
 fertilization.
 chemical
 industry
press,
 beijing
 (in
 chinese).
naklang,
 k.,
 harnpichitvitaya,
 d.,
 amarante,
 s.t.,
 wade,
 l.j.,
 haefele,
 s.m.,
 2006.
 inter-
nal efﬁciency,
 nutrient
 uptake,
 and
 the
 relation
 to
 ﬁeld
 water
 resources
 in
 rainfed
lowland
 rice
 of
 northeast
 thailand.
 plant
 soil
 286,
 193–208.
pampolino,
 m.f.,
 witt,
 c.,
 pasuquin,
 j.m.,
 sinohin,
 p.j.,
 2011.
 nutrient
 expert
 for
hybrid
 maize
 (version
 1.11).
 a
 software
 for
 formulating
 fertilizer
 guidelines
 for
tropical
 hybrid
 maize.
 international
 plant
 nutrition
 institute,
 penang,
 malaysia.
pampolino,
 
m.f.,
 
witt,
 
c.,
 
pasuquin,
 
j.m.,
 
johnston,
 
a.,
 
fisher,
 
m.j.,
 
2012.
development
 approach
 and
 evaluation
 of
 the
 nutrient
 expert
 software
for nutrient
 management
 in
 cereal
 crops.
 comput.
 electron.
 agric.
 88,
103–110.
pathak,
 h.,
 aggarwal,
 p.k.,
 roetter,
 r.p.,
 kalra,
 n.,
 bandyopadhaya,
 s.k.,
 prasad,
 s.,
2003. modelling
 the
 quantitative
 evaluation
 of
 soil
 nutrient
 supply,
 nutrient
 use
efﬁciency,
 and
 fertilizer
 requirements
 of
 wheat
 in
 india.
 nutr.
 cycl.
 agroecosyst.
65,
 105–113.
satyanarayana,
 t.,
 majumdar,
 m.,
 birdar,
 d.p.,
 2011.
 new
 approaches
 and
 tools
 for
site-speciﬁc
 nutrient
 management
 with
 reference
 to
 potassium.
 karnataka
 j.
agric.
 sci.
 24,
 86–90.
setiyono,
 t.d.,
 walters,
 d.t.,
 cassman,
 k.g.,
 witt,
 c.,
 dobermann,
 a.,
 2010.
 estimating
maize
 nutrient
 uptake
 requirements.
 field
 crops
 res.
 118,
 158–168.
smaling,
 e.m.a.,
 janssen,
 b.h.,
 1993.
 calibration
 of
 quefts:
 a model
 predicting
nutrient
 uptake
 and
 yields
 from
 chemical
 soil
 fertility
 indices.
 geoderma
 59,
21–44.
tang, j.c.,
 1994.
 achievements
 and
 tasks
 of
 soil
 and
 fertilizer
 work
 in
 china.
 acta
pedol.
 sin.
 31,
 341–347
 (in
 chinese
 with
 english
 abstract).
weigel,
 a.,
 russow,
 r.,
 korschens,
 m.,
 2000.
 quantiﬁcation
 of
 airborne
 n
 input
 in
long-term
 ﬁeld
 experiments
 and
 its
 validation
 through
 measurements
 using
15n
 isotope
 dilution.
 j.
 plant
 nutr.
 soil
 sci.
 163,
 261–265.
witt,
 c.,
 buresh,
 r.j.,
 peng,
 s.,
 balasubramanian,
 v.,
 dobermann,
 a.,
 2007.
 nutri-
ent  management.
 in:
 fairhurst,
 t.h.,
 witt,
 c.,
 buresh,
 r.j.,
 dobermann,
 a.
(eds.),
 rice:
 a
 practical
 guide
 to
 nutrient
 management,
 second
 ed.
 inter-
national
 rice
 research
 institute
 (irri)/international
 plant
 nutrition
 institute
(ipni)/international
 potash
 institute
 (ipi),
 los
 ba˜nos
 (philippines)/singapore,
 pp.
1–45.
witt,
 c.,
 dobermann,
 a.,
 abdulrachman,
 s.,
 gines,
 h.c.,
 wang,
 g.h.,
 nagarajan,
 r.,
satawatananont,
 s.,
 son,
 t.t.,
 tan,
 p.s.,
 tiem,
 l.v.,
 simbahan,
 g.c.,
 olk,
 d.c.,
 1999.
internal
 nutrient
 efﬁciencies
 of
 irrigated
 lowland
 rice
 in
 tropical
 and
 subtropical
asia.
 field
 crops
 res.
 63,
 113–138.
witt,
 c.,
 pasuquin,
 j.m.,
 dobermann,
 a.,
 2008.
 site-speciﬁc
 nutrient
 management
for maize
 in
 favorable
 tropical
 environments
 of
 asia.
 in:
 proc.
 5th
 international
crop
 sci.
 congress,
 jeju,
 korea.
zhang,
 f.s.,
 wang,
 j.q.,
 zhang,
 w.f.,
 cui,
 z.l.,
 ma,
 w.q.,
 chen,
 x.p.,
 jiang,
 r.f.,
2008a.
 nutrient
 use
 efﬁciencies
 of
 major
 cereal
 crops
 in
 china
 and
 measures
 for
improvement.
 acta
 pedol.
 sin.
 45,
 915–924
 (in
 chinese
 with
 english
 abstract).
zhang,
 y.,
 liu,
 x.j.,
 fangmeier,
 a.,
 goulding,
 k.t.w.,
 zhang,
 f.s.,
 2008b.
 nitrogen
inputs
 and
 isotopes
 in
 precipitation
 in
 the
 north
 china
 plain.
 atmos.
 environ.
42,  1436–1448.
zhao,
 j.r.,
 1997.
 the
 investigation
 and
 analysis
 of
 n
 application
 and
 yield
 in
 beijing
suburb.
 beijing
 agric.
 sci.
 15,
 36–38
 (in
 chinese
 with
 english
 abstract).
zhu,  z.l.,
 1992.
 fertilizer
 fate
 and
 n
 management
 in
 agro
 ecosystem.
 in:
 zhu,
 z.l.,
wen,
 q.x.
 (eds.),
 nitrogen
 in
 soil
 of
 china.
 jiangsu
 science
 and
 technology
 press,
nanjing,
 pp.
 228–245
 (in
 chinese).
"
2102.07750.pdf;"
references
[1] abiteboul, serge. foundations of databases, volume 8. addison-wesley reading, 1995.
[2] s. ackermann, k. schawinski, c. zhang, a. k. weigel, and m. d. turp. using transfer learning to detect galaxy mergers. monthly notices of the
royal astronomical society, 479(1), 2018.
[3] l. aguilar melgar, d. dao, s. gan, n. m. g¨urel, n. hollenstein, j. jiang, b. karlaˇs, t. lemmin, t. li, y. li, et al. ease.ml: a lifecycle
management system for machine learning. conference on innovative data systems research, 2021.
[4] s. alla and s. k. adari. what is mlops? in beginning mlops with mlflow, pages 79–124. springer, 2021.
[5] m. arenas, l. bertossi, and j. chomicki. consistent query answers in inconsistent databases. in acm sigmod-sigact-sigart symposium on
principles of database systems, 1999.
[6] l. bass, i. weber, and l. zhu. devops: a software architect’s perspective. addison-wesley professional, 2015.
[7] c. batini, c. cappiello, c. francalanci, and a. maurino. methodologies for data quality assessment and improvement. acm computing surveys,
41(3), 2009.
[8] j. beck, r. huang, d. lindner, t. guo, z. ce, d. helbing, and n. antulov-fantulin. sensing social media signals for cryptocurrency news. in
world wide web conference, 2019.
[9] a. blum and m. hardt. the ladder: a reliable leaderboard for machine learning competitions. in international conference on machine
learning, 2015.
[10] t. cover and p. hart. nearest neighbor pattern classiﬁcation. ieee transactions on information theory, 13(1), 1967.
[11] j. deng, w. dong, r. socher, l.-j. li, k. li, and l. fei-fei. imagenet: a large-scale hierarchical image database. in ieee conference on
computer vision and pattern recognition, 2009.
[12] p. m. duvall, s. matyas, and a. glover. continuous integration: improving software quality and reducing risk. pearson education, 2007.
[13] c. dwork, v. feldman, m. hardt, t. pitassi, o. reingold, and a. roth. the reusable holdout: preserving validity in adaptive data analysis. science,
349(6248), 2015.
[14] m. fern´andez-delgado, e. cernadas, s. barro, and d. amorim. do we need hundreds of classiﬁers to solve real world classiﬁcation problems?
the journal of machine learning research, 15(1), 2014.
[15] j. friedman, t. hastie, and r. tibshirani. the elements of statistical learning, volume 1. springer series in statistics new york, 2001.
[16] k. fukunaga and d. m. hummels. bayes error estimation using parzen and k-nn procedures. ieee transactions on pattern analysis and machine
intelligence, 5, 1987.
[17] j. gama, i. ˇzliobait˙e, a. bifet, m. pechenizkiy, and a. bouchachia. a survey on concept drift adaptation. acm computing surveys (csur), 46(4),
2014.
[18] i. girardi, j. pengfei, a.-p. nguyen, n. hollenstein, a. ivankay, l. kuhn, c. marchiori, and c. zhang. patient risk assessment and warning
symptom detection using deep attention-based neural networks. in international workshop on health text mining and information analysis,
2018.
[19] n. glaser, o. i. wong, k. schawinski, and c. zhang. radiogan–translations between different radio surveys with generative adversarial networks.
monthly notices of the royal astronomical society, 487(3), 2019.
[20] i. f. ilyas and x. chu. data cleaning. morgan & claypool, 2019.
[21] m. r. karimi, n. m. g¨urel, b. karlaˇs, j. rausch, c. zhang, and a. krause. online active model selection for pre-trained classiﬁers. arxiv
preprint arxiv:2010.09818, 2020.
[22] b. karlaˇs, m. interlandi, c. renggli, w. wu, c. zhang, d. mukunthu iyappan babu, j. edwards, c. lauren, a. xu, and m. weimer. building
continuous integration services for machine learning. in acm sigkdd international conference on knowledge discovery & data mining,
2020.
[23] b. karlaˇs, p. li, r. wu, n. m. g¨urel, x. chu, w. wu, and c. zhang. nearest neighbor classiﬁers over incomplete information: from certain
answers to certain predictions. proceedings of the vldb endowment, 14(3), 2021.
[24] a. karpathy. software 2.0. https://medium.com/@karpathy/software-2-0-a64152b37c35, 2017.
[25] k. katsiapis and k. haas. towards ml engineering with tensorflow extended (tfx). in acm sigkdd international conference on knowledge
discovery & data mining, 2019.
[26] t. kraska. northstar: an interactive data science system. proceedings of the vldb endowment, 11(12), 2018.
[27] s. krishnan, j. wang, e. wu, m. j. franklin, and k. goldberg. activeclean: interactive data cleaning for statistical modeling. proceedings of the
vldb endowment, 9(12), 2016.
[28] p. li, x. rao, j. blase, y. zhang, x. chu, and c. zhang. cleanml: a benchmark for joint data cleaning and machine learning [experiments and
analysis]. arxiv preprint arxiv:1904.09483, 2019.
11
[29] a. ratner, s. h. bach, h. ehrenberg, j. fries, s. wu, and c. r´e. snorkel: rapid training data creation with weak supervision. proceedings of the
vldb endowment, 11(3), 2017.
[30] c. r´e, f. niu, p. gudipati, and c. srisuwananukorn. overton: a data system for monitoring and improving machine-learned products. conference
on innovative data systems research, 2019.
[31] c. renggli, b. karlaˇs, b. ding, f. liu, k. schawinski, w. wu, and c. zhang. continuous integration of machine learning models with ease.ml/ci:
towards a rigorous yet practical treatment. proceedings of machine learning and systems, 2019.
[32] c. renggli, l. rimanic, l. kolar, n. hollenstein, w. wu, and c. zhang. on automatic feasibility study for machine learning application develop-
ment with ease.ml/snoopy. arxiv preprint arxiv:2010.08410, 2020.
[33] c. renggli, l. rimanic, l. kolar, w. wu, and c. zhang. ease.ml/snoopy in action: towards automatic feasibility analysis for machine learning
application development. proceedings of the vldb endowment, 13(12), 2020.
[34] l. rimanic, c. renggli, b. li, and c. zhang. on convergence of nearest neighbor classiﬁers over feature transformations. advances in neural
information processing systems, 33, 2020.
[35] l. f. sartori, k. schawinski, b. trakhtenbrot, n. caplar, e. treister, m. j. koss, c. megan urry, and c. zhang. a model for agn variability on
multiple time-scales. monthly notices of the royal astronomical society: letters, 476(1), 2018.
[36] l. f. sartori, b. trakhtenbrot, k. schawinski, n. caplar, e. treister, and c. zhang. a forward modeling approach to agn variability–method
description and early applications. the astrophysical journal, 883(2), 2019.
[37] m. scannapieco and t. catarci. data quality under a computer science perspective. archivi & computer, 2, 2002.
[38] k. schawinski, m. d. turp, and c. zhang. exploring galaxy evolution with generative models. astronomy & astrophysics, 616, 2018.
[39] k. schawinski, c. zhang, h. zhang, l. fowler, and g. k. santhanam. generative adversarial networks recover features in astrophysical images of
galaxies beyond the deconvolution limit. monthly notices of the royal astronomical society: letters, 467(1), 2017.
[40] s. schelter, s. grafberger, p. schmidt, t. rukat, m. kiessling, a. taptunov, f. biessmann, and d. lange. differential data quality veriﬁcation on
partitioned data. in ieee international conference on data engineering, 2019.
[41] s. schelter, d. lange, p. schmidt, m. celikel, f. biessmann, and a. grafberger. automating large-scale data quality veriﬁcation. proceedings of
the vldb endowment, 11(12), 2018.
[42] s. y. sekeh, b. oselio, and a. o. hero. multi-class bayes error estimation with a global minimal spanning tree. in annual allerton conference on
communication, control, and computing, 2018.
[43] b. settles. active learning literature survey. technical report, university of wisconsin-madison department of computer sciences, 2009.
[44] s. shalev-shwartz and s. ben-david. understanding machine learning: from theory to algorithms. cambridge university press, 2014.
[45] h. shimodaira. improving predictive inference under covariate shift by weighting the log-likelihood function. journal of statistical planning and
inference, 90(2), 2000.
[46] d. stark, b. launet, k. schawinski, c. zhang, m. koss, m. d. turp, l. f. sartori, h. zhang, y. chen, and a. k. weigel. psfgan: a generative
adversarial network system for separating quasar point sources and host galaxy light. monthly notices of the royal astronomical society, 477(2),
2018.
[47] d. m. strong, y. w. lee, and r. y. wang. data quality in context. communications of the acm, 40(5), 1997.
[48] m. su, h. zhang, k. schawinski, c. zhang, and m. a. cianfrocco. generative adversarial networks as a tool to recover structural information from
cryo-electron microscopy data. biorxiv, 2018.
[49] d. suciu, d. olteanu, c. r´e, and c. koch. probabilistic databases. synthesis lectures on data management, 3(2), 2011.
[50] m. sugiyama, s. nakajima, h. kashima, p. v. buenau, and m. kawanabe. direct importance estimation with model selection and its application
to covariate shift adaptation. in advances in neural information processing systems, 2008.
[51] a. tsymbal. the problem of concept drift: deﬁnitions and related work. computer science department, trinity college dublin, 106(2), 2004.
[52] v. n. vapnik and a. y. chervonenkis. on the uniform convergence of relative frequencies of events to their probabilities. in measures of complexity.
springer, 2015.
[53] g. widmer and m. kubat. learning in the presence of concept drift and hidden contexts. machine learning, 23(1), 1996.
[54] k. zhang, b. sch¨olkopf, k. muandet, and z. wang. domain adaptation under target and conditional shift. in international conference on machine
learning, 2013.
12
"
2103.07950.pdf;"
references
[1] mckinsey analytics, “the age of analytics: competing in a data-driven
world,” technical report, san francisco: mckinsey & company, tech.
rep.,
2016.
[online].
available:
https://www.sipotra.it/wp-content/
uploads/2017/01/the-age-of-analytics.pdf
[2] i. gerostathopoulos, m. konersmann, s. krusche, d. i. mattos, j. bosch,
t. bures, b. fitzgerald, m. goedicke, h. muccini, h. h. olsson et al.,
“continuous data-driven software engineering-towards a research
agenda: report on the joint 5th international workshop on rapid
continuous software engineering (rcose 2019) and 1st international
works,” acm sigsoft software engineering notes, vol. 44, no. 3, pp.
60–64, 2019.
[3] f. bernard marr. (2018, may) how much data do we create
every day? the mind-blowing stats everyone should read. [online].
available: https://tinyurl.com/2dyu68fr
[4] e. alpaydin, introduction to machine learning.
mit press, 2009.
[5] s.
business.
(2017,
mar.)
andrew
ng:
why
ai
is
the
new
electricity. [online]. available: https://www.gsb.stanford.edu/insights/
andrew-ng-why-ai-new-electricity
[6] gartner. (2019, oct.) gartner predicts the future of ai technolo-
gies. [online]. available: https://www.gartner.com/smarterwithgartner/
gartner-predicts-the-future-of-ai-technologies/
[7] j. bosch, i. crnkovic, and h. h. olsson, “engineering ai systems: a
research agenda,” arxiv preprint arxiv:2001.07522, 2020.
[8] z. wan, x. xia, d. lo, and g. c. murphy, “how does machine
learning change software development practices?” ieee transactions
on software engineering, 2019.
[9] t. bi, p. liang, a. tang, and c. yang, “a systematic mapping study on
text analysis techniques in software architecture,” journal of systems
and software, vol. 144, pp. 533–558, 2018.
[10] h. muccini and k. vaidhyanathan, “a machine learning-driven ap-
proach for proactive decision making in adaptive architectures,” in
2019 ieee international conference on software architecture compan-
ion (icsa-c).
ieee, 2019, pp. 242–245.
[11] d. e. perry and a. l. wolf, “foundations for the study of software
architecture,” acm sigsoft software engineering notes, vol. 17, no. 4,
pp. 40–52, 1992.
[12] itstat. (2016, jan.) i musei, le aree archeologiche e i
monumenti in italia. [online]. available: https://www.istat.it/it/
ﬁles/2016/12/report-musei.pdf?title=musei+
[13] n.
squires.
(2018,
jan.)
florence’s
ufﬁzi
gallery
to
reduce
four-hour
queues
to
ﬁve
minutes
with
the
new
ticketing
algorithm. [online]. available: https://www.telegraph.co.uk/news/2018/
10/09/ﬂorences-ufﬁzi-galleries-reduce-four-hour-queues-ﬁve-minutes/
[14] “iso/iec/ieee systems and software engineering – architecture
description,”
iso/iec/ieee
42010:2011(e)
(revision
of
iso/iec
42010:2007 and ieee std 1471-2000), pp. 1–46, 2011.
[15] j. a. zachman, “a framework for information systems architecture,”
ibm systems journal, vol. 26, no. 3, pp. 276–292, 1987.
[16] p. b. kruchten, “the 4+ 1 view model of architecture,” ieee software,
vol. 12, no. 6, pp. 42–50, 1995.
[17] k. raymond, “reference model of open distributed processing (rm-
odp): introduction,” in open distributed processing.
springer, 1995,
pp. 3–14.
[18] e. woods, “software architecture in a changing world,” ieee software,
vol. 33, no. 6, pp. 94–97, 2016.
[19] h. muccini and m. sharaf, “caps: architecture description of
situational aware cyber physical systems,” in 2017 ieee international
conference on software architecture, icsa 2017, gothenburg, sweden,
april 3-7, 2017. ieee computer society, 2017, pp. 211–220. [online].
available: https://doi.org/10.1109/icsa.2017.21
[20] s. amershi, a. begel, c. bird, r. deline, h. gall, e. kamar, n. na-
gappan, b. nushi, and t. zimmermann, “software engineering for ma-
chine learning: a case study,” in 2019 ieee/acm 41st international
conference on software engineering: software engineering in practice
(icse-seip).
ieee, 2019, pp. 291–300.
[21] l. e. lwakatare, a. raj, j. bosch, h. h. olsson, and i. crnkovic, “a
taxonomy of software engineering challenges for machine learning
systems: an empirical investigation,” in international conference on
agile software development.
springer, cham, 2019, pp. 227–243.
[22] l. dobrica and e. niemela, “a survey on software architecture analysis
methods,” ieee transactions on software engineering, vol. 28, no. 7,
pp. 638–653, 2002.
[23] m. kwiatkowska, “quantitative veriﬁcation: models techniques and
tools,” in proceedings of the the 6th joint meeting of the european
software engineering conference and the acm sigsoft symposium on
the foundations of software engineering, 2007, pp. 449–458.
[24] i. malavolta, p. lago, h. muccini, p. pelliccione, and a. tang, “what
industry needs from architectural languages: a survey,” ieee trans-
actions on software engineering, vol. 39, no. 6, pp. 869–891, 2013.
[25] b. randell, “the 1968/69 nato software engineering reports,” his-
tory of software engineering, vol. 37, 1996.
[26] d. weyns, an introduction to self-adaptive systems: a contemporary
software engineering perspective.
john wiley & sons, 2020.
[27] f. d. mac´ıas-escriv´a, r. haber, r. del toro, and v. hernandez,
“self-adaptive systems: a survey of current approaches, research
challenges
and
applications,”
expert
systems
with
applications,
vol.
40, no.
18, pp.
7267
– 7279,
2013.
[online]. available:
http://www.sciencedirect.com/science/article/pii/s0957417413005125
[28] c. krupitzer, f. m. roth, s. vansyckel, g. schiele, and c. becker, “a
survey on engineering approaches for self-adaptive systems,” pervasive
and mobile computing, vol. 17, pp. 184–206, 2015.
[29] p. jamshidi, j. c´amara, b. schmerl, c. k¨aestner, and d. garlan, “ma-
chine learning meets quantitative planning: enabling self-adaptation
in autonomous robots,” in 2019 ieee/acm 14th international sympo-
sium on software engineering for adaptive and self-managing systems
(seams).
ieee, 2019, pp. 39–50.
[30] f. quin, t. bamelis, s. b. sarpreet, and s. michiels, “efﬁcient analysis
of large adaptation spaces in self-adaptive systems using machine
learning,” in 2019 ieee/acm 14th international symposium on soft-
ware engineering for adaptive and self-managing systems (seams).
ieee, 2019, pp. 1–12.
[31] h. muccini and k. vaidhyanathan, “leveraging machine learning
techniques for architecting self-adaptive iot systems,” in 2020 ieee
international conference on smart computing (smartcomp), 2020,
pp. 65–72.
[32] m. wu and m. kwiatkowska, “robustness guarantees for deep neural
networks on videos,” in proceedings of the ieee/cvf conference on
computer vision and pattern recognition, 2020, pp. 311–320.
[33] j.
c´amara,
h.
muccini,
and
k.
vaidhyanathan,
“quantitative
veriﬁcation-aided machine learning: a tandem approach for ar-
chitecting self-adaptive iot systems,” in 2020 ieee international
conference on software architecture (icsa), 2020, pp. 11–22.
[34] r. courtland, “bias detectives: the researchers striving to make algo-
rithms fair,” nature, vol. 558, no. 7710, pp. 357–357, 2018.
[35] oreilly. (2021, jan.) the road to software 2.0. [online]. available:
https://www.oreilly.com/radar/the-road-to-software-2-0/
[36] h. p. breivold, i. crnkovic, and m. larsson, “a systematic review
of software architecture evolution research,” information and software
technology, vol. 54, no. 1, pp. 16–40, 2012.
[37] a. cummaudo, r. vasa, j. grundy, m. abdelrazek, and a. cain, “losing
conﬁdence in quality: unspoken eevolution of computer vision services,”
in 2019 ieee international conference on software maintenance and
evolution (icsme).
ieee, 2019, pp. 333–342.
[38] a. cummaudo, s. barnett, r. vasa, j. grundy, and m. abdelrazek,
“beware the evolving ‘intelligent’web service! an integration architec-
ture tactic to guard ai-ﬁrst components,” in proceedings of the 28th
acm joint meeting on european software engineering conference and
symposium on the foundations of software engineering, 2020, pp. 269–
280.
"
2107.00079.pdf;"
references
[1] james bergstra, rémi bardenet, yoshua bengio, and balázs kégl. 2011. algorithms
for hyper-parameter optimization. in neurips, vol. 24.
[2] james bergstra and yoshua bengio. 2012. random search for hyper-parameter
optimization. jmlr 13, 2 (2012).
[3] james bergstra, daniel yamins, and david cox. 2013.
making a science of
model search: hyperparameter optimization in hundreds of dimensions for vision
architectures. in icml. pmlr, 115–123.
[4] umang bhatt, javier antorán, yunfeng zhang, q vera liao, prasanna sattigeri,
riccardo fogliato, gabrielle gauthier melançon, ranganath krishnan, jason stan-
ley, omesh tickoo, et al. 2020. uncertainty as a form of transparency: measuring,
communicating, and using uncertainty. arxiv preprint arxiv:2011.07586 (2020).
[5] wj brown, rc malveau, mccormick brown, and mowbray hw iii. 1998. tj,
antipatterns: refactoring software, architectures, and projects in crisis.
[6] gavin c. cawley and nicola l.c. talbot. 2010. on over-fitting in model selection
and subsequent selection bias in performance evaluation. jmlr 11 (aug. 2010),
2079–2107.
[7] prithwish chakraborty, sathappan muthiah, ravi tandon, and naren ramakr-
ishnan. 2016. hierarchical quickest change detection via surrogates. arxiv
preprint arxiv:1603.09739 (2016).
[8] nitesh v. chawla, kevin w. bowyer, lawrence o. hall, and w. philip kegelmeyer.
2002. smote: synthetic minority over-sampling technique. j. artif. int. res. 16,
1 (june 2002), 321–357.
[9] ryan cotterell, sabrina j mielke, jason eisner, and brian roark. 2018. are all
languages equally hard to language-model? arxiv preprint arxiv:1806.03743
(2018).
[10] pedro domingos and geoff hulten. 2000. mining high-speed data streams. in
acm sigkdd. 71–80.
[11] thomas elsken, jan hendrik metzen, frank hutter, et al. 2019. neural architecture
search: a survey. jmlr 20, 55 (2019), 1–21.
[12] joão gama, indr˙e žliobait˙e, albert bifet, mykola pechenizkiy, and abdelhamid
bouchachia. 2014. a survey on concept drift adaptation. acm csur 46, 4 (2014),
1–37.
[13] oguzhan gencoglu, mark van gils, esin guldogan, chamin morikawa, mehmet
süzen, mathias gruber, jussi leinonen, and heikki huttunen. 2019. hark side
of deep learning–from grad student descent to automated machine learning.
arxiv preprint arxiv:1904.07633 (2019).
[14] peter henderson, riashat islam, philip bachman, joelle pineau, doina precup,
and david meger. 2018. deep reinforcement learning that matters. in proceedings
of the aaai conference on artificial intelligence, vol. 32.
[15] geoffrey e hinton. 2012. a practical guide to training restricted boltzmann
machines. in neural networks: tricks of the trade. springer, 599–619.
[16] charles isbell. 2020. you can’t escape hyperparameters and latent variables:
machine learning as a software engineering enterpri. neurips 2020 (dec 2020).
[17] tinu theckel joy, santu rana, sunil gupta, and svetha venkatesh. 2016. hy-
perparameter tuning for big data using bayesian optimisation. in icpr. ieee,
2574–2579.
[18] ralf klinkenberg. 2004. learning drifting concepts: example selection vs. example
weighting. intelligent data analysis 8, 3 (2004), 281–300.
[19] tom kwiatkowski, eunsol choi, yoav artzi, and luke zettlemoyer. 2013. scaling
semantic parsers with on-the-fly ontology matching. in emnlp. 1545–1556.
[20] hugo larochelle, dumitru erhan, aaron courville, james bergstra, and yoshua
bengio. 2007. an empirical evaluation of deep architectures on problems with
many factors of variation. in icml. 473–480.
[21] yann a lecun, léon bottou, genevieve b orr, and klaus-robert müller. 2012.
efficient backprop. in neural networks: tricks of the trade. springer, 9–48.
[22] zachary c lipton and jacob steinhardt. 2018. troubling trends in machine
learning scholarship. arxiv preprint arxiv:1807.03341 (2018).
[23] jie lu, anjin liu, fan dong, feng gu, joao gama, and guangquan zhang. 2018.
learning under concept drift: a review. ieee tkde 31, 12 (2018), 2346–2363.
[24] gábor melis, chris dyer, and phil blunsom. 2017. on the state of the art of
evaluation in neural language models. arxiv preprint arxiv:1707.05589 (2017).
[25] margaret mitchell, simone wu, andrew zaldivar, parker barnes, lucy vasserman,
ben hutchinson, elena spitzer, inioluwa deborah raji, and timnit gebru. 2019.
model cards for model reporting. in facct. 220–229.
[26] jishnu mukhoti, pontus stenetorp, and yarin gal. 2018. on the importance
of strong baselines in bayesian deep learning. arxiv preprint arxiv:1811.09385
(2018).
[27] vu nguyen, sebastian schulze, and michael a osborne. 2019. bayesian optimiza-
tion for iterative learning. arxiv preprint arxiv:1909.09593 (2019).
[28] andrei paleyes, raoul-gabriel urma, and neil d lawrence. 2020. challenges in de-
ploying machine learning: a survey of case studies. arxiv preprint arxiv:2011.09926
(2020).
[29] philipp probst, anne-laure boulesteix, and bernd bischl. 2019. tunability: im-
portance of hyperparameters of machine learning algorithms. jmlr 20, 53 (2019),
1–32.
[30] pranav rajpurkar, jian zhang, konstantin lopyrev, and percy liang. 2016.
squad: 100,000+ questions for machine comprehension of text. arxiv preprint
arxiv:1606.05250 (2016).
[31] naren ramakrishnan, patrick butler, sathappan muthiah, et al. 2014. ’beating
the news’ with embers: forecasting civil unrest using open source indicators.
in acm sigkdd (kdd ’14). association for computing machinery.
[32] marco tulio ribeiro, sameer singh, and carlos guestrin. 2016. "" why should i
trust you?"" explaining the predictions of any classifier. in acm sigkdd. 1135–
1144.
[33] ravi k samala, heang-ping chan, lubomir hadjiiski, and sathvik koneru. 2020.
hazards of data leakage in machine learning: a study on classification of breast
cancer using deep neural networks. in medical imaging 2020: computer-aided
mlops with financial applications
diagnosis, vol. 11314. international society for optics and photonics, 1131416.
[34] david sculley, gary holt, daniel golovin, eugene davydov, todd phillips, diet-
mar ebner, vinay chaudhary, michael young, jean-francois crespo, and dan
dennison. 2015. hidden technical debt in machine learning systems. neurips 28
(2015), 2503–2511.
[35] dinghan shen, guoyin wang, wenlin wang, martin renqiang min, qinliang
su, yizhe zhang, chunyuan li, ricardo henao, and lawrence carin. 2018. base-
line needs more love: on simple word-embedding-based models and associated
pooling mechanisms. arxiv preprint arxiv:1805.09843 (2018).
[36] jasper snoek, hugo larochelle, and ryan p adams. 2012. practical bayesian
optimization of machine learning algorithms. arxiv preprint arxiv:1206.2944
(2012).
[37] jan n van rijn and frank hutter. 2018. hyperparameter importance across
datasets. in acm sigkdd. 2367–2376.
[38] alex wang, amanpreet singh, julian michael, felix hill, omer levy, and samuel r
bowman. 2018. glue: a multi-task benchmark and analysis platform for natural
language understanding. arxiv preprint arxiv:1804.07461 (2018).
[39] geoffrey i webb, roy hyde, hong cao, hai long nguyen, and francois petitjean.
2016. characterizing concept drift. data mining and knowledge discovery 30, 4
(2016), 964–994.
[40] gerhard widmer and miroslav kubat. 1996. learning in the presence of concept
drift and hidden contexts. machine learning 23, 1 (1996), 69–101.
"
2112.11925.pdf;"
references
[1] alex krizhevsky, ilya sutskever, and geoffrey e hinton. imagenet classiﬁcation with deep convolutional neural
networks. advances in neural information processing systems, 25:1097–1105, 2012.
[2] martín abadi, paul barham, jianmin chen, zhifeng chen, andy davis, jeffrey dean, matthieu devin, sanjay
ghemawat, geoffrey irving, michael isard, et al. tensorﬂow: a system for large-scale machine learning. in 12th
{usenix} symposium on operating systems design and implementation ({osdi} 16), pages 265–283, 2016.
[3] adam paszke, sam gross, francisco massa, adam lerer, james bradbury, gregory chanan, trevor killeen,
zeming lin, natalia gimelshein, luca antiga, et al. pytorch: an imperative style, high-performance deep
learning library. advances in neural information processing systems, 32:8026–8037, 2019.
[4] christopher olston, noah fiedel, kiril gorovoy, jeremiah harmsen, li lao, fangwei li, vinu rajashekhar,
sukriti ramesh, and jordan soyke. tensorﬂow-serving: flexible, high-performance ml serving. arxiv preprint
arxiv:1712.06139, 2017.
[5] ekaba bisong. kubeﬂow and kubeﬂow pipelines. in building machine learning and deep learning models on
google cloud platform, pages 671–685. springer, 2019.
[6] ayush shridhar, phil tomson, and mike innes. interoperating deep learning models with onnx. jl. in proceedings
of the juliacon conferences, volume 1, page 59, 2020.
[7] junjie bai, fang lu, ke zhang, et al. onnx: open neural network exchange. github repository, 2019.
[8] alessandro del sole. introducing microsoft cognitive services. in microsoft computer vision apis distilled,
pages 1–4. springer, 2018.
[9] fabian pedregosa, gaël varoquaux, alexandre gramfort, vincent michel, bertrand thirion, olivier grisel,
mathieu blondel, peter prettenhofer, ron weiss, vincent dubourg, et al. scikit-learn: machine learning in python.
the journal of machine learning research, 12:2825–2830, 2011.
[10] mqtt. http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html.
[11] amqp.
http://docs.oasis-open.org/amqp/core/v1.0/os/amqp-core-overview-v1.0-os.html#
anchor-rfc2119.
[12] roy t. fielding, james gettys, jeffrey c. mogul, henrik frystyk nielsen, larry masinter, paul j. leach,
and tim berners-lee. hypertext transfer protocol – http/1.1. rfc 2616, rfc editor, june 1999. http:
//www.rfc-editor.org/rfc/rfc2616.txt.
6
[13] ravi teja mullapudi, william r mark, noam shazeer, and kayvon fatahalian. hydranets: specialized dynamic
architectures for efﬁcient inference. in proceedings of the ieee conference on computer vision and pattern
recognition, pages 8080–8089, 2018.
[14] mingxing tan and quoc le. efﬁcientnet: rethinking model scaling for convolutional neural networks. in
international conference on machine learning, pages 6105–6114. pmlr, 2019.
[15] devito et al. torchscript. https://pytorch.org/docs/1.9.0/jit.html, 2019.
7
"
2201.00162.pdf;"
references
[1] s. m¨akinen, h. skogstr¨om, v. turku, e. laaksonen, and
t. mikkonen, “who needs mlops: what data scientists seek
to accomplish and how can mlops help?”
[2] c. renggli, l. rimanic, n. m. g¨urel, b. karlaˇs, w. wu,
c. zhang, and e. zurich, “a data quality-driven view of
mlops,” 2 2021. [online]. available: https://arxiv.org/abs/
2102.07750v1
[3] p. ruf, m. madan, c. reich, and d. ould-abdeslam,
“demystifying
mlops
and
presenting
a
recipe
for
the
selection of open-source tools,” applied sciences 2021,
vol.
11,
page
8861,
vol.
11,
p.
8861,
9
2021.
[online].
available:
https://www.mdpi.com/2076-3417/11/
19/8861/htmhttps://www.mdpi.com/2076-3417/11/19/8861
[4] j. klaise, a. v. looveren, c. cox, g. vacanti, and a. coca,
“monitoring and explainability of models in production,” 7
2020. [online]. available: https://arxiv.org/abs/2007.06299v1
[5] d. a. tamburri, “sustainable mlops: trends and challenges,”
proceedings - 2020 22nd international symposium on sym-
bolic and numeric algorithms for scientiﬁc computing,
synasc 2020, pp. 17–23, 9 2020.
[6] s. alla and s. k. adari, “what is mlops?” in beginning
mlops with mlflow.
springer, 2021, pp. 79–124.
[7] s. sharma, “the devops adoption playbook : a guide to
adopting devops in a multi-speed it enterprise,” ibm press,
pp. 34–58.
[8] “continuous software engineering: a roadmap and agenda,”
journal of systems and software, vol. 123, pp. 176–189, 1
2017.
[9] n. gift and a. deza, practical mlops: operationalizing
machine learning models.
o’reilly media, inc, 2020.
[10] e. raj, “mlops using azure machine learning rapidly test,
build, and manage production-ready machine learning life
cycles at scale.” packt publishing limited, pp. 45–62,
2021.
[11] c. a. ioannis karamitsos, saeed albarhami, “applying
devops practices of continuous automation for machine learn-
ing,” information 2020, vol. 11, page 363, vol. 11, p. 363, 7
2020. [online]. available: https://www.mdpi.com/2078-2489/
11/7/363/htmhttps://www.mdpi.com/2078-2489/11/7/363
[12] b.
fitzgerald
and
k.-j.
stol,
“continuous
software
engineering
and
beyond:
trends
and
challenges
general
terms,”
proceedings
of
the
1st
international
workshop
on
rapid
continuous
software
engineering
-
rcose
2014,
vol.
14,
2014.
[online].
available:
http://dx.doi.org/10.1145/2593812.2593813
[13] m. m. john, h. h. olsson, and j. bosch, “towards
mlops: a framework and maturity model,” 2021 47th
euromicro
conference
on
software
engineering
and
advanced applications (seaa), pp. 1–8, 9 2021. [online].
available: https://ieeexplore.ieee.org/document/9582569/
[14] m. treveil and d. team, “introducing mlops how to
scale
machine
learning
in
the
enterprise,”
p.
185,
2020. [online]. available: https://www.oreilly.com/library/
view/introducing-mlops/9781492083283/
[15] d.
sato,
“thoughtworksinc/cd4ml-workshop:
repository
with
sample
code
and
instructions
for
”continuous
intelligence”
and
”continuous
delivery
for
machine
learning:
cd4ml”
workshops.”
[online].
available:
https://github.com/thoughtworksinc/cd4ml-workshop
[16] t. granlund, a. kopponen, v. stirbu, l. myllyaho, and
t. mikkonen, “mlops challenges in multi-organization setup:
experiences from two real-world cases.” [online]. available:
https://oraviz.io/
[17] d.
sato,
a.
wider,
and
c.
windheuser,
“continuous
delivery for machine learning.” [online]. available: https:
//martinfowler.com/articles/cd4ml.htmldatapipelines
[18] google,
“mlops:
continuous
delivery
and
automation
pipelines
in
machine
learning-google
cloud.”
[online].
available:
https://cloud.google.com/architecture/
mlops-continuous-delivery-and-automation-pipelines-in-machine-learning
[19] “machine
learning
operations
maturity
model-
azure
architecture
center-microsoft
docs.”
[on-
line].
available:
https://docs.microsoft.com/en-us/azure/
architecture/example-scenario/mlops/mlops-maturity-model
[20] a. felipe and v. maya, “the state of mlops,” 2016.
[21] l. zhou, s. pan, j. wang, and a. v. vasilakos, “machine
learning on big data: opportunities and challenges,” neuro-
computing, vol. 237, pp. 350–361, 5 2017.
[22] t. g. dietterich, “machine learning for sequential data:
a review,” lecture notes in computer science (including
subseries
lecture
notes
in
artiﬁcial
intelligence
and
lecture notes in bioinformatics), vol. 2396, pp. 15–30,
2002. [online]. available: https://link.springer.com/chapter/
10.1007/3-540-70659-3.2
[23] t.
fredriksson,
d.
i.
mattos,
j.
bosch,
and
h.
h.
olsson, “data labeling: an empirical investigation into
industrial challenges and mitigation strategies,” lecture
notes in computer science (including subseries lecture
notes
in
artiﬁcial
intelligence
and
lecture
notes
in
bioinformatics),
vol.
12562
lncs,
pp.
202–216,
11
2020. [online]. available: https://link.springer.com/chapter/
10.1007/978-3-030-64148-1.13
[24] m.
armbrust,
t.
das,
l.
sun,
b.
yavuz,
s.
zhu,
m.
murthy,
j.
torres,
h.
van
hovell,
a.
ionescu,
a.
łuszczak,
m.
´switakowski,
m.
szafra´nski,
x.
li,
t. ueshin, m. mokhtar, p. boncz, a. ghodsi, s. paranjpye,
p.
senster,
r.
xin,
and
m.
zaharia,
“delta
lake,”
proceedings of the vldb endowment, vol. 13, pp. 3411–
3424, 8 2020. [online]. available: https://dl.acm.org/doi/abs/
10.14778/3415478.3415560
[25] s. khalid, t. khalil, and s. nasreen, “a survey of feature se-
lection and feature extraction techniques in machine learning,”
proceedings of 2014 science and information conference,
sai 2014, pp. 372–378, 10 2014.
[26] r. bardenet, m. brendel, b. k´egl, m. sebag, and s. fr,
“collaborative hyperparameter tuning,” vol. 28, pp. 199–207,
5 2013. [online]. available: https://proceedings.mlr.press/
v28/bardenet13.html
[27] l. savu, “cloud computing: deployment models, delivery
models, risks and research challanges,” 2011 international
conference on computer and management, caman 2011,
2011.
[28] j. de la r´ua mart´ınez, “scalable architecture for automating
machine
learning
model
monitoring,”
2020.
[online].
available: http://oatd.org/oatd/record?record=oai
[29] j. bosch and h. h. olsson, “digital for real: a multicase
study on the digital transformation of companies in the
embedded systems domain,” journal of software: evolution
and process, vol. 33, p. e2333, 5. [online]. available:
https://onlinelibrary.wiley.com/doi/full/10.1002/smr.2333
[30] “tensorﬂow
extended
(tfx)-ml
production
pipelines.”
[online]. available: https://www.tensorﬂow.org/tfx
[31] “meet
michelangelo:
uber’s
machine
learning
platform.”
[online].
available:
https://eng.uber.com/
michelangelo-machine-learning-platform/
[32] “bighead:
airbnb’s
end-to-end
ma-
chine
learning
platform-databricks.”
[on-
line].
available:
https://databricks.com/session/
bighead-airbnbs-end-to-end-machine-learning-platform
[33] “metaﬂow.” [online]. available: https://metaﬂow.org/
[34] s. a. s. k. adari, “beginning mlops with mlﬂow deploy
models in aws sagemaker, google cloud, and microsoft
azure,” 2021. [online]. available: https://doi.org/10.1007/
978-1-4842-6549-9
[35] s.
k.
karmaker,
m.
hassan,
m.
j.
smith,
m.
m.
hassan,
l.
xu,
c.
zhai,
k.
veeramachaneni,
s.
k.
karmaker, m. m. hassan, s. ginn, m. j. smith, l. xu,
k. veeramachaneni, and c. zhai, “automl to date and
beyond: challenges and opportunities,” acm computing
surveys (csur), vol. 54, p. 175, 10 2021. [online].
available: https://dl.acm.org/doi/abs/10.1145/3470918
[36] p. gijsbers, e. ledell, j. thomas, s. poirier, b. bischl, and
j. vanschoren, “an open source automl benchmark,” 7 2019.
[online]. available: https://arxiv.org/abs/1907.00909v1
[37] “auto-sklearn 2.0: hands-free automl via meta-learning,” 7
2020. [online]. available: http://arxiv.org/abs/2007.04074
[38] “autokeras.” [online]. available: https://autokeras.com/
[39] “tpot.” [online]. available: http://epistasislab.github.io/tpot/
[40] l. zimmer, m. lindauer, and f. hutter, “auto-pytorch
tabular: multi-ﬁdelity metalearning for efﬁcient and robust
autodl,” ieee transactions on pattern analysis and machine
intelligence, vol. 43, pp. 3079–3090, 6 2020. [online].
available: https://arxiv.org/abs/2006.13799v3
[41] “bigml.com.” [online]. available: https://bigml.com/
[42] “cloud automl custom machine learning models-google
cloud.” [online]. available: https://cloud.google.com/automl
[43] “modern business runs on ai-no code ai-akkio.” [online].
available: https://www.akkio.com/
[44] “h2o.ai-ai
cloud
platform.”
[online].
available:
https:
//www.h2o.ai/
[45] “what is automated ml?-automl-azure machine learning.”
[online]. available: https://docs.microsoft.com/en-us/azure/
machine-learning/concept-automated-ml
[46] “amazon sagemaker autopilot-amazon sagemaker.” [online].
available: https://aws.amazon.com/sagemaker/autopilot/
[47] m. feurer and f. hutter, “practical automated machine learn-
ing for the automl challenge 2018,” icml 2018 automl
workshop, 2018.
[48] g. fursin, “the collective knowledge project: making ml
models more portable and reproducible with open apis,
reusable best practices and mlops,” 6 2020. [online].
available: https://arxiv.org/abs/2006.07161v2
[49] y. zhou, y. yu, and b. ding, “towards mlops: a case study of
ml pipeline platform,” proceedings - 2020 international con-
ference on artiﬁcial intelligence and computer engineering,
icaice 2020, pp. 494–500, 10 2020.
[50] s. schelter, f. biessmann, t. januschowski, d. salinas,
s. seufert, and g. szarvas, “on challenges in machine
learning model management,” 2018.
[51] a. banerjee, c.-c. chen, c.-c. hung, x. huang, y. wang,
and
r.
chevesaran,
“challenges
and
experiences
with
mlops for performance diagnostics in hybrid-cloud enterprise
software deployments,” 2020. [online]. available: https:
//www.vmware.com/solutions/trustvm-
[52] k. d. apostolidis and g. a. papakostas, “a survey on adver-
sarial deep learning robustness in medical image analysis,”
electronics, vol. 10, p. 2132, 2021.
[53] g. p. avramidis, m. p. avramidou, and g. a. papakostas,
“rheumatoid arthritis diagnosis: deep learning vs. humane,”
applied sciences, vol. 12, p. 10, 2022.
"
2201.04233.pdf;"
references
[1]
the open group, archimate® 3.1 specification.
van haren publishing, 2019.
[2]
dama international, dama-dmbok: data man-
agement body of knowledge, 2nd edition. technics
publications, 2017.
[3]
r. edjlali and m. beyer, “understanding the logical
data warehouse: the emerging practice,” gartner,
tech. rep. g00234996, 2012.
[4]
e. zaidi, e. thoo, g. de simoni, and m. beyer,
“data fabrics add augmented intelligence to mod-
ernize your data integration,” gartner, tech. rep.
g00450706, 2019.
[5]
z. dehghani, how to move beyond a monolithic
data lake to a distributed data mesh, 2019. [on-
line]. available: https : / / martinfowler . com /
articles/data-monolith-to-mesh.html (visited on
11/19/2021).
[6]
r. kimball and m. ross, the data warehouse
toolkit: the definitive guide to dimensional mod-
eling, 3rd edition. wiley, 2013.
[7]
w. h. inmon, building the data warehouse, 4th
edition. wiley, 2005.
[8]
d. linstedt and m. olschimke, building a scalable
data warehouse with data vault 2.0. morgan kauf-
mann, 2015.
[9]
n. marz and j. warren, big data: principles and
best practices of scalable realtime data systems.
manning, 2015.
[10]
j. kreps, i heart logs: event data, stream process-
ing, and data integration. o’reilly media, 2014.
[11]
e. gamma, r. helm, r. e. johnson, and j. vlis-
sides, design patterns: elements of reusable object-
oriented software. prentice hall, 1995.
[12]
h. cook, “solution path for planning and imple-
menting the logical data warehouse,” gartner,
tech. rep. g00320563, 2017.
[13]
m. fowler, polyglot persistence, 2011. [online].
available: https : / / martinfowler . com / bliki /
polyglotpersistence.html (visited on 11/19/2021).
[14]
z. dehghani, data mesh principles and logical
architecture, 2020. [online]. available: https : / /
martinfowler.com/articles/data-mesh-principles.
html (visited on 11/19/2021).
"
2201.04876.pdf;"
references
[1] saleema amershi, andrew begel, christian bird, robert deline, harald gall, ece
kamar, nachiappan nagappan, besmira nushi, and thomas zimmermann. 2019.
software engineering for machine learning: a case study. in 2019 ieee/acm
41st international conference on software engineering: software engineering in
practice (icse-seip). 291–300. https://doi.org/10.1109/icse-seip.2019.00042
[2] anders arpteg, björn brinne, luka crnkovic-friis, and jan bosch. 2018. software
engineering challenges of deep learning. in 2018 44th euromicro conference on
software engineering and advanced applications (seaa). 50–59. https://doi.org/
10.1109/seaa.2018.00018
[3] jan bosch, ivica crnkovic, and helena holmström olsson. 2020. engineering ai
systems: a research agenda. arxiv:2001.07522 [cs] (2020). http://arxiv.org/abs/
2001.07522 arxiv: 2001.07522.
[4] opc foundation. 2008. what is opc? https://opcfoundation.org/about/what-
is-opc/ accessed 2022-01-13.
[5] felix gervits, terry w. fong, and matthias scheutz. 2018. shared mental models
to support distributed human-robot teaming in space. in 2018 aiaa space
and astronautics forum and exposition. american institute of aeronautics and
astronautics. https://doi.org/10.2514/6.2018-5340
[6] thilo hagendorff. 2020. the ethics of ai ethics: an evaluation of guidelines.
minds and machines 30, 1 (2020), 99–120. https://doi.org/10.1007/s11023-020-
09517-8
[7] hleg. 2019.
ethics guidelines for trustworthy ai | shaping europe’s digi-
tal future.
https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-
trustworthy-ai accessed 2022-01-13.
[8] ieee. 2019. ieee global a/is ethics initiative.
https://standards.ieee.org/
industry-connections/ec/autonomous-systems.html accessed 2022-01-13.
[9] brittany johnson and justin smith. 2021. towards ethical data-driven software:
filling the gaps in ethics research & practice. in 2021 ieee/acm 2nd international
workshop on ethics in software engineering research and practice (sethics). 18–25.
https://doi.org/10.1109/sethics52569.2021.00011
[10] matthew johnson and alonso vera. 2019. no ai is an island: the case for
teaming intelligence. ai magazine 40, 1 (2019), 16–28. https://doi.org/10.1609/
aimag.v40i1.2842
[11] catholijn m. jonker, m. birna van riemsdijk, and bas vermeulen. 2011. shared
mental models. in coordination, organizations, institutions, and norms in agent
systems vi (lecture notes in computer science), marina de vos, nicoletta fornara,
jeremy v. pitt, and george vouros (eds.). springer, berlin, heidelberg, 132–151.
https://doi.org/10.1007/978-3-642-21268-0_8
[12] ruhul amin khalil, nasir saeed, mudassir masood, yasaman moradi fard,
mohamed-slim alouini, and tareq y. al-naffouri. 2021.
deep learning in
the industrial internet of things: potentials, challenges, and emerging ap-
plications. ieee internet of things journal 8, 14 (2021), 11016–11040.
https:
//doi.org/10.1109/jiot.2021.3051414
[13] franz georg listl, jan fischer, dagmar beyer, and michael weyrich. 2020. knowl-
edge representation in modeling and simulation: a survey for the produc-
tion and logistic domain. in 2020 25th ieee international conference on emerg-
ing technologies and factory automation (etfa), vol. 1. 1051–1056.
https:
//doi.org/10.1109/etfa46521.2020.9211994
[14] jakob mökander, jessica morley, mariarosaria taddeo, and luciano floridi. 2021.
ethics-based auditing of automated decision-making systems: nature, scope,
and limitations. science and engineering ethics 27, 4 (2021), 44. https://doi.org/
10.1007/s11948-021-00319-4
[15] anh nguyen-duc and pekka abrahamsson. 2020. continuous experimentation
on artificial intelligence software: a research agenda. in proceedings of the 28th
acm joint meeting on european software engineering conference and symposium
on the foundations of software engineering (esec/fse 2020). association for
computing machinery, new york, ny, usa, 1513–1516. https://doi.org/10.1145/
3368089.3417039
[16] maximilian nickel, kevin murphy, volker tresp, and evgeniy gabrilovich. 2016.
a review of relational machine learning for knowledge graphs. proc. ieee 104,
1 (2016), 11–33. https://doi.org/10.1109/jproc.2015.2483592
[17] sudip phuyal, diwakar bista, and rabindra bista. 2020. challenges, opportunities
and future directions of smart manufacturing: a state of art review. sustainable
futures 2 (2020), 100023. https://doi.org/10.1016/j.sftr.2020.100023
[18] s pichai. 2018. ai at google: our principles. https://blog.google/technology/ai/ai-
principles/ accessed 2022-01-13.
[19] davy preuveneers, yolande berbers, and wouter joosen. 2016. samurai: a
batch and streaming context architecture for large-scale intelligent applications
and environments. journal of ambient intelligence and smart environments 8, 1
(2016), 63–78. https://doi.org/10.3233/ais-150357
[20] y. j. qu, x. g. ming, z. w. liu, x. y. zhang, and z. t. hou. 2019. smart man-
ufacturing systems: state of the art and future trends. the international jour-
nal of advanced manufacturing technology 103, 9 (2019), 3751–3768.
https:
//doi.org/10.1007/s00170-019-03754-7
[21] martin ringsquandl, e. kharlamov, daria stepanova, marcel hildebrandt, s. lam-
parter, r. lepratti, i. horrocks, and peer kröger. 2018. filling gaps in industrial
knowledge graphs via event-enhanced embedding. in proceedings of iswc 2018
posters & demonstrations, industry and blue sky ideas tracks co-located with 17th
international semantic web conference (iswc 2018). ceur workshop proceedings
(ceur-ws.org). http://ceur-ws.org/vol-2180/paper-52.pdf accessed 2022-01-13.
[22] martin ringsquandl, evgeny kharlamov, daria stepanova, steffen lamparter,
raffaello lepratti, ian horrocks, and peer kröger. 2017. on event-driven knowl-
edge graph completion in digital factories. in 2017 ieee international conference
on big data (big data). 1676–1681. https://doi.org/10.1109/bigdata.2017.8258105
[23] cynthia rudin. 2019. stop explaining black box machine learning models for
high stakes decisions and use interpretable models instead. nature machine
intelligence 1, 5 (2019), 206–215. https://doi.org/10.1038/s42256-019-0048-x
[24] eduardo salas, dana e. sims, and c. shawn burke. 2005. is there a “big five” in
teamwork? small group research 36, 5 (2005), 555–599. https://doi.org/10.1177/
1046496405277134
[25] nuno santos, francisco morais, helena rodrigues, and ricardo j. machado. 2019.
systems development for the industrial iot: challenges from industry r&d
projects. in the internet of things in the industrial sector: security and device
connectivity, smart environments, and industry 4.0, zaigham mahmood (ed.).
springer international publishing, cham, 55–78. https://doi.org/10.1007/978-3-
030-24892-5_3
[26] matthias scheutz, scott a. deloach, and julie a. adams. 2017. a framework for
developing and using shared mental models in human-agent teams. journal
of cognitive engineering and decision making 11, 3 (2017), 203–224.
https:
//doi.org/10.1177/1555343416682891
[27] julien siebert, lisa jöckel, jens heidrich, koji nakamichi, kyoko ohashi, isao
namba, rieko yamamoto, and mikio aoyama. 2020. towards guidelines for
assessing qualities of machine learning systems. in quality of information
and communications technology - 13th international conference, quatic 2020,
faro, portugal, september 9-11, 2020, proceedings (communications in computer
and information science, vol. 1266), martin j. shepperd, fernando brito e abreu,
alberto rodrigues da silva, and ricardo pérez-castillo (eds.). springer, 17–31.
https://doi.org/10.1007/978-3-030-58793-2_2
[28] théo trouillon, christopher r. dance, éric gaussier, johannes welbl, sebastian
riedel, and guillaume bouchard. 2017. knowledge graph completion via complex
tensor factorization. the journal of machine learning research 18, 1 (2017), 4735–
4772.
[29] ville vakkuri, kai-kristian kemell, and pekka abrahamsson. 2019. ethically
aligned design: an empirical evaluation of the resolvedd-strategy in soft-
ware and systems development context. in 2019 45th euromicro conference
on software engineering and advanced applications (seaa). 46–50.
https:
//doi.org/10.1109/seaa.2019.00015
[30] ville vakkuri, kai-kristian kemell, marianna jantunen, erika halme, and pekka
abrahamsson. 2021. eccola — a method for implementing ethically aligned
ai systems. journal of systems and software 182 (2021), 111067. https://doi.org/
10.1016/j.jss.2021.111067
[31] james warren and nathan marz. 2015. big data: principles and best practices of
scalable realtime data systems (1st edition). manning.
"
2202.03541.pdf;"
references
[1] p. langley and h. a. simon, “applications of machine learning and
rule induction,” communications of the acm, vol. 38, no. 11, pp. 54–
64, 1995.
[2] l. e. lwakatare, a. raj, i. crnkovic, j. bosch, and h. h. olsson, “large-
scale machine learning systems in real-world industrial settings: a re-
view of challenges and solutions,” information and software technology,
vol. 127, p. 106368, 2020.
[3] s. garg and s. garg, “automated cloud infrastructure, continuous
integration and continuous delivery using docker with robust container
security,” in 2019 ieee conference on multimedia information process-
ing and retrieval (mipr).
ieee, 2019, pp. 467–470.
[4] y. wu, e. dobriban, and s. davidson, “deltagrad: rapid retraining
of machine learning models,” in international conference on machine
learning.
pmlr, 2020, pp. 10 355–10 366.
[5] d. sculley, g. holt, d. golovin, e. davydov, t. phillips, d. ebner,
v. chaudhary, m. young, j.-f. crespo, and d. dennison, “hidden tech-
nical debt in machine learning systems,” advances in neural information
processing systems, vol. 28, pp. 2503–2511, 2015.
[6] s. alla and s. k. adari, “what is mlops?” in beginning mlops with
mlflow.
springer, 2021, pp. 79–124.
[7] z. wan, x. xia, d. lo, and g. c. murphy, “how does machine
learning change software development practices?” ieee transactions
on software engineering, 2019.
[8] c. t. wolf and d. paine, “sensemaking practices in the everyday work
of ai/ml software engineering,” in proceedings of the ieee/acm 42nd
international conference on software engineering workshops, 2020, pp.
86–92.
[9] “empowering app development for developers — docker,” https://www.
docker.com/, (accessed on 09/13/2021).
[10] g. sayfan, mastering kubernetes.
packt publishing ltd, 2017.
[11] “amazon sagemaker – machine learning – amazon web services,” https:
//aws.amazon.com/sagemaker/, (accessed on 09/13/2021).
[12] “introduction to kubeﬂow — kubeﬂow,” https://www.kubeﬂow.org/docs/
about/kubeﬂow/, (accessed on 09/02/2021).
[13] m. zaharia, a. chen, a. davidson, a. ghodsi, s. a. hong, a. konwin-
ski, s. murching, t. nykodym, p. ogilvie, m. parkhe et al., “accelerat-
ing the machine learning lifecycle with mlﬂow.” ieee data eng. bull.,
vol. 41, no. 4, pp. 39–45, 2018.
[14] “gitops — gitops is continuous deployment for cloud native applica-
tions,” https://www.gitops.tech/, (accessed on 09/02/2021).
[15] c. singh, n. s. gaba, m. kaur, and b. kaur, “comparison of different
ci/cd tools integrated with cloud platform,” in 2019 9th international
conference on cloud computing, data science & engineering (conﬂu-
ence).
ieee, 2019, pp. 7–12.
[16] j. turnbull, the docker book: containerization is the new virtualization.
james turnbull, 2014.
[17] j. ruﬁno, m. alam, j. ferreira, a. rehman, and k. f. tsang, “orchestra-
tion of containerized microservices for iiot using docker,” in 2017 ieee
international conference on industrial technology (icit). ieee, 2017,
pp. 1532–1536.
[18] “mlops:
continuous
delivery
and
automation
pipelines
in
machine
learning,”
https://cloud.google.com/architecture/
mlops-continuous-delivery-and-automation-pipelines-in-machine-learning,
(accessed on 08/13/2021).
[19] i. karamitsos, s. albarhami, and c. apostolopoulos, “applying devops
practices of continuous automation for machine learning,” information,
vol. 11, no. 7, p. 363, 2020.
[20] “detecting data drift with mlops — 10clouds,” https://10clouds.com/
blog/detecting-data-drift-mlops/, (accessed on 09/7/2021).
[21] t. a. limoncelli, “gitops: a path to more self-service it,” communica-
tions of the acm, vol. 61, no. 9, pp. 38–42, 2018.
[22] “gitops — gitops is continuous deployment for cloud native applica-
tions,” https://www.gitops.tech/, (accessed on 09/15/2021).
[23] v. lenarduzzi, f. lomio, s. moreschini, d. taibi, and d. a. tamburri,
“software quality for ai: where we are now?” in international confer-
ence on software quality.
springer, 2021, pp. 43–53.
[24] c. renggli, b. karlaˇs, b. ding, f. liu, k. schawinski, w. wu, and
c. zhang, “continuous integration of machine learning models with
ease. ml/ci: towards a rigorous yet practical treatment,” arxiv preprint
arxiv:1903.00278, 2019.
[25] j. lim, h. lee, y. won, and h. yeon, “mlop lifecycle scheme for
vision-based inspection process in manufacturing,” in 2019 {usenix}
conference on operational machine learning (opml 19), 2019, pp.
9–11.
[26] c. murphy, g. e. kaiser, and m. arias, “a framework for quality
assurance of machine learning applications,” 2006.
[27] j. m. zhang, m. harman, l. ma, and y. liu, “machine learning test-
ing: survey, landscapes and horizons,” ieee transactions on software
engineering, 2020.
[28] t. y. chen, “metamorphic testing: a simple method for alleviating the
test oracle problem,” in 2015 ieee/acm 10th international workshop
on automation of software test.
ieee, 2015, pp. 53–54.
[29] j. wang, l. li, and a. zeller, “better code, better sharing: on the
need of analyzing jupyter notebooks,” in proceedings of the acm/ieee
42nd international conference on software engineering: new ideas and
emerging results, 2020, pp. 53–56.
[30] “quality
assurance
for
machine
learning
models
-
part
1,”
https://blog.sasken.com/quality-assurance-for-machine-learning-models-
part-1-why-quality-assurance-is-critical-for-machine-learning-models,
(accessed on 08/25/2021).
"
2202.05505.pdf;
2202.10169.pdf;"
references
[1] i. d. rubasinghe, d. a. meedeniya, and i. perera, “towards traceability management in continuous integration
with sat-analyzer,” in proceedings of 3rd international conference on communication and information processing
(iccip), tokyo, japan, 2017, pp. 77–81. [online]. available: https://doi.org/10.1145/3162957.3162985
[2] m. senapathi and j. buchan, “devops capabilities, practices, and challenges: insights from a case study,” in
proceedings of the 22nd international conference on evaluation and assessment in software engineering 2018,
2018, pp. 57–67. [online]. available: https://doi.org/10.1145/3210459.3210465
[3] d. meedeniya, i. rubasinghe, and i. perera, “artefact consistency management in devops practice: a
survey,” in tools and techniques for software development in large organizations: emerging research
and opportunities, v. pendyala, ed.
igi global, 2020, ch. 4, pp. 98–129. [online]. available:
http://dx.doi.org/10.4018/978-1-7998-1863-2.ch004
10
nipuni hewage and dulani meedeniya
[4] s. palihawadana, c. wijeweera, n. sanjitha, v. liyanage, i. perera, and d. meedeniya, “tool support
for traceability management of software artefacts with devops practices,” in proceedings of moratuwa
engineering research conference (mercon), moratuwa, sri lanka, 2017, pp. 129–134. [online]. available:
https://doi.org/10.1109/mercon.2017.7980469
[5] i. rubasinghe, d. meedeniya, and i. perera, “traceability management with impact analysis in devops
based software development,” in proceedings of international conference on advances in computing,
communications and informatics (icacci), bangalore, india, 2018, pp. 1956–1962. [online]. available:
https://doi.org/10.1109/icacci.2018.8554399
[6] r. ashmore, r. calinescu, and c. paterson, “assuring the machine learning lifecycle: desiderata, methods, and
challenges,” acm computing surveys, vol. 54, no. 5, 2019. [online]. available: https://doi.org/10.1145/3453444
[7] l. leite, c. rocha, f. kon, d. milojicic, and p. meirelles, “a survey of devops concepts and challenges,” acm
computing surveys, vol. 52, no. 6, 2019.
[8] d. a. meedeniya, i. d. rubasinghe, and i. perera, “software artefacts consistency management towards continuous
integration: a roadmap,” international journal of advanced computer science and applications (ijacsa), vol. 10,
no. 4, pp. 100–110, 2019. [online]. available: http://dx.doi.org/10.14569/ijacsa.2019.0100411
[9] ——, “traceability establishment and visualization of software artefacts in devops practice:
a survey,”
international journal of advanced computer science and applications (ijacsa), vol. 10, no. 7, pp. 66–76, 2019.
[online]. available: http://dx.doi.org/10.14569/ijacsa.2019.0100711
[10] i. rubasinghe, d. meedeniya, and i. perera, “sat-analyser traceability management tool support for devops,”
journal of information processing systems (jips), vol. 17, no. 5, pp. 972–988, 2021. [online]. available:
https://doi.org/10.3745/jips.04.0225
[11] v. manish, “understanding devops & bridging the gap from continuous integration to continuous delivery,” in
proceedings of innovative computing technology (intech), 2015 fifth international conference on., galcia,
spain, 2015. [online]. available: https://doi.org/10.1109/intech.2015.7173368
[12] i. rubasinghe, d. meedeniya, and i. perera, “automated inter-artefact traceability establishment for devops
practice,” in proceedings of 17th international conference on computer and information science (icis),
singapore, singapore, 2018, pp. 211–216. [online]. available: https://doi.org/10.1109/icis.2018.8466414
[13] d. meedeniya and h. thennakoon, “impact factors and best practices to improve effort estimation
strategies and practices in devops,” in proceedings of the 11th international conference on information
communication management (icicm), tokyo,
japan,
2021,
p. 11–17. [online]. available:
https:
//dl.acm.org/doi/10.1145/3484399.3484401
[14] a. paleyes, r.-g. urma, and n. d. lawrence, “challenges in deploying machine learning: a survey of case
studies,” in proceedings of the ml-retrospectives, surveys & meta-analyses workshop, neurips 2020, 2020.
[online]. available: http://arxiv.org/abs/2011.09926
[15] o. spjuth, j. frid, and a. hellander, “the machine learning life cycle and the cloud: implications for drug
discovery,” expert opinion on drug discovery, vol. 16, no. 9, pp. 1071–1079, 2021.
[16] s. amershi, a. begel, c. bird, r. deline, h. gall, e. kamar, n. nagappan, b. nushi, and t. zimmermann,
“software engineering for machine learning: a case study,” in proceedings of ieee/acm international
conference on software engineering: software engineering in practice track (icse-seip), montreal, qc,
canada, 2019, pp. 291 – 300. [online]. available: https://10.1109/icse-seip.2019.00042
[17] m. treveil, n. omont, s. clément, k. lefevre, d. phan, j. zentici, a. lavoillotte, m. miyazaki, and l. heidmann,
introducing mlops.
o’reilly media, inc., 2020.
[18] m. zaharia, a. chen, a. davidson, a. ghodsi, s. ann hong, a. konwinski, s. murching, t. nykodym, p. ogilvie,
m. parkhe, f. xie, and c. zumar, “accelerating the machine learning lifecycle with mlﬂow,” ieee data eng.
bull., vol. 41, pp. 39–45, 2018.
[19] a. banerjee, c.-c. chen, c.-c. hung, x. huang, y. wang, and r. chevesaran, “challenges and
experiences with mlops for performance diagnostics in hybrid-cloud enterprise software deployments,” in
proceedings of 2020 usenix conference on operational machine learning, 2020. [online]. available:
https://www.usenix.org/conference/opml20/presentation/banerjee
11
nipuni hewage and dulani meedeniya
[20] p. ruf, m. madan, c. reich, and d. ould-abdeslam, “demystifying mlops and presenting a recipe
for the selection of open-source tools,” applied sciences, vol. 11, no. 19, 2021. [online]. available:
https://doi.org/10.3390/app11198861
[21] i. rubasinghe, d. meedeniya, and i. perera, “tool support for software artefact traceability in devops
practice: sat-analyser,” in tools and techniques for software development in large organizations: emerging
research and opportunities, v. pendyala, ed.
igi global, 2020, ch. 5, pp. 130–167. [online]. available:
http://dx.doi.org/10.4018/978-1-7998-1863-2.ch005
[22] s. mäkinen, h. skogström, e. laaksonen, and t. mikkonen, “who needs mlops: what data scientists seek to
accomplish and how can mlops help?” in proceedings of 2021 ieee/acm 1st workshop on ai engineering -
software engineering for ai (wain) of 43rd international conference on software engineering (icse), 2021.
[online]. available: http://arxiv.org/abs/2103.08942
[23] g. fursin, h. guillou, and n. essayan, “codereef: an open platform for portable mlops, reusable automation
actions and reproducible benchmarking,” in proceedings of workshop on mlops systems at mlsys’20, 2020.
[24] a. arunthavanathan, s. shanmugathasan, s. ratnavel, v. thiyagarajah, i. perera, d. meedeniya, and
d. balasubramaniam, “support for traceability management of software artefacts using natural language
processing,” in proceedings of moratuwa engineering research conference (mercon), moratuwa, sri lanka,
2016, pp. 18–23. [online]. available: https://doi.org/10.1109/mercon.2016.7480109
12
"
2204.13291.pdf;"
references
1. ahn, j., simeone, o., kang, j.: wireless federated distillation for distributed edge
learning with heterogeneous data. in: pimrc 2019. pp. 1–6 (2019)
16
sk. lo et al.
2. bao, x., su, c., xiong, y., huang, w., hu, y.: flchain: a blockchain for auditable
federated learning with trust and incentive. in: bigcom ’19. pp. 151–159 (2019)
3. bonawitz, k.a., eichner, h., grieskamp, w., huba, d., ingerman, a., ivanov, v.,
kiddon, c.m., koneˇcn´y, j., mazzocchi, s., mcmahan, b., overveldt, t.v., petrou,
d., ramage, d., roselander, j.: towards federated learning at scale: system design.
in: sysml 2019 (2019), to appear
4. caldiera, v.r.b.g., rombach, h.d.: the goal question metric approach. encyclo-
pedia of software engineering pp. 528–532 (1994)
5. jeong, e., oh, s., kim, h., park, j., bennis, m., kim, s.l.: communication-
eﬃcient on-device machine learning: federated distillation and augmentation under
non-iid private data (2018)
6. jobin, a., ienca, m., vayena, e.: the global landscape of ai ethics guidelines.
nature machine intelligence 1(9), 389–399 (2019)
7. kairouz, p., mcmahan, h.b., avent, b., bellet, a., bennis, m., bhagoji, a.n.,
bonawitz, k., charles, z., cormode, g., cummings, r., et al.: advances and open
problems in federated learning. arxiv preprint arxiv:1912.04977 (2019)
8. lewis, g.a., lago, p., avgeriou, p.: a decision model for cyber-foraging systems.
in: 2016 13th working ieee/ifip conference on software architecture (wicsa).
pp. 51–60 (2016)
9. lo, s.k., liew, c.s., tey, k.s., mekhilef, s.: an interoperable component-based
architecture for data-driven iot system. sensors 19(20) (2019)
10. lo, s.k., liu, y., lu, q., wang, c., xu, x., paik, h.y., zhu, l.: towards trust-
worthy ai: blockchain-based architecture design for accountability and fairness of
federated learning systems. ieee internet of things journal pp. 1–1 (2022)
11. lo, s.k., lu, q., paik, h.y., zhu, l.: flra: a reference architecture for feder-
ated learning systems. in: software architecture. pp. 83–98. springer international
publishing (2021)
12. lo, s.k., lu, q., wang, c., paik, h.y., zhu, l.: a systematic literature review
on federated machine learning: from a software engineering perspective. acm
comput. surv. 54(5) (may 2021)
13. lo, s.k., lu, q., zhu, l., paik, h.y., xu, x., wang, c.: architectural patterns for
the design of federated learning systems. arxiv preprint arxiv:2101.02373 (2021)
14. maclean, a., young, r.m., bellotti, v.m., moran, t.p.: questions, options, and
criteria: elements of design space analysis. human–computer interaction 6(3-4),
201–250 (1991)
15. mcmahan,
h.b.,
moore,
e.,
ramage,
d.,
hampson,
s.,
y
arcas,
b.a.:
communication-eﬃcient learning of deep networks from decentralized data (2017)
16. roy, a.g., siddiqui, s., p¨olsterl, s., navab, n., wachinger, c.: braintorrent: a
peer-to-peer environment for decentralized federated learning (2019)
17. sattler, f., wiedemann, s., m¨uller, k., samek, w.: robust and communication-
eﬃcient federated learning from non-i.i.d. data. ieee transactions on neural net-
works and learning systems pp. 1–14 (2019)
18. thiebes, s., lins, s., sunyaev, a.: trustworthy artiﬁcial intelligence. electronic
markets (2020). https://doi.org/10.1007/s12525-020-00441-4
19. xu, x., dilum bandara, h., lu, q., weber, i., bass, l., zhu, l.: a decision model
for choosing patterns in blockchain-based applications. in: 2021 ieee 18th inter-
national conference on software architecture (icsa). pp. 47–57 (2021)
20. zhang, w., lu, q., yu, q., li, z., liu, y., lo, s.k., chen, s., xu, x., zhu, l.:
blockchain-based federated learning for device failure detection in industrial iot.
ieee internet things j. pp. 1–12 (2020)
"
2205.02302.pdf;"
references 
 
[1] 
muratahan aykol, patrick herring, and abraham anapolsky. 2020. 
machine learning for continuous innovation in battery technologies. nat. 
rev. mater. 5, 10 (2020), 725–727. 
[2] 
lucas baier, fabian jöhren, and stefan seebacher. 2020. challenges in the 
deployment and operation of machine learning in practice. 27th eur. conf. 
inf. syst. - inf. syst. a shar. soc. ecis 2019 (2020), 0–15. 
[3] 
lucas baier, niklas kühl, and gerhard satzger. 2019. how to cope with 
change? preserving validity of predictive services over time. in hawaii 
international conference on system sciences (hicss-52), grand wailea, 
maui, hawaii, usa. 
[4] 
amitabha banerjee, chien chia chen, chien chun hung, xiaobo huang, 
yifan wang, and razvan chevesaran. 2020. challenges and experiences 
with mlops for performance diagnostics in hybrid-cloud enterprise 
software deployments. opml 2020 - 2020 usenix conf. oper. mach. 
learn. (2020), 7–9. 
[5] 
kent beck, mike beedle, arie van bennekum, alistair cockburn, ward 
cunningham, martin fowler, james grenning, jim highsmith, andrew 
hunt, ron jeffries, jon kern, brian marick, robert c. martin, steve 
mellor, ken schwaber, jeff sutherland, and dave thomas. 2001. 
manifesto for agile software development. (2001). 
[6] 
benjamin benni, blay fornarino mireille, mosser sebastien, precisio 
frederic, and jungbluth gunther. 2019. when devops meets meta-
learning: a portfolio to rule them all. proc. - 2019 acm/ieee 22nd int. 
conf. model driven eng. lang. syst. companion, model. 2019 (2019), 
605–612. doi:https://doi.org/10.1109/models-c.2019.00092 
[7] 
lucas cardoso silva, fernando rezende zagatti, bruno silva sette, lucas 
nildaimon dos santos silva, daniel lucredio, diego furtado silva, and 
helena de medeiros caseli. 2020. benchmarking machine learning 
solutions in production. proc. - 19th ieee int. conf. mach. learn. appl. 
icmla 
2020 
(2020), 
626–633. 
doi:https://doi.org/10.1109/icmla51294.2020.00104 
[8] 
juliet m. corbin and anselm strauss. 1990. grounded theory research: 
procedures, canons, and evaluative criteria. qual. sociol. 13, 1 (1990), 3–
21. doi:https://doi.org/10.1007/bf00988593 
[9] 
patrick debois. 2009. patrick debois - devopsdays ghent. retrieved march 
25, 2021 from https://devopsdays.org/events/2019-ghent/speakers/patrick-
debois/ 
[10] 
behrouz derakhshan, alireza rezaei mahdiraji, tilmann rabl, and volker 
markl. 2019. continuous deployment of machine learning pipelines. adv. 
database 
technol. 
- 
edbt 
2019-march, 
(2019), 
397–408. 
doi:https://doi.org/10.5441/002/edbt.2019.35 
[11] 
grigori fursin. 2021. collective knowledge: organizing research projects 
as a database of reusable components and portable workflows with 
common interfaces. philos. trans. a. math. phys. eng. sci. 379, 2197 
(2021), 20200211. doi:https://doi.org/10.1098/rsta.2020.0211 
[12] 
barney glaser and anselm strauss. 1967. the discovery of grounded 
theory: strategies for qualitative research.  
[13] 
mahendra kumar gourisaria, rakshit agrawal, g m harshvardhan, 
manjusha pandey, and siddharth swarup rautaray. 2021. application of 
machine learning in industry 4.0. in machine learning: theoretical 
foundations and practical applications. springer, 57–87. 
[14] 
akshita goyal. 2020. mlops - machine learning operations. int. j. inf. 
technol. insights transform. (2020). retrieved april 15, 2021 from 
http://technology.eurekajournals.com/index.php/ijitit/article/view/655/7
69 
[15] 
tuomas granlund, aleksi kopponen, vlad stirbu, lalli myllyaho, and 
tommi mikkonen. 2021. mlops challenges in multi-organization setup: 
experiences from two real-world cases. (2021). retrieved from 
http://arxiv.org/abs/2103.08937 
[16] 
willem jan van den heuvel and damian a. tamburri. 2020. model-driven 
ml-ops for intelligent enterprise applications: vision, approaches and 
challenges. 
springer 
international 
publishing. 
doi:https://doi.org/10.1007/978-3-030-52306-0_11 
[17] 
ioannis karamitsos, saeed albarhami, and charalampos apostolopoulos. 
2020. applying devops practices of continuous automation for machine 
learning. 
inf. 
11, 
7 
(2020), 
1–15. 
doi:https://doi.org/10.3390/info11070363 
[18] 
bojan karlaš, matteo interlandi, cedric renggli, wentao wu, ce zhang, 
deepak mukunthu iyappan babu, jordan edwards, chris lauren, andy 
xu, and markus weimer. 2020. building continuous integration services 
for machine learning. proc. acm sigkdd int. conf. knowl. discov. 
data 
min. 
(2020), 
2407–2415. 
doi:https://doi.org/10.1145/3394486.3403290 
[19] 
rupesh raj karn, prabhakar kudva, and ibrahim abe m. elfadel. 2019. 
dynamic autoselection and autotuning of machine learning models for 
cloud network analytics. ieee trans. parallel distrib. syst. 30, 5 (2019), 
1052–1064. doi:https://doi.org/10.1109/tpds.2018.2876844 
[20] 
barbara kitchenham, o. pearl brereton, david budgen, mark turner, john 
bailey, and stephen linkman. 2009. systematic literature reviews in 
software engineering - a systematic literature review. inf. softw. technol. 
51, 1 (2009), 7–15. doi:https://doi.org/10.1016/j.infsof.2008.09.009 
[21] 
rafal kocielnik, saleema amershi, and paul n bennett. 2019. will you 
accept an imperfect ai? exploring designs for adjusting end-user 
expectations of ai systems. in proceedings of the 2019 chi conference on 
human factors in computing systems, 1–14. 
[22] 
ana de las heras, amalia luque-sendra, and francisco zamora-polo. 
2020. machine learning technologies for sustainability in smart cities in the 
post-covid era. sustainability 12, 22 (2020), 9320. 
[23] 
leonardo leite, carla rocha, fabio kon, dejan milojicic, and paulo 
meirelles. 2019. a survey of devops concepts and challenges. acm 
comput. surv. 52, 6 (2019). doi:https://doi.org/10.1145/3359981 
[24] 
yan liu, zhijing ling, boyu huo, boqian wang, tianen chen, and esma 
mouine. 2020. building a platform for machine learning operations from 
open source frameworks. ifac-papersonline 53, 5 (2020), 704–709. 
doi:https://doi.org/10.1016/j.ifacol.2021.04.161 
[25] 
alvaro lopez garcia, viet tran, andy s. alic, miguel caballer, isabel 
campos plasencia, alessandro costantini, stefan dlugolinsky, doina 
cristina duma, giacinto donvito, jorge gomes, ignacio heredia cacha, 
jesus marco de lucas, keiichi ito, valentin y. kozlov, giang nguyen, 
pablo orviz fernandez, zdenek sustr, pawel wolniewicz, marica 
antonacci, wolfgang zu castell, mario david, marcus hardt, lara lloret 
iglesias, germen molto, and marcin plociennik. 2020. a cloud-based 
framework for machine learning workloads and applications. ieee access 
8, 
(2020), 
18681–18692. 
doi:https://doi.org/10.1109/access.2020.2964386 
[26] 
lwakatare. 2020. from a data science driven process to a continuous 
delivery process for machine learning systems. lect. notes comput. sci. 
(including subser. lect. notes artif. intell. lect. notes bioinformatics) 
12562 lncs, (2020), 185–201. doi:https://doi.org/10.1007/978-3-030-
64148-1_12 
[27] 
lwakatare. 2020. devops for ai - challenges in development of ai-
enabled 
applications. 
(2020). 
doi:https://doi.org/10.23919/softcom50211.2020.9238323 
[28] 
ruth w. macarthy and julian m. bass. 2020. an empirical taxonomy of 
devops in practice. in 2020 46th euromicro conference on software 
engineering and advanced applications (seaa), ieee, 221–228. 
doi:https://doi.org/10.1109/seaa51224.2020.00046 
[29] 
sasu mäkinen, henrik skogström, eero laaksonen, and tommi 
mikkonen. 2021. who needs mlops: what data scientists seek to 
accomplish and how can mlops help? ml (2021). retrieved from 
http://arxiv.org/abs/2103.08942 
[30] 
rob van der meulen and thomas mccall. 2018. gartner says nearly half 
of cios are planning to deploy artificial intelligence. retrieved 
december 4, 2021 from https://www.gartner.com/en/newsroom/press-
releases/2018-02-13-gartner-says-nearly-half-of-cios-are-planning-to-
deploy-artificial-intelligence 
[31] 
steve mezak. 2018. the origins of devops: what’s in a name? - 
devops.com. retrieved march 25, 2021 from https://devops.com/the-
origins-of-devops-whats-in-a-name/ 
[32] 
antonio molner domenech and alberto guillén. 2020. ml-experiment: a 
python framework for reproducible data science. j. phys. conf. ser. 1603, 
1 (2020). doi:https://doi.org/10.1088/1742-6596/1603/1/012025 
[33] 
michael d. myers and michael newman. 2007. the qualitative interview 
in is research: examining the craft. inf. organ. 17, 1 (2007), 2–26. 
doi:https://doi.org/10.1016/j.infoandorg.2006.11.001 
[34] 
pulasthi perera, roshali silva, and indika perera. 2017. improve software 
quality through practicing devops. in 2017 seventeenth international 
conference on advances in ict for emerging regions (icter), 1–6. 
[35] 
alexandra posoldova. 2020. machine learning pipelines: from research 
to production. ieee potentials (2020). 
[36] 
cedric renggli, luka rimanic, nezihe merve gürel, bojan karlaš, 
wentao wu, and ce zhang. 2021. a data quality-driven view of mlops. 
1 (2021), 1–12. retrieved from http://arxiv.org/abs/2102.07750 
[37] 
winston w. royce. 1970. managing the development of large software 
systems. (1970). 
[38] 
martin rütz. 2019. devops: a systematic literature 
review. inf. softw. technol. (2019). 
[39] 
ulrike schultze and michel avital. 2011. designing interviews to generate 
rich data for information systems research. inf. organ. 21, 1 (2011), 1–16. 
doi:https://doi.org/10.1016/j.infoandorg.2010.11.001 
[40] 
ola spjuth, jens frid, and andreas hellander. 2021. the machine learning 
life cycle and the cloud: implications for drug discovery. expert opin. 
drug 
discov. 
00, 
00 
(2021), 
1–9. 
doi:https://doi.org/10.1080/17460441.2021.1932812 
mlops 
kreuzberger, kühl, and hirschl 
 
 
[41] 
damian a. tamburri. 2020. sustainable mlops: trends and challenges. 
proc. - 2020 22nd int. symp. symb. numer. algorithms sci. comput. 
synasc 
2020 
(2020), 
17–23. 
doi:https://doi.org/10.1109/synasc51798.2020.00015 
[42] 
chandrasekar vuppalapati, anitha ilapakurti, karthik chillara, sharat 
kedari, and vanaja mamidi. 2020. automating tiny ml intelligent 
sensors devops using microsoft azure. proc. - 2020 ieee int. conf. big 
data, 
big 
data 
2020 
(2020), 
2375–2384. 
doi:https://doi.org/10.1109/bigdata50022.2020.9377755 
[43] 
jane webster and richard watson. 2002. analyzing the past to prepare for 
the future: writing a literature review. mis q. 26, 2 (2002), xiii–xxiii. 
doi:https://doi.org/10.1.1.104.6570 
[44] 
chaoyu wu, e. haihong, and meina song. 2020. an automatic artificial 
intelligence training platform based on kubernetes. acm int. conf. 
proceeding 
ser. 
(2020), 
58–62. 
doi:https://doi.org/10.1145/3378904.3378921 
[45] 
geum seong yoon, jungsu han, seunghyung lee, and jong won kim. 
2020. devops portal design for smartx ai cluster employing cloud-
native machine learning workflows. springer international publishing. 
doi:https://doi.org/10.1007/978-3-030-39746-3_54 
[46] 
yue zhou, yue yu, and bo ding. 2020. towards mlops: a case study 
of ml pipeline platform. proc. - 2020 int. conf. artif. intell. comput. eng. 
icaice 
2020 
(2020), 
494–500. 
doi:https://doi.org/10.1109/icaice51518.2020.00102 
 
 
 
mlops: overview, definition, and architecture 
kreuzberger, kühl, and hirschl 
 
 
 
appendix 
table 1. list of evaluated technologies 
 
technology 
name 
description 
sources 
open-source 
examples 
tensorflow 
extended 
tensorflow extended (tfx) is a configuration framework 
providing libraries for each of the tasks of an end-to-end ml 
pipeline. examples are data validation, data distribution 
checks, model training, and model serving. 
[7,10,26,46] [δ, θ]  
airflow 
airflow is a task and workflow orchestration tool, which can 
also be used for ml workflow orchestration. it is also used 
for orchestrating data engineering jobs. tasks are executed 
according to directed acyclic graphs (dags). 
[26,40,41] [α, β, ζ, η] 
kubeflow 
kubeflow is a kubernetes-based end-to-end ml platform. 
each kubeflow component is wrapped into a container and 
orchestrated by kubernetes. also, each task of an ml 
workflow pipeline is handled with one container. 
[26,35,40,41,46] [α, β, γ, δ, ζ, η, θ] 
 
 
mlflow 
mlflow is an ml platform that allows for the management 
of the ml lifecycle end-to-end. it provides an advanced 
experiment tracking functionality, a model registry, and 
model serving component. 
[11,32,35] [α, γ, ε, ζ, η, θ] 
 
 
commercial 
examples 
databricks 
managed 
mlflow 
the databricks platform offers managed services based on 
other cloud providers’ infrastructure, e.g., managed 
mlflow. 
[26,32,35,40] [α, ζ] 
amazon 
codepipeline 
amazon codepipeline is a ci/cd automation tool to 
facilitate the build, test, and delivery steps. it also allows one 
to schedule and manage the different stages of an ml 
pipeline. 
[18] [γ] 
amazon 
sagemaker 
with sagemaker, amazon aws offers an end-to-end ml 
platform. it provides, out-of-the-box, a feature store, 
orchestration with sagemaker pipelines, and model serving 
with sagemaker endpoints. 
[7,11,18,24,35] [α, β, γ, ζ, θ] 
 
azure devops 
pipelines 
azure devops pipelines is a ci/cd automation tool to 
facilitate the build, test, and delivery steps. it also allows one 
to schedule and manage the different stages of an ml 
pipeline. 
[18,42] [γ, ε] 
azure ml 
microsoft azure offers, in combination with azure devops 
pipelines and azure ml, an end-to-end ml platform. 
[6,24,25,35,42] [α, γ, ε, ζ, η, θ] 
mlops 
kreuzberger, kühl, and hirschl 
 
 
gcp - vertex 
ai 
gcp offers, along with vertex ai, a fully managed end-to-
end platform. in addition, they offer a managed kubernetes 
cluster with kubeflow as a service. 
[25,35,40,41] [α, γ, δ, ζ, θ] 
ibm cloud 
pak for data 
(ibm watson 
studio) 
ibm cloud pak for data combines a list of software in a 
package that offers data and ml capabilities. 
[41] [γ] 
 
table 2. list of interview partners 
interviewee 
pseudonym 
job title 
years of 
experience with 
devops 
years of 
experience with 
ml 
industry 
company size 
(number of 
employees) 
alpha (α) 
senior data platform 
engineer  
3 
4 
sporting goods / retail 
60,000 
beta (β) 
solution architect / 
specialist for ml and ai 
6 
10 
it services / cloud 
provider / cloud 
computing 
25,000 
gamma (γ) 
ai architect / consultant  
5 
7 
cloud provider 
350,000 
delta (δ) 
technical marketing & 
manager in ml / ai 
10 
5 
cloud provider 
139,995 
epsilon (ε) 
technical architect - data 
& ai 
1 
2 
cloud provider 
160,000 
zeta (ζ) 
ml engineering 
consultant 
5 
6 
consulting company 
569,000 
eta (η) 
engineering manager in 
ai / senior deep learning 
engineer  
10 
10 
conglomerate (multi-
industry) 
400,000 
theta (θ) 
ml platform product 
lead 
8 
10 
music / audio streaming 
6,500 
 
mlops: overview, definition, and architecture 
kreuzberger, kühl, and hirschl 
 
 
 
 
figure 5. intersection of disciplines of the mlops paradigm 
22/04/22
machine
learning
software
engineering
data engineering
mlops
cd4ml
ml model
data
1
ci/cd
pipeline 
code
devops
"
2205.14664.pdf;"
references
[1] a. boroumand et al., “google workloads for consumer devices: miti-
gating data movement bottlenecks,” in asplos, 2018.
[2] o. mutlu, “memory scaling: a systems architecture perspective,” in
imw, 2013.
[3] g. f. oliveira et al., “damov: a new methodology and benchmark
suite for evaluating data movement bottlenecks,” arxiv:2105.03725
[cs.ar], 2021.
[4] g. f. oliveira et al., “damov: a new methodology and benchmark
suite for evaluating data movement bottlenecks,” ieee access, 2021.
[5] s. ghose et al., “the processing-in-memory paradigm: mechanisms to
enable adoption,” in beyond-cmos technologies for next generation
computer design, 2019.
[6] o. mutlu et al., “a modern primer on processing in memory,”
arxiv:2012.03112 [cs.ar], 2021.
[7] s. ghose et al., “processing-in-memory: a workload-driven perspec-
tive,” ibm jrd, 2019.
[8] o. mutlu et al., “enabling practical processing in and near memory for
data-intensive computing,” in dac, 2019.
[9] o. mutlu and l. subramanian, “research problems and opportunities in
memory systems,” superfri, 2014.
[10] o. mutlu, “intelligent architectures for intelligent computing systems,”
in date, 2021.
[11] o. mutlu, “intelligent architectures for intelligent machines,” in vlsi-
dat, 2020.
[12] o. mutlu, “main memory scaling: challenges and solution directions,”
in more than moore technologies for next generation computer de-
sign.
springer-verlag, 2015.
[13] f. devaux, “the true processing in memory accelerator,” in hcs, 2019.
[14] j. ahn et al., “pim-enabled instructions: a low-overhead, locality-
aware processing-in-memory architecture,” in isca, 2015.
[15] j. ahn et al., “a scalable processing-in-memory accelerator for parallel
graph processing,” in isca, 2015.
[16] l. nai et al., “graphpim: enabling instruction-level pim ofﬂoading in
graph computing frameworks,” in hpca, 2017.
[17] l. song et al., “graphr: accelerating graph processing using reram,”
in hpca, 2018.
[18] a. boroumand et al., “lazypim: an efﬁcient cache coherence mecha-
nism for processing-in-memory,” ieee cal, 2017.
3
[19] a. boroumand et al., “conda: efﬁcient cache coherence support for
near-data accelerators,” in isca, 2019.
[20] m. zhang et al., “graphp: reducing communication for pim-based
graph processing with efﬁcient data partition,” in hpca, 2018.
[21] s. angizi et al., “graphs: a graph processing accelerator leveraging
sot-mram,” in date, 2019.
[22] s. angizi and d. fan, “graphide: a graph processing accelerator
leveraging in-dram-computing,” in glsvlsi, 2019.
[23] y. zhuo et al., “graphq: scalable pim-based graph processing,” in
micro, 2019.
[24] g. dai et al., “graphh: a processing-in-memory architecture for large-
scale graph processing,” ieee tcad, 2018.
[25] y. huang et al., “a heterogeneous pim hardware-software co-design
for energy-efﬁcient graph processing,” in ipdps, 2020.
[26] m. besta et al., “sisa: set-centric instruction set architecture for graph
mining on processing-in-memory systems,” in micro, 2021.
[27] l. nai and h. kim, “instruction ofﬂoading with hmc 2.0 standard: a
case study for graph traversals,” in memsys, 2015.
[28] g. dai et al., “graphsar: a sparsity-aware processing-in-memory
architecture for large-scale graph processing on rerams,” in asp-
dac, 2019.
[29] s. li et al., “pinatubo: a processing-in-memory architecture for bulk
bitwise operations in emerging non-volatile memories,” in dac, 2016.
[30] l. han et al., “a novel reram-based processing-in-memory architec-
ture for graph traversal,” tos, 2018.
[31] y. zhuo et al., “distributed graph processing system and processing-in-
memory architecture with precise loop-carried dependency guaran-
tee,” tocs, 2021.
[32] e. azarkhish et al., “design and evaluation of a processing-in-memory
architecture for the smart memory cube,” in arcs, 2016.
[33] x. xie et al., “spacea: sparse matrix vector multiplication on
processing-in-memory accelerator,” in hpca, 2021.
[34] m. imani et al., “digitalpim: digital-based processing in-memory for
big data acceleration,” in glsvlsi, 2019.
[35] l. han et al., “a novel reram-based processing-in-memory architec-
ture for graph computing,” in nvmsa, 2017.
[36] m. zhou et al., “gram: graph processing in a reram-based compu-
tational memory,” in asp-dac, 2019.
[37] l. zheng et al., “spara: an energy-efﬁcient reram-based accelerator
for sparse graph analytics applications,” in ipdps, 2020.
[38] h. liu et al., “regra: accelerating graph traversal applications using
reram with lower communication cost,” ieee access, 2020.
[39] g. li et al., “graphia: an in-situ accelerator for large-scale graph
processing,” in memsys, 2018.
[40] e. kim and h. kim, “things to consider to enable dynamic graphs in
processing-in-memory,” in memsys, 2020.
[41] m. gao et al., “tetris: scalable and efﬁcient neural network acceler-
ation with 3d memory,” in asplos, 2017.
[42] d. kim et al., “neurocube: a programmable digital neuromorphic
architecture with high-density 3d memory,” in isca, 2016.
[43] a. shaﬁee et al., “isaac: a convolutional neural network accelerator
with in-situ analog arithmetic in crossbars,” in isca, 2016.
[44] p. chi et al., “prime: a novel processing-in-memory architecture
for neural network computation in reram-based main memory,” in
isca, 2016.
[45] a. boroumand et al., “mitigating edge machine learning inference
bottlenecks: an empirical study on accelerating google edge models,”
arxiv:2103.00768 [cs.ar], 2021.
[46] a. boroumand, “practical mechanisms for reducing processor-memory
data movement in modern workloads,” ph.d. dissertation, carnegie
mellon university, 2020.
[47] s. lee et al., “a 1ynm 1.25v 8gb, 16gb/s/pin gddr6-based
accelerator-in-memory supporting 1tflops mac operation and var-
ious activation functions for deep-learning applications,” in isscc,
2022.
[48] y.-c. kwon et al., “25.4 a 20nm 6gb function-in-memory dram,
based on hbm2 with a 1.2 tflops programmable computing unit
using bank-level parallelism, for machine learning applications,” in
isscc, 2021.
[49] s. lee et al., “hardware architecture and software stack for pim based
on commercial dram technology: industrial product,” in isca, 2021.
[50] l. ke et al., “near-memory processing in action: accelerating person-
alized recommendation with axdimm,” ieee micro, 2021.
[51] d. niu et al., “184qps/w 64mb/mm2 3d logic-to-dram hybrid bond-
ing with process-near-memory engine for recommendation system,”
in isscc, 2022.
[52] z. he et al., “sparse bd-net: a multiplication-less dnn with sparse
binarized depth-wise separable convolution,” jetc, 2020.
[53] m. peemen et al., “memory-centric accelerator design for convolu-
tional neural networks,” in iccd, 2013.
[54] s. angizi et al., “imce: energy-efﬁcient bitwise in-memory convolu-
tion engine for deep neural network,” in asp-dac, 2018.
[55] q. deng et al., “dracc: a dram based accelerator for accurate cnn
inference,” in dac, 2018.
[56] c. eckert et al., “neural cache: bit-serial in-cache acceleration of
deep neural networks,” in isca, 2018.
[57] m. imani et al., “floatpim: in-memory acceleration of deep neural
network training with high precision,” in isca, 2019.
[58] s. cho et al., “mcdram v2: in-dynamic random access memory
systolic array accelerator to address the large model problem in
deep neural networks on the edge,” ieee access, 2020.
[59] h. shin et al., “mcdram: low latency and energy-efﬁcient matrix
computations in dram,” ieee tcadics, 2018.
[60] n. hajinazar et al., “simdram: a framework for bit-serial simd
processing using dram,” in asplos, 2021.
[61] l. ke et al., “recnmp: accelerating personalized recommendation
with near-memory processing,” in isca, 2020.
[62] j. park et al., “trim: enhancing processor-memory interfaces with
scalable tensor reduction in memory,” in micro, 2021.
[63] y. wang et al., “rerec: in-reram acceleration with access-aware
mapping for personalized recommendation,” in iccad, 2021.
[64] m. wilkening et al., “recssd: near data processing for solid state
drive based recommendation inference,” in asplos, 2021.
[65] l. huang et al., “practical near-data-processing architecture for large-
scale distributed graph neural network,” ieee access, 2022.
[66] g. yuan et al., “forms: fine-grained polarized reram-based in-situ
computation for mixed-signal dnn accelerator,” in isca, 2021.
[67] y. huang et al., “accelerating graph convolutional networks using
crossbar-based processing-in-memory architectures,” in hpca, 2022.
[68] m. imani et al., “dual: acceleration of clustering algorithms using
digital-based processing in-memory,” in micro, 2020.
[69] s. resch et al., “mouse: inference in non-volatile memory for energy
harvesting applications,” in micro, 2020.
[70] y. kwon et al., “tensordimm: a practical near-memory processing
architecture for embeddings and tensor operations in deep learning,”
in micro, 2019.
[71] h. kim et al., “nand-net: minimizing computational complexity of
in-memory processing for binary neural networks,” in hpca, 2019.
[72] t. cao et al., “performance analysis of convolutional neural network
using multi-level memristor crossbar for edge computing,” in icoias,
2020.
[73] a. v. nori et al., “reduct: keep it close, keep it cool!: efﬁcient scal-
ing of dnn inference on multi-core cpus with near-cache compute,”
in isca, 2021.
[74] p. das et al., “towards near data processing of convolutional neural
networks,” in vlsid, 2018.
[75] y. long et al., “reram-based processing-in-memory architecture for
recurrent neural network acceleration,” tvlsi, 2018.
[76] a. s. rakin et al., “pim-tgan: a processing-in-memory accelerator
for ternary generative adversarial networks,” in iccd, 2018.
[77] y. long et al., “q-pim: a genetic algorithm based flexible dnn quan-
tization method and application to processing-in-memory platform,” in
dac, 2020.
[78] j. s. kim et al., “grim-filter: fast seed location filtering in dna read
mapping using processing-in-memory technologies,” bmc genomics,
2018.
4
[79] j. s. kim et al., “grim-filter: fast seed location filtering in dna
read mapping using processing-in-memory technologies,” in psb,
2018.
[80] j. kim et al., “genome read in-memory (grim) filter: fast location
filtering in dna read mapping using emerging memory technologies,”
in psb, 2017.
[81] j. kim et al., “genome read in-memory (grim) filter: fast location
filtering in dna read mapping with emerging memory technologies,”
in recomb, 2016.
[82] g. f. oliveira et al., “nim: an hmc-based machine for neuron com-
putation,” in arc, 2017.
[83] d. s. cali et al., “genasm: a high-performance, low-power approxi-
mate string matching acceleration framework for genome sequence
analysis,” in micro, 2020.
[84] n. m. ghiasi et al., “genstore: a high-performance and energy-
efﬁcient in-storage computing system for genome sequence analysis,”
in asplos, 2022.
[85] d. s. cali et al., “segram: a universal hardware accelerator for ge-
nomic sequence-to-graph and sequence-to-sequence mapping,” in
isca, 2022.
[86] s. angizi et al., “aligns: a processing-in-memory accelerator for dna
short read alignment leveraging sot-mram,” in dac, 2019.
[87] s. gupta et al., “rapid: a reram processing in-memory architecture
for dna sequence alignment,” in islped, 2019.
[88] x.-q. li et al., “pim-align: a processing-in-memory architecture for
fm-index search algorithm,” jcst, 2021.
[89] s. angizi et al., “exploring dna alignment-in-memory leveraging
emerging sot-mram,” in glsvlsi, 2020.
[90] z. i. chowdhury et al., “a dna read alignment accelerator based on
computational ram,” jxcdc, 2020.
[91] s. angizi et al., “pim-aligner: a processing-in-mram platform for
biological sequence alignment,” in date, 2020.
[92] r. kaplan et al., “bioseal: in-memory biological sequence alignment
accelerator for large-scale genomic data,” in systor, 2020.
[93] f. zhang et al., “pim-quantiﬁer: a processing-in-memory platform for
mrna quantiﬁcation,” in dac, 2021.
[94] f. chen et al., “parc: a processing-in-cam architecture for genomic
long read pairwise alignment using reram,” in asp-dac, 2020.
[95] g. singh et al., “fpga-based near-memory acceleration of modern
data-intensive applications,” ieee micro, 2021.
[96] m. alser et al., “accelerating genome analysis: a primer on an ongoing
journey,” ieee micro, 2020.
[97] a. nag et al., “gencache: leveraging in-cache operators for efﬁcient
sequence alignment,” in micro, 2019.
[98] m. zhou et al., “ultra efﬁcient acceleration for de novo genome
assembly via near-memory computing,” in pact, 2021.
[99] l. wu et al., “sieve: scalable in-situ dram-based accelerator designs
for massively parallel k-mer matching,” in isca, 2021.
[100] s. xu et al., “aquoman: an analytic-query ofﬂoading machine,” in
micro, 2020.
[101] g. singh et al., “nero: a near high-bandwidth memory stencil ac-
celerator for weather prediction modeling,” in fpl, 2020.
[102] a. denzler et al., “casper: accelerating stencil computation using
near-cache processing,” arxiv:2112.14216 [cs.ar], 2021.
[103] e. vermij et al., “boosting the efﬁciency of hpcg and graph500 with
near-data processing,” in icpp, 2017.
[104] c. giannoula et al., “syncron: efﬁcient synchronization support for
near-data-processing architectures,” in hpca, 2021.
[105] i. fernandez et al., “natsa: a near-data processing accelerator for
time series analysis,” in iccd, 2020.
[106] g. singh et al., “napel: near-memory computing application perfor-
mance prediction via ensemble learning,” in dac, 2019.
[107] z. liu et al., “concurrent data structures for near-memory computing,”
in spaa, 2017.
[108] a. pattnaik et al., “scheduling techniques for gpu architectures with
processing-in-memory capabilities,” in pact, 2016.
[109] k. hsieh et al., “transparent ofﬂoading and mapping (tom): enabling
programmer-transparent near-data processing in gpu systems,” in
isca, 2016.
[110] b. gu et al., “biscuit: a framework for near-data processing of big
data workloads,” in isca, 2016.
[111] c. xie et al., “processing-in-memory enabled graphics processors for
3d rendering,” in hpca, 2017.
[112] f. wang et al., “reram-based processing-in-memory architecture for
blockchain platforms,” in asp-dac, 2019.
[113] m. drumond et al., “the mondrian data engine,” in isca, 2017.
[114] p. c. santos et al., “operand size reconﬁguration for big data process-
ing in memory,” in date, 2017.
[115] v. seshadri et al., “ambit: in-memory accelerator for bulk bitwise
operations using commodity dram technology,” in micro, 2017.
[116] k. hsieh et al., “accelerating pointer chasing in 3d-stacked memory:
challenges, mechanisms, evaluation,” in iccd, 2016.
[117] a. boroumand et al., “polynesia: enabling effective hybrid transac-
tional/analytical databases with specialized hardware/software co-
design,” arxiv:2103.00798 [cs.ar], 2021.
[118] v. seshadri and o. mutlu, “in-dram bulk bitwise execution engine,”
arxiv:1905.09822 [cs.ar], 2019.
[119] v. seshadri and o. mutlu, “the processing using memory paradigm:
in-dram bulk copy,
initialization,
bitwise and and or,”
arxiv:1610.09603 [cs:ar], 2016.
[120] v. seshadri and o. mutlu, “simple operations in memory to reduce
data movement,” in advances in computers, volume 106, 2017.
[121] v. seshadri et al., “buddy-ram: improving the performance and efﬁ-
ciency of bulk bitwise operations using dram,” arxiv:1611.09988
[cs:ar], 2016.
[122] m. gao et al., “practical near-data processing for in-memory analytics
frameworks,” in pact, 2015.
[123] s. l. xi et al., “beyond the wall: near-data processing for databases,”
in damon, 2015.
[124] s. lu et al., “agile query processing in statistical databases: a process-
in-memory approach,” in ksem, 2019.
[125] d. g. tomé et al., “hipe: hmc instruction predication extension ap-
plied on database processing,” in date, 2018.
[126] t. r. kepe et al., “database processing-in-memory: an experimental
study,” proc. vldb endow., 2019.
[127] y. sun et al., “bidirectional database storage and sql query exploiting
rram-based process-in-memory structure,” tos, 2018.
[128] d. lee et al., “optimizing data movement with near-memory accelera-
tion of in-memory dbms,” in edbt, 2020.
[129] o. o. babarinsa and s. idreos, “jafar: near-data processing for
databases,” in sigmod, 2015.
[130] b. lekshmi and k. meyer-wegener, “coprao: a capability aware
query optimizer for reconﬁgurable near data processors,” in icdew,
2021.
[131] p. gu et al., “leveraging 3d technologies for hardware security: op-
portunities and challenges,” in glsvlsi, 2016.
[132] j. s. kim et al., “d-range: using commodity dram devices to gen-
erate true random numbers with low latency and high throughput,”
in hpca, 2019.
[133] j. s. kim et al., “the dram latency puf: quickly evaluating physical
unclonable functions by exploiting the latency-reliability tradeoff in
modern commodity dram devices,” in hpca, 2018.
[134] w. xiong et al., “secndp: secure near-data processing with untrusted
memory,” in hpca, 2022.
[135] h. nejatollahi et al., “cryptopim: in-memory acceleration for lattice-
based cryptographic hardware,” in dac, 2020.
[136] d. reis et al., “computing-in-memory for performance and energy-
efﬁcient homomorphic encryption,” tvlsi, 2020.
[137] w. li et al., “leveraging memory pufs and pim-based encryption to
secure edge deep learning systems,” in vts, 2019.
[138] a. o. glova et al., “near-data acceleration of privacy-preserving
biomarker search with 3d-stacked memory,” in date, 2019.
[139] f. n. bostancı et al., “dr-strange: end-to-end system design for
dram-based true random number generators,” in hpca, 2022.
[140] a. olgun et al., “quac-trng: high-throughput true random number
generation using quadruple row activation in commodity dram
chips,” in isca, 2021.
5
[141] s. li et al., “drisa: a dram-based reconﬁgurable in-situ accelera-
tor,” in micro, 2017.
[142] v. seshadri et al., “rowclone: fast and energy-efﬁcient in-dram bulk
data copy and initialization,” in micro, 2013.
[143] y. wang et al., “figaro: improving system performance via fine-
grained in-dram data relocation and caching,” in micro, 2020.
[144] k. k. chang et al., “low-cost inter-linked subarrays (lisa): enabling
fast inter-subarray data movement in dram,” in hpca, 2016.
[145] s. h. s. rezaei et al., “nom: network-on-memory for inter-bank data
transfer in highly-banked memories,” cal, 2020.
[146] v. seshadri et al., “fast bulk bitwise and and or in dram,” cal,
2015.
[147] a. j. awan et al., “identifying the potential of near data processing for
apache spark,” in memsys, 2017.
[148] j. gómez-luna et al., “benchmarking a new paradigm:
an ex-
perimental analysis of a real processing-in-memory architecture,”
arxiv:2105.03814 [cs.ar], 2021.
[149] j. gómez-luna et al., “benchmarking memory-centric computing sys-
tems: analysis of real processing-in-memory hardware,” in cut,
2021.
[150] j. gómez-luna et al., “benchmarking a new paradigm: experimental
analysis and characterization of a real processing-in-memory system,”
ieee access, 2022.
[151] w. a. simon et al., “blade: an in-cache computing architecture for
edge devices,” ieee transactions on computers, 2020.
[152] x. si et al., “circuit design challenges in computing-in-memory for ai
edge devices,” in asicon, 2019.
[153] c. wu et al., “machine learning at facebook: understanding inference
at the edge,” in hpca, 2019.
[154] x. xu et al., “scaling for edge inference of deep neural networks,”
nature electronics, 2018.
[155] y.-h. chen et al., “eyeriss v2: a flexible accelerator for emerging
deep neural networks on mobile devices,” jetcas, 2019.
[156] google llc, “edge tpu,” https://cloud.google.com/edge-tpu/.
[157] nvidia corp., “nvidia jetson nano,” https://developer.nvidia.com/
embedded/jetson-nano-developer-kit.
[158] intel
corp.,
“intel
movidius
neural
compute
stick,”
https://software.intel.com/content/www/us/en/develop/articles/
intel-movidius-neural-compute-stick.html.
[159] h. sak et al., “long short-term memory based recurrent neural
network architectures for large vocabulary speech recognition,”
arxiv:1402.1128 [cs.ne], 2014.
[160] y. he et al., “streaming end-to-end speech recognition for mobile
devices,” in icassp, 2019.
[161] j. li et al., “improving rnn transducer modeling for end-to-end
speech recognition,” in asru, 2019.
[162] k. rao et al.,
“exploring architectures,
data and units for
streaming end-to-end speech recognition with rnn-transducer,”
arxiv:1801.00841 [cs.cl], 2018.
[163] o. vinyals et al., “show and tell: a neural image caption generator,”
in cvpr, 2015.
[164] j. donahue et al., “long-term recurrent convolutional networks for
visual recognition and description,” in cvpr, 2015.
[165] jedec solid state technology assn., “jesd235b: high bandwidth
memory (hbm) dram,” december 2018.
[166] hybrid memory cube consortium, “hmc speciﬁcation 2.0,” 2014.
[167] d. lee et al., “simultaneous multi-layer access: improving 3d-stacked
memory bandwidth at low cost,” acm taco, 2016.
[168] a. boroumand et al., “google neural network models for edge devices:
analyzing and mitigating machine learning inference bottlenecks,” in
pact, 2021.
[169] s. cao et al., “titant: online real-time transaction fraud detection in
ant financial,” arxiv:1906.07407 [cs.lg], 2019.
[170] x. qiu et al., “real-time constrained cycle detection in large dynamic
graphs,” proc. vldb endow., 2018.
[171] j. t. quah and m. sriganesh, “real-time credit card fraud detection
using computational intelligence,” expert systems with applications,
2008.
[172] p.-å. larson et al., “real-time analytical processing with sql server,”
pvldb, 2015.
[173] j. ramnarayan et al., “snappydata: streaming, transactions, and inter-
active analytics in a uniﬁed engine,” in cidr, 2016.
[174] b. sahay and j. ranjan, “real time business intelligence in supply
chain analytics,” information management & computer security, 2008.
[175] s. chisholm, “adopting medical technologies and diagnostics rec-
ommended by nice: the health technologies adoption programme,”
annals of the royal college of surgeons of england, 2014.
[176] v.-d. ta et al., “big data stream computing in healthcare real-time
analytics,” in icccbda, 2016.
[177] r. barber et al., “wiser: a highly available htap dbms for iot
applications,” arxiv:1908.01908 [cs.db], 2019.
[178] j. zhou et al., “kunpeng: parameter server based distributed learning
systems and its applications in alibaba and ant financial,” in sigkdd,
2017.
[179] p.-a. larson et al., “real-time analytical processing with sql server,”
proc. vldb endow., 2015.
[180] d. huang et al., “tidb: a raft-based htap database,” proc. vldb
endow., 2020.
[181] gartner research, “hybrid transaction/analytical processing will
foster opportunities for dramatic business innovation,” 2013. [online].
available: https://www.gartner.com/en/documents/2657815
[182] v. sikka et al., “sap hana: the evolution from a modern main-
memory data platform to an enterprise application platform,” proc.
vldb endow., 2013.
[183] j. giceva and m. sadoghi, hybrid oltp and olap, 2018.
[184] j. arulraj et al., “bridging the archipelago between row-stores and
column-stores for hybrid workloads,” in sigmod, 2016.
[185] d. makreshanski et al., “batchdb: efﬁcient isolated execution of hybrid
oltp+olap workloads for interactive applications,” in sigmod
conference, 2017.
[186] f. özcan et al., “hybrid transactional/analytical processing: a survey,”
in sigmod, 2017.
[187] o. mutlu et al., “processing data where it makes sense: enabling
in-memory computation,” micpro, 2019.
[188] a. kemper and t. neumann, “hyper: a hybrid oltp&olap main
memory database system based on virtual memory snapshots,” in
icde, 2011.
[189] a. sharma et al., “accelerating analytical processing in mvcc us-
ing fine-granular high-frequency virtual snapshotting,” in sigmod,
2018.
[190] a. boroumand et al., “polynesia: enabling high-performance and
energy-efﬁcient hybrid transactional/analytical databases with hard-
ware/software co-design,” in icde, 2022.
[191] safari research group, “polynesia — github repository,” https:
//github.com/cmu-safari/polynesia/, 2022.
[192] a. boroumand et al., “google neural network models for edge devices:
analyzing and mitigating machine learning inference bottlenecks,”
arxiv:2109.14320 [cs.ar], 2021.
[193] g. f. oliveira, “google neural network models for edge devices: ana-
lyzing and mitigating ml inference bottleneck – talk at pact 2021,”
https://www.youtube.com/watch?v=a5gxjdblras.
[194] g. f. oliveira, “polynesia: enabling high-performance and energy-
efﬁcient hybrid transactional/analytical databases with hard-
ware/software co-design – talk at icde 2022,” https://www.youtube.
com/watch?v=3ihmadjtwce.
6
"
2206.03809.pdf;"
references
[1] a. levis, challenges to control: a collective view–report of the workshop held at the university of santa clara on
september 18-19, 1986, ieee transactions on automatic control, vol. 32, 1987, pp. 275-285.
[2] r.m. murray, k.j. astrom, s.p. boyd, r.w. brockett, and g. stein, future directions in control in an information-rich
world, ieee control systems magazine, vol. 23, 2003, pp. 20-33.
[3] z.-s. hou and z. wang, from model-based control to data-driven control: survey, classiﬁcation and perspective,
information sciences, vol. 235, 2013, pp 3-35.
[4] d.p. bertsekas. reinforcement learning and optimal control, athena scientiﬁc, 1st edition; 2019.
[5] r.s. sutton and a.g. barto, reinforcement learning: an introduction, the mit press, cambridge; 2012.
[6] w. tan and a. packard, searching for control lyapunov functions using sums of squares programming, 42nd annual
allerton conference on communications, control and computing, 2004, pp 210-219.
[7] h. ravanbakhsh and s. sankaranarayanan, learning control lyapunov functions from counterexamples and demonstra-
tions, autonomous robots, vol. 43, 2019, pp 275-307.
[8] o.d. richard, e.h. peter and g.s. david, pattern classiﬁcation, john wiley, 2nd edition; 2001.
[9] u. helmke and j.b. moore, optimization and dynamical systems, springer-verlag, london; 1994.
[10] n.j. higham, computing a nearest symmetric positive semideﬁnite matrix, linear algebra and its applications, vol. 103,
1988, pp 103-118.
[11] s. boyd, l. el. ghaoui, e. feron and v. balakrishnan, linear matrix inequalities in system and control theory, siam;
1994.
[12] yalmip. available: https://yalmip.github.io/.
[13] a. nedic, optimization i, available: https://netﬁles.uiuc.edu/angelia/www/optimization one.pdf
[14] d. liberzon and a.s. morse, basic problems in stability and design of switched systems, ieee control systems, vol.
19, 1999, pp 59-70.
[15] j. zhao and i. kanellakopoulos, flexible backstepping design for tracking and disturbance attenuation, international
journal of robust and nonlinear control, vol. 8, 1998, pp 331-348.
[16] k.-y. cai and l. chen, analyzing software science data with partial repeatability, journal of system and software, vol.
63, 2002, pp. 73-186.
[17] z. zhang, r. sun, x. wang, and c. zhao, a situational analytic method for user behavior pattern in multimedia social
networks, ieee transaction on big data, vol. 5, 2019, pp 520-528.
june 9, 2022
draft
22
[18] j. lu, a. liu, f. dong, f. gu, j. gama, and g. zhang. learning under concept drift: a review, ieee transactions on
knowledge and data engineering, vol. 31, 2019, pp 2346-2363.
[19] k.-y. cai, j.-w. cangussu, r.-a. decarlo, a.-p. mathur. an overview of software cybernetics. ieee international
workshop on software technology and engineering practice, 2003, pp 77-86.
june 9, 2022
draft
"
2209.03499.pdf;"
references 
abdollahi, b., nasraoui, o., 2016. explainable restricted boltzmann machines 
for collaborative filtering. arxiv160607129 cs stat. 
adadi, a., berrada, m., 2018. peeking inside the black-box: a survey on 
explainable artificial intelligence (xai). ieee access 6, 52138–52160. 
https://doi.org/10.1109/access.2018.2870052 
ahmad, m.a., eckert, c., teredesai, a., mckelvey, g., 2018. interpretable 
machine learning in healthcare 7. 
ascarza, e., 2018. retention futility: targeting high-risk customers might be 
ineffective. j. mark. res. 55, 80–98. https://doi.org/10.1509/jmr.16.0163 
assad, s., clark, r., ershov, d., xu, l., 2020. algorithmic pricing and 
competition: empirical evidence from the german retail gasoline market. 
ssrn electron. j. https://doi.org/10.2139/ssrn.3682021 
barredo arrieta, a., díaz-rodríguez, n., del ser, j., bennetot, a., tabik, s., 
barbado, a., garcia, s., gil-lopez, s., molina, d., benjamins, r., chatila, 
r., herrera, f., 2020. explainable artificial intelligence (xai): concepts, 
12 
taxonomies, opportunities and challenges toward responsible ai. inf. fusion 
58, 82–115. https://doi.org/10.1016/j.inffus.2019.12.012 
bertini, m., koenigsberg, o., 2021. the pitfalls of pricing algorithms. harv. 
bus. rev. 
bhatt, u., xiang, a., sharma, s., weller, a., taly, a., jia, y., ghosh, j., puri, 
r., moura, j.m.f., eckersley, p., 2019. explainable machine learning in 
deployment. 
bojarski, m., del testa, d., dworakowski, d., firner, b., flepp, b., goyal, p., 
jackel, l.d., monfort, m., muller, u., zhang, j., zhang, x., zhao, j., zieba, 
k., 2016. end to end learning for self-driving cars. arxiv160407316 cs. 
caruana, r., lou, y., gehrke, j., koch, p., sturm, m., elhadad, n., 2015. 
intelligible models for healthcare: predicting pneumonia risk and hospital 
30-day readmission, in: proceedings of the 21th acm sigkdd international 
conference on knowledge discovery and data mining, kdd ’15. association 
for computing machinery, new york, ny, usa, pp. 1721–1730. 
https://doi.org/10.1145/2783258.2788613 
castelvecchi, d., 2016. can we open the black box of ai? nature 538, 20–23. 
https://doi.org/10.1038/538020a 
che, z., purushotham, s., khemani, r., liu, y., 2016. interpretable deep models 
for icu outcome prediction. amia annu. symp. proc. amia symp. 2016, 
371–380. 
csiszár, o., csiszár, g., dombi, j., 2020. interpretable neural networks based on 
continuous-valued logic and multicriteria decision operators. knowl.-based 
syst. 199, 105972. https://doi.org/10.1016/j.knosys.2020.105972 
cunha, m., osório c., a.m., ribeiro, r.m., 2020. endogenous product design 
and quality when consumers have heterogeneous limited attention (ssrn 
scholarly paper no. 2860456). social science research network, rochester, 
ny. https://doi.org/10.2139/ssrn.2860456 
datta, amit, tschantz, m.c., datta, anupam, 2015. automated experiments on 
ad privacy settings: a tale of opacity, choice, and discrimination. 
https://doi.org/10.48550/arxiv.1408.6491 
dekimpe, m.g., 2020. retailing and retailing research in the age of big data 
analytics. 
int. 
j. 
res. 
mark. 
37, 
3–14. 
https://doi.org/10.1016/j.ijresmar.2019.09.001 
13 
dentons 
[www 
document], 
2021. 
url 
https://www.dentons.com/en/insights/guides-reports-and-
whitepapers/2021/january/28/global-guide-to-autonomous-vehicles-2021 
(accessed 12.16.21). 
doshi-velez, f., kim, b., 2017. towards a rigorous science of interpretable 
machine learning. arxiv170208608 cs stat. 
doshi-velez, f., kortz, m., budish, r., bavitz, c., gershman, s., o’brien, d., 
scott, k., schieber, s., waldo, j., weinberger, d., weller, a., wood, a., 2017. 
accountability of ai under the law: the role of explanation. 
https://doi.org/10.48550/arxiv.1711.01134 
drew, j.h., mani, d.r., betz, a.l., datta, p., 2001. targeting customers with 
statistical and data-mining techniques. j. serv. res. 3, 205–219. 
https://doi.org/10.1177/109467050133002 
fu, r., aseri, m., singh, p.v., srinivasan, k., 2020. ’un’fair machine learning 
algorithms. https://doi.org/10.2139/ssrn.3408275 
fu, r., jin, g.z., liu, m., 2022. human-algorithm interactions: evidence from 
zillow.com. working paper series. https://doi.org/10.3386/w29880 
goodfellow, i.j., shlens, j., szegedy, c., 2015. explaining and harnessing 
adversarial examples. arxiv14126572 cs stat. 
goodman, b., flaxman, s., 2017. european union regulations on algorithmic 
decision-making and a “right to explanation.” ai mag. 38, 50–57. 
https://doi.org/10.1609/aimag.v38i3.2741 
haspiel, j., du, n., meyerson, j., robert jr., l.p., tilbury, d., yang, x.j., 
pradhan, a.k., 2018. explanations and expectations: trust building in 
automated vehicles, in: companion of the 2018 acm/ieee international 
conference on human-robot interaction, hri ’18. association for 
computing 
machinery, 
new 
york, 
ny, 
usa, 
pp. 
119–120. 
https://doi.org/10.1145/3173386.3177057 
holzinger, a., biemann, c., pattichis, c.s., kell, d.b., 2017a. what do we need 
to build explainable ai systems for the medical domain? arxiv171209923 cs 
stat. 
holzinger, a., plass, m., holzinger, k., crisan, g.c., pintea, c.-m., palade, v., 
2017b. a glass-box interactive machine learning approach for solving np-hard 
problems with the human-in-the-loop. arxiv170801104 cs stat. 
14 
howard, a., zhang, c., horvitz, e., 2017. addressing bias in machine learning 
algorithms: a pilot study on emotion recognition for intelligent systems, in: 
2017 ieee workshop on advanced robotics and its social impacts (arso). 
presented at the 2017 ieee workshop on advanced robotics and its social 
impacts 
(arso), 
ieee, 
austin, 
tx, 
usa, 
pp. 
1–7. 
https://doi.org/10.1109/arso.2017.8025197 
huang, m.-h., rust, r.t., 2021. a strategic framework for artificial intelligence 
in marketing. j. acad. mark. sci. 49, 30–50. https://doi.org/10.1007/s11747-
020-00749-9 
katuwal, g.j., chen, r., 2016. machine learning model interpretability for 
precision medicine. arxiv161009045 q-bio. 
law library of congress (u.s.)., g.l.r.d., , 2019. regulation of artificial 
intelligence 
in 
selected 
jurisdictions. 
[www 
document]. 
url 
https://purl.fdlp.gov/gpo/gpo123733 
lei, j., g’sell, m., rinaldo, a., tibshirani, r.j., wasserman, l., 2017. 
distribution-free predictive inference for regression. arxiv160404173 math 
stat. 
lipton, z.c., 2017. the mythos of model interpretability. arxiv160603490 cs 
stat. 
lundberg, s.m., lee, s.-i., 2017. a unified approach to interpreting model 
predictions, in: advances in neural information processing systems. curran 
associates, inc. 
malik, n., 2020. does machine learning amplify pricing errors in housing 
market? : 
economics 
of 
ml 
feedback 
loops. 
https://doi.org/10.2139/ssrn.3694922 
mceneney, m.f., kaufmann, k.f., 2005. implementing the fact act: self-
executing provisions. bus. lawyer 60, 737–747. 
miller, t., 2019. explanation in artificial intelligence: insights from the social 
sciences. artif. intell. 267, 1–38. https://doi.org/10.1016/j.artint.2018.07.007 
mittelstadt, b., russell, c., wachter, s., 2019. explaining explanations in ai, 
in: proceedings of the conference on fairness, accountability, and 
transparency, fat* ’19. association for computing machinery, new york, 
ny, usa, pp. 279–288. https://doi.org/10.1145/3287560.3287574 
moore, j., swartout, w., 1988. explanation in expert systems: a survey 58. 
15 
murdoch, w.j., singh, c., kumbier, k., abbasi-asl, r., yu, b., 2019. 
interpretable machine learning: definitions, methods, and applications. proc. 
natl. acad. sci. 116, 22071–22080. https://doi.org/10.1073/pnas.1900654116 
netzer, o., lemaire, a., herzenstein, m., 2019. when words sweat: identifying 
signals for loan default in the text of loan applications. j. mark. res. 56, 
960–980. https://doi.org/10.1177/0022243719852959 
neumann, n., tucker, c.e., whitfield, t., 2019. frontiers: how effective is 
third-party consumer profiling? evidence from field studies. mark. sci. 38, 
918–926. https://doi.org/10.1287/mksc.2019.1188 
neven, d., thisse, j.-f., 1989. on quality and variety competition. 
orekondy, t., schiele, b., fritz, m., 2019. knockoff nets: stealing functionality 
of black-box models. presented at the proceedings of the ieee/cvf 
conference on computer vision and pattern recognition, pp. 4954–4963. 
ribeiro, m.t., singh, s., guestrin, c., 2016. “why should i trust you?”: 
explaining the predictions of any classifier. arxiv160204938 cs stat. 
rudin, c., 2019. stop explaining black box machine learning models for high 
stakes decisions and use interpretable models instead. nat. mach. intell. 1, 
206–215. https://doi.org/10.1038/s42256-019-0048-x 
stanton, n.a., salmon, p.m., walker, g.h., stanton, m., 2019. models and 
methods for collision analysis: a comparison study based on the uber collision 
with 
a 
pedestrian. 
saf. 
sci. 
120, 
117–128. 
https://doi.org/10.1016/j.ssci.2019.06.008 
tan, s., caruana, r., hooker, g., lou, y., 2018. distill-and-compare: auditing 
black-box models using transparent model distillation. proc. 2018 
aaaiacm 
conf. 
ai 
ethics 
soc. 
303–310. 
https://doi.org/10.1145/3278721.3278725 
valls, a., gibert, k., orellana, a., antón-clavé, s., 2018. using ontology-based 
clustering to understand the push and pull factors for british tourists visiting 
a 
mediterranean 
coastal 
destination. 
inf. 
manage. 
55, 
145–159. 
https://doi.org/10.1016/j.im.2017.05.002 
van lent, m., fisher, w., mancuso, m., 2004. an explainable artificial 
intelligence system for small-unit tactical behavior, in: proceedings of the 
16th conference on innovative applications of artifical intelligence, iaai’04. 
aaai press, san jose, california, pp. 900–907. 
16 
vandenbosch, m.b., weinberg, c.b., 1995. product and price competition in a 
two-dimensional vertical differentiation model. mark. sci. 14, 224–249. 
wang, q., huang, y., jasin, s., singh, p.v., 2020. algorithmic transparency 
with strategic users (ssrn scholarly paper no. id 3652656). social science 
research network, rochester, ny. https://doi.org/10.2139/ssrn.3652656 
wattal, s., telang, r., mukhopadhyay, t., 2009. information personalization in 
a two-dimensional product differentiation model. j. manag. inf. syst. 26, 
69–95. https://doi.org/10.2753/mis0742-1222260204 
yurtsever, e., lambert, j., carballo, a., takeda, k., 2020. a survey of 
autonomous driving: common practices and emerging technologies. ieee 
access 8, 58443–58469. https://doi.org/10.1109/access.2020.2983149 
zhou, j., chen, f., holzinger, a., 2022. towards explainability for ai fairness, 
in: holzinger, a., goebel, r., fong, r., moon, t., müller, k.-r., samek, w. 
(eds.), xxai - beyond explainable ai: international workshop, held in 
conjunction with icml 2020, july 18, 2020, vienna, austria, revised and 
extended papers, lecture notes in computer science. springer international 
publishing, cham, pp. 375–386. https://doi.org/10.1007/978-3-031-04083-
2_18 
zhu, j., liapis, a., risi, s., bidarra, r., youngblood, g.m., 2018. explainable 
ai for designers: a human-centered perspective on mixed-initiative co-
creation, in: 2018 ieee conference on computational intelligence and 
games (cig). presented at the 2018 ieee conference on computational 
intelligence 
and 
games 
(cig), 
ieee, 
maastricht, 
pp. 
1–8. 
https://doi.org/10.1109/cig.2018.8490433 
 
 
17 
 
 
 
web links 
1 https://www.gartner.com/en/information-technology/insights/top-technology-trends 
2 https://www.idc.com/getdoc.jsp?containerid=idc_p33198 
3 https://www.newyorker.com/magazine/2017/04/03/ai-versus-md 
4 https://www.bloomberg.com/graphics/2016-amazon-same-day/ 
5 https://www.inverse.com/article/39483-apple-refund-china 
6 https://futurism.com/ai-bias-black-box 
7 https://www.darpa.mil/program/explainable-artificial-intelligence 
8 https://ec.europa.eu/futurium/en/ai-alliance-consultation.1.html 
9 https://tinyurl.com/theguardian-ai-watchdog 
10 https://research.aimultiple.com/xai/ 
11 https://tinyurl.com/theguardian-transparent-ai 
12 http://www.ftc.gov/legal-library/browse/statutes/fair-credit-reporting-act 
13 https://insight.equifax.com/visualizing-xai-in-credit-risk/ 
14 https://www.ntsb.gov/news/events/pages/2020-hwy18fh011-bmg.aspx 
15 https://www.compact.nl/en/articles/autonomous-compliance/ 
16 https://www.wired.com/story/europes-new-privacy-law-will-change-the-web-and-more/ 
17 https://tinyurl.com/vox-gdpr-data-protection 
18 https://tinyurl.com/cnet-gdpr-fb-google 
19 https://tinyurl.com/forbes-gdpr-benefits 
20 https://tinyurl.com/vox-gdpr-data-protection 
21 https://tinyurl.com/cnbc-fb-call-for-gdpr 
22 https://tinyurl.com/comply-or-explain 
23 https://www.wipo.int/news/en/wipolex/2016/article_0014.html 
24 https://tinyurl.com/gdpr-harmful-for-ai 
25 https://tinyurl.com/cnbc-intel-ai-infancy 
"
2209.04963.pdf;"
references
[1] a. jobin, m. ienca, and e. vayena, “the global landscape of ai ethics guidelines,” nature machine intelligence,
vol. 1, no. 9, pp. 389–399, 2019.
[2] j. fjeld et. al., “principled artiﬁcial intelligence: mapping consensus in ethical and rights-based approaches to
principles for ai,” berkman klein center research publication, no. 2020-1, 2020.
[3] q. v. liao, d. gruen, and s. miller, “questioning the ai: informing design practices for explainable ai user
experiences,” in proceedings of the 2020 chi conference on human factors in computing systems, 2020, pp.
1–15.
[4] q. v. liao, m. pribi´c, j. han, s. miller, and d. sow, “question-driven design process for explainable ai user
experiences,” arxiv preprint arxiv:2104.03483, 2021.
[5] r. larasati, a. de liddo, and e. motta, “ai healthcare system interface: explanation design for non-expert user
trust,” in acmiui-ws 2021: joint proceedings of the acm iui 2021 workshops, vol. 2903.
ceur workshop
proceedings, 2021.
[6] s.-h. han and h.-j. choi, “checklist for validating trustworthy ai,” in 2022 ieee international conference on
big data and smart computing (bigcomp).
ieee, 2022, pp. 391–394.
[7] i. d. raji, a. smart, r. n. white, m. mitchell, t. gebru, b. hutchinson, j. smith-loud, d. theron, and p. barnes,
“closing the ai accountability gap: deﬁning an end-to-end framework for internal algorithmic auditing,” in
proceedings of the 2020 conference on fairness, accountability, and transparency, 2020, pp. 33–44.
[8] a. jacovi, a. marasovi´c, t. miller, and y. goldberg, “formalizing trust in artiﬁcial intelligence: prerequisites,
causes and goals of human trust in ai,” in proceedings of the 2021 acm conference on fairness, accountability,
and transparency, 2021, pp. 624–635.
[9] m. k. ahuja, m.-b. belaid, p. bernab´e, m. collet, a. gotlieb, c. lal, d. marijan, s. sen, a. sharif, and
h. spieker, “opening the software engineering toolbox for the assessment of trustworthy ai,” arxiv preprint
arxiv:2007.07768, 2020.
[10] b. hutchinson, a. smart, a. hanna, e. denton, c. greer, o. kjartansson, p. barnes, and m. mitchell, “towards
accountability for machine learning datasets: practices from software engineering and infrastructure,” in
proceedings of the 2021 acm conference on fairness, accountability, and transparency, 2021, pp. 560–575.
[11] z. zhou, z. li, y. zhang, and l. sun, “transparent-ai blueprint: developing a conceptual tool to support the
design of transparent ai agents,” international journal of human–computer interaction, pp. 1–28, 2022.
[12] d. adkins, b. alsallakh, a. cheema, n. kokhlikyan, e. mcreynolds, p. mishra, c. procope, j. sawruk, e. wang,
and p. zvyagina, “prescriptive and descriptive approaches to machine-learning transparency,” in chi conference
on human factors in computing systems extended abstracts, 2022, pp. 1–9.
[13] k. beck and w. cunningham, “using pattern languages for object oriented programs,” in conference on
object-oriented programming, systems, languages, and applications (oopsla), 1987.
26
a preprint - september 16, 2022
[14] b. a. kitchenham and s. charters, “guidelines for performing systematic literature reviews in software engi-
neering,” tech. rep., 2007.
[15] v. garousi, m. felderer, and m. v. m¨antyl¨a, “guidelines for including grey literature and conducting multivocal
literature reviews in software engineering,” information and software technology, vol. 106, pp. 101–121, 2019.
[online]. available: https://www.sciencedirect.com/science/article/pii/s0950584918301939
[16] diser (australian government), “australia’s ai ethics principles,” https://industry.gov.au/data-and-publicati
ons/australias-artificial-intelligence-ethics-framework/australias-ai-ethics-principles, 2020, accessed: 17 aug
2022.
[17] r. c. martin, d. riehle, and f. buschmann, pattern languages of program design 3.
addison-wesley longman
publishing co., inc., 1997.
[18] b. shneiderman, “bridging the gap between ethics and practice: guidelines for reliable, safe, and trustworthy
human-centered ai systems,” acm trans. interact. intell. syst., vol. 10, no. 4, 2020.
[19] ——, “responsible ai: bridging from ethics to practice,” communications of the acm, vol. 64, no. 8, pp. 32–35,
2021.
[20] b. r. jackson, y. ye, j. m. crawford, m. j. becich, s. roy, j. r. botkin, m. e. de baca, and l. pantanowitz,
“the ethics of artiﬁcial intelligence in pathology and laboratory medicine: principles and practice,” academic
pathology, vol. 8, p. 2374289521990784, 2021.
[21] j. schaich borg, “four investment areas for ethical ai: transdisciplinary opportunities to close the publication-to-
practice gap,” big data & society, vol. 8, no. 2, p. 20539517211040197, 2021.
[22] e. papagiannidis, i. m. enholm, c. dremel, p. mikalef, and j. krogstie, “deploying ai governance practices: a
revelatory case study,” in conference on e-business, e-services and e-society.
springer, 2021, pp. 208–219.
[23] j. c. ib´a˜nez and m. v. olmeda, “operationalising ai ethics: how are companies bridging the gap between practice
and principles? an exploratory study,” ai & society, pp. 1–25, 2021.
[24] v. dignum, “ensuring responsible ai in practice,” in responsible artiﬁcial intelligence.
springer, 2019, pp.
93–105.
[25] a. zhobe, h. jahankhani, r. fong, p. elevique, and h. baajour, “the magic quadrant: assessing ethical maturity
for artiﬁcial intelligence,” in cybersecurity, privacy and freedom protection in the connected world.
springer,
2021, pp. 313–326.
[26] p. fukas, j. rebstadt, f. remark, and o. thomas, “developing an artiﬁcial intelligence maturity model for
auditing.” in ecis, 2021.
[27] s. alsheiabni, y. cheung, and c. messom, “towards an artiﬁcial intelligence maturity model: from science
ﬁction to business facts,” 2019.
[28] a. henriksen, s. enni, and a. bechmann, “situated accountability: ethical principles, certiﬁcation standards,
and explanation methods in applied ai,” in proceedings of the 2021 aaai/acm conference on ai, ethics, and
society, 2021, pp. 574–585.
[29] c. d. martin and t. t. makoundou, “taking the high road ethics by design in ai,” acm inroads, vol. 8, no. 4, pp.
35–37, 2017.
[30] r. h. yap, “towards certifying trustworthy machine learning systems,” in international workshop on the
foundations of trustworthy ai integrating learning, optimization and reasoning.
springer, 2020, pp. 77–82.
[31] p. cihon, m. j. kleinaltenkamp, j. schuett, and s. d. baum, “ai certiﬁcation: advancing ethical practice by
reducing information asymmetries,” ieee transactions on technology and society, vol. 2, no. 4, pp. 200–209,
2021.
[32] p. boza and t. evgeniou, “implementing ai principles: frameworks, processes, and tools,” 2021.
[33] d. d. luxton, “recommendations for the ethical use and design of artiﬁcial intelligent care providers,” artiﬁcial
intelligence in medicine, vol. 62, no. 1, pp. 1–10, 2014.
[34] m. d. mccradden, s. joshi, j. a. anderson, m. mazwi, a. goldenberg, and r. zlotnik shaul, “patient safety
and quality improvement: ethical principles for a regulatory approach to bias in healthcare machine learning,”
journal of the american medical informatics association, vol. 27, no. 12, pp. 2024–2027, 2020.
[35] l. zhu, x. xu, q. lu, g. governatori, and j. whittle, “ai and ethics—operationalizing responsible ai,” in
humanity driven ai.
springer, 2022, pp. 15–33.
27
a preprint - september 16, 2022
[36] m. sloane and j. zakrzewski, “german ai start-ups and “ai ethics”: using a social practice lens for assessing
and implementing socio-technical innovation,” in 2022 acm conference on fairness, accountability, and
transparency, 2022, pp. 935–947.
[37] j. n. hooker and t. w. n. kim, “toward non-intuition-based machine and artiﬁcial intelligence ethics: a
deontological approach based on modal logic,” in proceedings of the 2018 aaai/acm conference on ai, ethics,
and society, 2018, pp. 130–136.
[38] m. d’aquin, p. troullinou, n. e. o’connor, a. cullen, g. faller, and l. holden, “towards an” ethics by design”
methodology for ai research projects,” in proceedings of the 2018 aaai/acm conference on ai, ethics, and
society, 2018, pp. 54–59.
[39] r. benjamins, a. barbado, and d. sierra, “responsible ai by design in practice,” arxiv preprint arxiv:1909.12838,
2019.
[40] r. eitel-porter, “beyond the promise: implementing ethical ai,” ai and ethics, vol. 1, no. 1, pp. 73–80, 2021.
[41] m. s. a. lee and j. singh, “risk identiﬁcation questionnaire for detecting unintended bias in the machine
learning development lifecycle,” in proceedings of the 2021 aaai/acm conference on ai, ethics, and society,
2021, pp. 704–714.
[42] f. redmill, “risk analysisa subjective process,” engineering management journal, vol. 12, no. 2, pp. 91–96,
2002.
[43] m. d. schultz and p. seele, “towards ai ethics’ institutionalization: knowledge bridges from business ethics to
advance organizational ai ethics,” ai and ethics, pp. 1–13, 2022.
[44] i. barclay, a. preece, i. taylor, and d. verma, “towards traceability in data ecosystems using a bill of materials
model,” in international workshop on science gateways.
ceur-ws, 2019.
[45] ntia, “the minimum elements for a software bill of materials (sbom),” https://www.ntia.doc.gov/files/ntia/
publications/sbom minimum elements report.pdf, 2021, accessed 18 aug 2022.
[46] d. kasenberg, t. arnold, and m. scheutz, “norms, rewards, and the intentional stance: comparing machine
learning approaches to ethical training,” in proceedings of the 2018 aaai/acm conference on ai, ethics, and
society, 2018, pp. 184–190.
[47] m. sand, j. m. dur´an, and k. r. jongsma, “responsibility beyond design: physicians’ requirements for ethical
medical ai,” bioethics, vol. 36, no. 2, pp. 162–169, 2022.
[48] s. amershi, a. begel, c. bird, r. deline, h. gall, e. kamar, n. nagappan, b. nushi, and t. zimmermann,
“software engineering for machine learning: a case study,” in 2019 ieee/acm 41st international conference on
software engineering: software engineering in practice (icse-seip).
ieee, 2019, pp. 291–300.
[49] w. hussain, m. shahin, r. hoda, j. whittle, h. perera, a. nurwidyantoro, r. a. shams, and g. oliver, “how can
human values be addressed in agilemethods a case study on safe,” ieee transactions on software engineering,
2022.
[50] q. lu, l. zhu, x. xu, j. whittle, and z. xing, “towards a roadmap on software engineering for responsible
ai,” in 2022 ieee/acm 1st international conference on ai engineering – software engineering for ai (cain),
2022, pp. 101–112.
[51] r. v. zicari, j. brusseau, s. n. blomberg, h. c. christensen, m. coffee, m. b. ganapini, s. gerke, t. k. gilbert,
e. hickman, e. hildt et al., “on assessing trustworthy ai in healthcare. machine learning as a supportive tool to
recognize cardiac arrest in emergency calls,” frontiers in human dynamics, p. 30, 2021.
[52] k.-e. k. bilstrup, m. h. kaspersen, and m. g. petersen, “staging reﬂections on ethical dilemmas in machine
learning: a card-based design workshop for high school students,” in proceedings of the 2020 acm designing
interactive systems conference, 2020, pp. 1211–1222.
[53] r. v. zicari, s. ahmed, j. amann, s. a. braun, j. brodersen, f. bruneault, j. brusseau, e. campano, m. coffee,
a. dengel et al., “co-design of a trustworthy ai system in healthcare: deep learning based skin lesion classiﬁer,”
frontiers in human dynamics, vol. 3, p. 688152, 2021.
[54] m. mitchell, s. wu, a. zaldivar, p. barnes, l. vasserman, b. hutchinson, e. spitzer, i. d. raji, and t. gebru,
“model cards for model reporting,” in proceedings of the conference on fairness, accountability, and transparency,
2019, pp. 220–229.
[55] a. wadhwani and p. jain, “machine learning model cards transparency review: using model card toolkit,” in
2020 ieee pune section international conference (punecon).
ieee, 2020, pp. 133–137.
[56] t. gebru, j. morgenstern, b. vecchione, j. w. vaughan, h. wallach, h. d. iii, and k. crawford, “datasheets for
datasets,” communications of the acm, vol. 64, no. 12, pp. 86–92, 2021.
28
a preprint - september 16, 2022
[57] m. arnold, r. k. bellamy, m. hind, s. houde, s. mehta, a. mojsilovi´c, r. nair, k. n. ramamurthy, a. olteanu,
d. piorkowski et al., “factsheets: increasing trust in ai services through supplier’s declarations of conformity,”
ibm journal of research and development, vol. 63, no. 4/5, pp. 6–1, 2019.
[58] d. adkins, b. alsallakh, a. cheema, n. kokhlikyan, e. mcreynolds, p. mishra, c. procope, j. sawruk,
e. wang, and p. zvyagina, “method cards for prescriptive machine-learning transparency,” in 2022 ieee/acm
1st international conference on ai engineering–software engineering for ai (cain).
ieee, 2022, pp. 90–100.
[59] c. ebert and m. weyrich, “validation of autonomous systems,” ieee software, vol. 36, no. 5, pp. 15–23, 2019.
[60] l. gauerhof, r. hawkins, c. picardi, c. paterson, y. hagiwara, and i. habli, “assuring the safety of machine
learning for pedestrian detection at crossings,” in international conference on computer safety, reliability, and
security.
springer, 2020, pp. 197–212.
[61] g. pair, “people + ai guidebook,” pair.withgoogle.com/guidebook, 2021, accessed: 17 aug 2022.
[62] a. vogelsang and m. borg, “requirements engineering for machine learning: perspectives from data scientists,”
in 2019 ieee 27th international requirements engineering conference workshops (rew).
ieee, 2019, pp.
245–251.
[63] j. horkoff, “non-functional requirements for machine learning: challenges and new directions,” in 2019 ieee
27th international requirements engineering conference (re).
ieee, 2019, pp. 386–391.
[64] a. bibal, m. lognoul, a. de streel, and b. fr´enay, “legal requirements on explainability in machine learning,”
artiﬁcial intelligence and law, vol. 29, no. 2, pp. 149–169, 2021.
[65] h. perera, r. hoda, r. a. shams, a. nurwidyantoro, m. shahin, w. hussain, and j. whittle, “the impact of
considering human values during requirements engineering activities,” arxiv preprint arxiv:2111.15293, 2021.
[66] i. society, p. bourque, and r. fairley, “guide to the software engineering body of knowledge (swebok (r)),”
2014.
[67] e. halme, v. vakkuri, j. kultanen, m. jantunen, k.-k. kemell, r. rousi, and p. abrahamsson, “how to write
ethical user stories? impacts of the eccola method,” in international conference on agile software development.
springer, cham, 2021, pp. 36–52.
[68] h. muccini and k. vaidhyanathan, “software architecture for ml-based systems: what exists and what lies ahead,”
in 2021 ieee/acm 1st workshop on ai engineering-software engineering for ai (wain).
ieee, 2021, pp.
121–128.
[69] s. k. lo, q. lu, h.-y. paik, and l. zhu, “flra: a reference architecture for federated learning systems,” in
european conference on software architecture.
springer, 2021, pp. 83–98.
[70] g. a. lewis, i. ozkaya, and x. xu, “software architecture challenges for ml systems,” in 2021 ieee international
conference on software maintenance and evolution (icsme).
ieee, 2021, pp. 634–638.
[71] s. umbrello, “the role of engineers in harmonising human values for ai systems design,” journal of responsible
technology, vol. 10, p. 100031, 2022.
[72] m. takeda, y. hirata, y.-h. weng, t. katayama, y. mizuta, and a. koujina, “accountable system design
architecture for embodied ai: a focus on physical human support robots,” advanced robotics, vol. 33, no. 23, pp.
1248–1263, 2019.
[73] b. fish and l. stark, “reﬂexive design for fairness and other human values in formal models,” in proceedings of
the 2021 aaai/acm conference on ai, ethics, and society, 2021, pp. 89–99.
[74] i. naja, m. markovic, p. edwards, and c. cottrill, “a semantic framework to support ai system accountability
and audit,” in european semantic web conference.
springer, 2021, pp. 160–176.
[75] p. ayranci, p. lai, n. phan, h. hu, a. kolinowski, d. newman, and d. dou, “onml: an ontology-based approach
for interpretable machine learning,” journal of combinatorial optimization, pp. 1–24, 2022.
[76] k. sekiguchi and k. hori, “organic and dynamic tool for use with knowledge base of ai ethics for promoting
engineers’ practice of ethical ai design,” ai & society, vol. 35, no. 1, pp. 51–71, 2020.
[77] m. anderson and s. l. anderson, “geneth: a general ethical dilemma analyzer,” paladyn, journal of behavioral
robotics, vol. 9, no. 1, pp. 337–357, 2018.
[78] v. singh, s. k. s. hari, t. tsai, and m. pitale, “simulation driven design and test for safety of ai based
autonomous vehicles,” in proceedings of the ieee/cvf conference on computer vision and pattern recognition,
2021, pp. 122–128.
[79] a. dosovitskiy, g. ros, f. codevilla, a. lopez, and v. koltun, “carla: an open urban driving simulator,” in
conference on robot learning.
pmlr, 2017, pp. 1–16.
29
a preprint - september 16, 2022
[80] c. regli and b. annighoefer, “an anthropomorphic approach to establish an additional layer of trustworthiness
of an ai pilot,” in software engineering 2022 workshops.
gesellschaft f¨ur informatik ev, 2022.
[81] s. f. jentzsch, s. h¨ohn, and n. hochgeschwender, “conversational interfaces for explainable ai: a human-centred
approach,” in international workshop on explainable, transparent autonomous agents and multi-agent systems.
springer, 2019, pp. 77–92.
[82] f. hussain, r. hussain, b. noye, and s. sharieh, “enterprise api security and gdpr compliance: design and
implementation perspective,” it professional, vol. 22, no. 5, pp. 81–89, 2020.
[83] h. j. pandit, d. o’sullivan, and d. lewis, “towards knowledge-based systems for gdpr compliance.” in
ckgsemstats@ iswc, 2018.
[84] m. fan, l. yu, s. chen, h. zhou, x. luo, s. li, y. liu, j. liu, and t. liu, “an empirical evaluation of
gdpr compliance violations in android mhealth apps,” in 2020 ieee 31st international symposium on software
reliability engineering (issre).
ieee, 2020, pp. 253–264.
[85] e. mitchell, p. henderson, c. d. manning, d. jurafsky, and c. finn, “self-destructing models: increasing the
costs of harmful dual uses in foundation models,” in first workshop on pre-training: perspectives, pitfalls, and
paths forward at icml 2022, 2002.
[86] t. shevlane, “structured access to ai capabilities: an emerging paradigm for safe ai deployment,” arxiv preprint
arxiv:2201.05159, 2022.
[87] n. six, a. perrichon-chr´etien, and n. herbaut, “saiaas: a blockchain-based solution for secure artiﬁcial intelli-
gence as-a-service,” in the international conference on deep learning, big data and blockchain.
springer,
2021, pp. 67–74.
[88] a. chattopadhyay, a. ali, and d. thaxton, “assessing the alignment of social robots with trustworthy ai
design guidelines: a preliminary research study,” in proceedings of the eleventh acm conference on data and
application security and privacy, 2021, pp. 325–327.
[89] w. xie and p. wu, “fairness testing of machine learning models using deep reinforcement learning,” in 2020 ieee
19th international conference on trust, security and privacy in computing and communications (trustcom).
ieee, 2020, pp. 121–128.
[90] a. aggarwal, p. lohia, s. nagar, k. dey, and d. saha, “black box fairness testing of machine learning models,” in
proceedings of the 2019 27th acm joint meeting on european software engineering conference and symposium
on the foundations of software engineering, 2019, pp. 625–635.
[91] r. b. l. dixon, “a principled governance for emerging ai regimes: lessons from china, the european union, and
the united states,” ai and ethics, pp. 1–18, 2022.
[92] c. murphy, g. e. kaiser, and m. arias, “an approach to software testing of machine learning applications,”
2007.
[93] n. j. goodall, “machine ethics and automated vehicles,” in road vehicle automation.
springer, 2014, pp.
93–102.
[94] n. mehrabi, f. morstatter, n. saxena, k. lerman, and a. galstyan, “a survey on bias and fairness in machine
learning,” acm computing surveys (csur), vol. 54, no. 6, pp. 1–35, 2021.
[95] s. mart´ınez-fern´andez, x. franch, a. jedlitschka, m. oriol, and a. trendowicz, “developing and operating
artiﬁcial intelligence models in trustworthy autonomous systems,” in international conference on research
challenges in information science.
springer, 2021, pp. 221–229.
[96] t. hirsch, k. merced, s. narayanan, z. e. imel, and d. c. atkins, “designing contestability: interaction design,
machine learning, and mental health,” in proceedings of the 2017 conference on designing interactive systems,
2017, pp. 95–99.
[97] m. m. john, h. holmstr¨om olsson, and j. bosch, “architecting ai deployment: a systematic review of state-of-
the-art and state-of-practice literature,” in international conference on software business.
springer, 2020, pp.
14–29.
[98] d. schiff, b. rakova, a. ayesh, a. fanti, and m. lennon, “principles to practices for responsible ai: closing the
gap,” arxiv preprint arxiv:2006.04707, 2020.
[99] r. v. zicari, j. brodersen, j. brusseau, b. d¨udder, t. eichhorn, t. ivanov, g. kararigas, p. kringen, m. mccul-
lough, f. m¨oslein et al., “z-inspection®: a process to assess trustworthy ai,” ieee transactions on technology
and society, vol. 2, no. 2, pp. 83–97, 2021.
30
a preprint - september 16, 2022
[100] j. henderson, s. sharma, a. gee, v. alexiev, s. draper, c. marin, y. hinojosa, c. draper, m. perng, l. aguirre
et al., “certifai: a toolkit for building trust in ai systems,” in proceedings of the twenty-ninth international
conference on international joint conferences on artiﬁcial intelligence, 2021, pp. 5249–5251.
[101] m. staples, l. zhu, and j. grundy, “continuous validation for data analytics systems,” in proceedings of the 38th
international conference on software engineering companion, 2016, pp. 769–772.
[102] s. k. lo, q. lu, l. zhu, h.-y. paik, x. xu, and c. wang, “architectural patterns for the design of federated
learning systems,” journal of systems and software, vol. 191, p. 111357, 2022.
[103] t. u. s. d. of commerce, “the minimum elements for a software bill of materials (sbom),” https://www.ntia.doc
.gov/files/ntia/publications/sbom minimum elements report.pdf, 2021, accessed 17 aug 2022.
[104] i. barclay, a. preece, i. taylor, s. k. radha, and j. nabrzyski, “providing assurance and scrutability on shared
data and machine learning models with veriﬁable credentials,” concurrency and computation: practice and
experience, p. e6997, 2022.
[105] w. chu, “a decentralized approach towards responsible ai in social ecosystems,” in proceedings of the interna-
tional aaai conference on web and social media, vol. 16, 2022, pp. 79–89.
[106] m. c. paulk, b. curtis, m. b. chrissis, and c. v. weber, “capability maturity model, version 1.1,” ieee software,
vol. 10, no. 4, pp. 18–27, 1993.
[107] w. w. w. consortium et al., “veriﬁable credentials data model 1.0: expressing veriﬁable information on the
web,” https://www. w3. org/tr/vc-data-model/?# core-data-model, 2019.
[108] k. bonawitz, v. ivanov, b. kreuter, a. marcedone, h. b. mcmahan, s. patel, d. ramage, a. segal, and k. seth,
“practical secure aggregation for privacy-preserving machine learning,” in proceedings of the 2017 acm sigsac
conference on computer and communications security, 2017, pp. 1175–1191.
[109] a. a. s¨uzen and m. a. s¸ims¸ek, “a novel approach to machine learning application to protection privacy data in
healthcare: federated learning,” namık kemal tıp dergisi, vol. 8, no. 1, pp. 22–30, 2020.
[110] s. bennati and c. m. jonker, “primal: a privacy-preserving machine learning method for event detection in
distributed sensor networks,” arxiv preprint arxiv:1703.07150, 2017.
[111] w. verachtert, t. j. ashby, i. chakroun, r. wuyts, s. das, s. halder, and p. leray, “privacy preserving amalga-
mated machine learning for process control,” in metrology, inspection, and process control for semiconductor
manufacturing xxxv, vol. 11611.
spie, 2021, pp. 329–341.
[112] n. sugianto, d. tjondronegoro, r. stockdale, and e. i. yuwono, “privacy-preserving ai-enabled video surveil-
lance for social distancing: responsible design and deployment for public spaces,” information technology &
people, 2021.
[113] s. k. lo, y. liu, q. lu, c. wang, x. xu, h.-y. paik, and l. zhu, “blockchain-based trustworthy federated
learning architecture,” arxiv preprint arxiv:2108.06912, 2021.
[114] s. warnat-herresthal, h. schultze, k. l. shastry, s. manamohan, s. mukherjee, v. garg, r. sarveswara,
k. h¨andler, p. pickkers, n. a. aziz et al., “swarm learning for decentralized and conﬁdential clinical machine
learning,” nature, vol. 594, no. 7862, pp. 265–270, 2021.
[115] p. vassilakopoulou, “sociotechnical approach for accountability by design in ai systems.” in ecis, 2020.
[116] microsoft, “microsoft hax toolkit,” https://www.microsoft.com/en-us/haxtoolkit/, 2022, accessed: 17 aug 2022.
[117] j. dai, s. lei, l. dong, x. lin, h. zhang, d. sun, and k. yuan, “more reliable ai solution: breast ultrasound
diagnosis using multi-ai combination,” arxiv preprint arxiv:2101.02639, 2021.
[118] m. nafreen, s. bhattacharya, and l. fiondella, “architecture-based software reliability incorporating fault
tolerant machine learning,” in 2020 annual reliability and maintainability symposium (rams).
ieee, 2020,
pp. 1–6.
[119] j. c. knight, “n-version programming,” encyclopedia of software engineering, 2002.
[120] a. lavaei, b. zhong, m. caccamo, and m. zamani, “towards trustworthy ai: safe-visor architecture for
uncertiﬁed controllers in stochastic cyber-physical systems,” in proceedings of the workshop on computation-
aware algorithmic design for cyber-physical systems, 2021, pp. 7–8.
[121] i. esnaola-gonzalez, “an ontology-based approach for making machine learning systems accountable,” 2021.
[122] m. shafto, m. conroy, r. doyle, e. glaessgen, c. kemp, j. lemoigne, and l. wang, “modeling, simulation,
information technology & processing roadmap,” national aeronautics and space administration, vol. 32, no.
2012, pp. 1–38, 2012.
31
a preprint - september 16, 2022
[123] j. weng, j. weng, j. zhang, m. li, y. zhang, and w. luo, “deepchain: auditable and privacy-preserving deep
learning with blockchain-based incentive,” ieee transactions on dependable and secure computing, vol. 18,
no. 5, pp. 2438–2455, 2019.
[124] w. zhang, q. lu, q. yu, z. li, y. liu, s. k. lo, s. chen, x. xu, and l. zhu, “blockchain-based federated learning
for device failure detection in industrial iot,” ieee internet of things journal, vol. 8, no. 7, pp. 5926–5937, 2020.
[125] g. falco and j. e. siegel, “a distributedblack box’audit trail design speciﬁcation for connected and automated
vehicle data and software assurance,” arxiv preprint arxiv:2002.02780, 2020.
[126] a. f. winﬁeld and m. jirotka, “the case for an ethical black box,” in annual conference towards autonomous
robotic systems.
springer, 2017, pp. 262–273.
[127] g. falco, b. shneiderman, j. badger, r. carrier, a. dahbura, d. danks, m. eling, a. goodloe, j. gupta, c. hart
et al., “governing ai safety through independent audits,” nature machine intelligence, vol. 3, no. 7, pp. 566–571,
2021.
[128] b. s. miguel, a. naseer, and h. inakoshi, “putting accountability of ai systems into practice,” in proceedings of
the twenty-ninth international conference on international joint conferences on artiﬁcial intelligence, 2021,
pp. 5276–5278.
[129] oecd, “tools for trustworthy ai,” 2021. [online]. available: https://www.oecd-ilibrary.org/content/paper/0082
32ec-en
[130] k. smit, m. zoet, and j. van meerten, “a review of ai principles in practice,” 2020.
[131] m. anagnostou, o. karvounidou, c. katritzidaki, c. kechagia, k. melidou, e. mpeza, i. konstantinidis,
e. kapantai, c. berberidis, i. magnisalis et al., “characteristics and challenges in the industries towards
responsible ai: a systematic literature review,” ethics and information technology, vol. 24, no. 3, pp. 1–18, 2022.
[132] q. lu, l. zhu, x. xu, j. whittle, and z. xing, “towards a roadmap on software engineering for responsible ai,”
arxiv preprint arxiv:2203.08594, 2022.
32
"
2209.07282.pdf;"
references
[1] 2017. unified modeling language. standard. object management group (omg).
https://www.omg.org/spec/uml/2.5.1/about-uml/
[2] zeeshan et al. ahmed. 2019. machine learning at microsoft with ml. net.
in proceedings of the 25th acm sigkdd international conference on knowledge
discovery & data mining. 2448–2458.
[3] a al-imam. 2019. a gateway towards machine learning: predictive analytics
and neural networks in ibm-spss (spss v. 24). retrieved january 3 (2019), 2019.
[4] abdallah atouani, jörg christian kirchhof, evgeny kusmenko, and bernhard
rumpe. 2021. artifact and reference models for generative machine learning
frameworks and build systems. in gpce’21. 55–68.
[5] autonialm 2018. ukrit wattanavaekin’s master’s thesis source code - automated
data analytics for motifs and discords mining. https://github.com/ukritw/
autonialm. accessed: 2022-03-24.
[6] michael r. berthold, nicolas cebron, fabian dill, thomas r. gabriel, tobias
kötter, thorsten meinl, peter ohl, kilian thiel, and bernd wiswedel. 2009. knime
- the konstanz information miner: version 2.0 and beyond. sigkdd explor. newsl.
11, 1 (nov. 2009), 26–31. https://doi.org/10.1145/1656274.1656280
[7] christopher m. bishop. 2013. model-based machine learning. philosophical
transactions of the royal society a 371, 1984 (february 2013), 1–17.
https:
//doi.org/10.1098/rsta.2012.0222
[8] tianqi chen, mu li, yutian li, min lin, naiyan wang, minjie wang, tianjun
xiao, bing xu, chiyuan zhang, and zheng zhang. 2015. mxnet: a flexible
and efficient machine learning library for heterogeneous distributed systems.
arxiv:1512.01274 [cs.dc]
[9] françois chollet et al. 2015. keras. https://keras.io.
[10] ronan collobert, koray kavukcuoglu, and clément farabet. 2011. torch7: a
matlab-like environment for machine learning. in nips 2011.
[11] corinna cortes, xavier gonzalvo, vitaly kuznetsov, mehryar mohri, and scott
yang. 2017. adanet: adaptive structural learning of artificial neural networks. in
international conference on machine learning. pmlr, 874–883.
[12] emf (publication date not applicable). eclipse modeling framework (emf).
https://www.eclipse.org/modeling/emf/. accessed: 2022-03-23.
[13] martín abadi et al. 2015. tensorflow: large-scale machine learning on hetero-
geneous systems. http://tensorflow.org/ software available from tensorflow.org.
[14] nicola gatto, evgeny kusmenko, and bernhard rumpe. 2019. modeling deep
reinforcement learning based architectures for cyber-physical systems. in
models 2019. workshop mde intelligence (munich). 196–202.
[15] mark hall, eibe frank, geoffrey holmes, bernhard pfahringer, peter reutemann,
and ian h. witten. 2009. the weka data mining software: an update. sigkdd
explor. newsl. 11, 1 (nov. 2009), 10–18.
[16] nicolas harrand, franck fleurey, brice morin, and knut eilif husa. 2016.
thingml: a language and code generation framework for heterogeneous
targets. in proceedings of the acm/ieee 19th international conference on model
driven engineering languages and systems (models ’16).
[17] thomas hartmann, francois fouquet, assaad moawad, romain rouvoy, and
yves le traon. 2018. greycat: efficient what-if analytics for data in motion at
scale. arxiv:1803.09627 [cs.db]
[18] thomas hartmann, assaad moawad, francois fouquet, and yves le traon. 2017.
the next evolution of mde: a seamless integration of machine learning into do-
main modeling. in 2017 models’17. 180–180. https://doi.org/10.1109/models.
2017.32
[19] t. hartmann, a. moawad, f. fouquet, and y. le traon. 2019. the next evolution of
mde: a seamless integration of machine learning into domain modeling. software
and system modeling (sosym) 18 (may 2019), 1285–1304. https://doi.org/10.1007/
s10270-017-0600-2
[20] nils kaminski, evgeny kusmenko, and bernhard rumpe. 2019. modeling dy-
namic architectures of self-adaptive cooperative systems. the journal of object
technology 18, 2 (july 2019), 1–20. https://doi.org/10.5381/jot.2019.18.2.a2 the
15th european conference on modelling foundations and applications.
[21] jörg christian kirchhof, bernhard rumpe, david schmalzing, and andreas wort-
mann. 2022. montithings: model-driven development and deployment of reli-
able iot applications. journal of systems and software 183 (january 2022), 1–21.
https://doi.org/10.1016/j.jss.2021.111087
[22] evgeny kusmenko. 2021. model-driven development methodology and domain-
specific languages for the design of artificial intelligence in cyber-physical
systems. shaker verlag.
http://www.se-rwth.de/phdtheses/diss-kusmenko-
model-driven-development-methodology-and-domain-specific-languages-
for-the-design-of-artificial-intelligence-in-cyber-physical-systems.pdf
[23] evgeny kusmenko, sebastian nickels, svetlana pavlitskaya, bernhard rumpe,
and thomas timmermanns. 2019. modeling and training of neural processing
systems. in models’19 (munich). ieee, 283–293.
[24] evgeny kusmenko, svetlana pavlitskaya, bernhard rumpe, and sebastian stüber.
2019. on the engineering of ai-powered systems. in ase19. software engineering
intelligence workshop (sei19) (san diego, california, usa), lisa o’conner (ed.).
ieee, 126–133. http://www.se-rwth.de/publications/on-the-engineering-of-ai-
powered-systems.pdf
[25] evgeny kusmenko, alexander roth, bernhard rumpe, and michael von wenck-
stern. 2017. modeling architectures of cyber-physical systems. in ecmfa’17
(marburg) (lncs 10376). springer, 34–50.
[26] evgeny kusmenko, bernhard rumpe, sascha schneiders, and michael von wenck-
stern. 2018. highly-optimizing and multi-target compiler for embedded system
models: c++ compiler toolchain for the component and connector language
embeddedmontiarc. in models’18 (copenhagen). acm, 447 – 457.
[27] yann lecun, yoshua bengio, and geoffrey hinton. 2015. deep learning. nature
521, 7553 (2015), 436–444.
[28] jure leskovec, anand rajaraman, and jeffrey david ullman. 2014. mining of
massive datasets (2nd ed.). cambridge university press, usa. http://www.mmds.
org
[29] t. minka, j. m. winn, j. p. guiver, y. zaykov, d. fabian, and j. bronskill. 2018.
infer.net 0.3.
microsoft research cambridge, http://dotnet.github.io/infer,
accessed: 2020-09-08.
[30] ml-quadrat 2020. ml2. https://github.com/arminmoin/ml-quadrat. accessed:
2020-09-12.
[31] armin moin, moharram challenger, atta badii, and stephan günnemann. 2022.
a model-driven approach to machine learning and software modeling for the iot.
software and systems modeling (sosym) (2022). https://doi.org/10.1007/s10270-
021-00967-x
[32] armin moin, moharram challenger, atta badii, and stephan günnemann. 2022.
supporting ai engineering on the iot edge through model-driven tinyml.
https://arxiv.org/abs/2107.02690
[33] onnx (publication date not applicable). open neural network exchange. https:
//github.com/onnx. accessed: 2021-03-09.
[34] f. pedregosa, g. varoquaux, a. gramfort, v. michel, b. thirion, o. grisel, m.
blondel, p. prettenhofer, r. weiss, v. dubourg, j. vanderplas, a. passos, d. cour-
napeau, m. brucher, m. perrot, and e. duchesnay. 2011. scikit-learn: machine
learning in python. journal of machine learning research 12 (2011), 2825–2830.
[35] rapidminer (publication date not applicable). depth for data scientists, simplified
for everyone else. https://rapidminer.com. accessed: 2021-09-08.
[36] tensorboard (publication date not applicable). tensorflow’s visualization toolkit.
https://www.tensorflow.org/tensorboard. accessed: 2021-09-08.
[37] things modeling language 2015.
thingml.
https://github.com/telluiot/
thingml. accessed: 2020-04-29.
[38] jon whittle, john hutchinson, and mark rouncefield. 2014. the state of practice
in model-driven engineering. ieee software 31, 3 (2014), 79–85. https://doi.org/
10.1109/ms.2013.65
[39] xtext (publication date not applicable). language engineering for ev-
eryone! https://www.eclipse.org/xtext/. accessed: 2022-03-20.
"
2209.09125.pdf;"
references
[1] evaluating a model - advice for applying machine learning.
[2] single number evaluation metric - ml strategy.
[3] leonel aguilar, david dao, shaoduo gan, nezihe merve gurel, nora hollenstein,
jiawei jiang, bojan karlas, thomas lemmin, tian li, yang li, susie rao, johannes
rausch, cedric renggli, luka rimanic, maurice weber, shuai zhang, zhikuan
zhao, kevin schawinski, wentao wu, and ce zhang. ease.ml: a lifecycle man-
agement system for mldev and mlops. in conference on innovative data systems
research (cidr 2021), january 2021.
[4] sridhar alla and suman kalyan adari. what is mlops? in beginning mlops with
mlflow, pages 79–124. springer, 2021.
[5] saleema amershi, andrew begel, christian bird, robert deline, harald gall,
ece kamar, nachiappan nagappan, besmira nushi, and thomas zimmermann.
software engineering for machine learning: a case study. in 2019 ieee/acm 41st
international conference on software engineering: software engineering in practice
(icse-seip), pages 291–300, 2019.
[6] lukas biewald. tracking with weights and biases www.wandb.com/, 2020.
[7] catherine billington, gonzalo rivero, andrew jannett, and jiating chen. a
machine learning model helps process interviewer comments in computer-
assisted personal interview instruments: a case study. field methods, page
1525822x221107053, 2022.
[8] mike brachmann, carlos bautista, sonia castelo, su feng, juliana freire, boris
glavic, oliver kennedy, heiko müeller, rémi rampin, william spoth, and ying
operationalizing machine learning: an interview study
yang. data debugging and exploration with vizier. in proceedings of the 2019
international conference on management of data, sigmod ’19, page 1877–1880,
new york, ny, usa, 2019. association for computing machinery.
[9] eric breck, marty zinkevich, neoklis polyzotis, steven whang, and sudip roy.
data validation for machine learning. in proceedings of sysml, 2019.
[10] ji young cho and eun-hee lee. reducing confusion about grounded theory
and qualitative content analysis: similarities and differences. qualitative report,
19(32), 2014.
[11] xu chu, ihab f. ilyas, sanjay krishnan, and jiannan wang. data cleaning:
overview and emerging challenges. in proceedings of the 2016 international
conference on management of data, sigmod ’16, page 2201–2206, new york,
ny, usa, 2016. association for computing machinery.
[12] daniel crankshaw, xin wang, guilio zhou, michael j franklin, joseph e gonzalez,
and ion stoica. clipper: a {low-latency} online prediction serving system. in
14th usenix symposium on networked systems design and implementation (nsdi
17), pages 613–627, 2017.
[13] christof ebert, gorka gallardo, josune hernantes, and nicolas serrano. devops.
ieee software, 33(3):94–100, 2016.
[14] mihail eric. mlops is a mess but that’s to be expected.
[15] tongtong fang, nan lu, gang niu, and masashi sugiyama. rethinking impor-
tance weighting for deep learning under distribution shift. in h. larochelle,
m. ranzato, r. hadsell, m.f. balcan, and h. lin, editors, advances in neural in-
formation processing systems, volume 33, pages 11996–12007. curran associates,
inc., 2020.
[16] asbjørn følstad, cecilie bertinussen nordheim, and cato alexander bjørkli.
what makes users trust a chatbot for customer service? an exploratory interview
study. in international conference on internet science, pages 194–208. springer,
2018.
[17] rolando garcia, eric liu, vikram sreekanti, bobby yan, anusha dandamudi,
joseph e. gonzalez, joseph m hellerstein, and koushik sen. hindsight logging
for model training. in vldb, 2021.
[18] rolando garcia, vikram sreekanti, neeraja yadwadkar, daniel crankshaw,
joseph e gonzalez, and joseph m hellerstein. context: the missing piece in the
machine learning lifecycle. in cmi, 2018.
[19] satvik garg, pradyumn pundir, geetanjali rathee, p.k. gupta, somya garg, and
saransh ahlawat. on continuous integration / continuous delivery for automated
deployment of machine learning models using mlops. in 2021 ieee fourth inter-
national conference on artificial intelligence and knowledge engineering (aike),
pages 25–28, 2021.
[20] inc gartner. understanding mlops to operationalize machine learning projects.
[21] samadrita ghosh. mlops challenges and how to face them, aug 2021.
[22] stefan grafberger, shubha guha, julia stoyanovich, and sebastian schelter. mlin-
spect: a data distribution debugger for machine learning pipelines. in sigmod’21,
2021.
[23] andrew head, fred hohman, titus barik, steven m. drucker, and robert deline.
managing messes in computational notebooks. in proceedings of the 2019 chi
conference on human factors in computing systems, chi ’19, page 1–12, new
york, ny, usa, 2019. association for computing machinery.
[24] joseph m hellerstein, vikram sreekanti, joseph e gonzalez, james dalton,
akon dey, sreyashi nag, krishna ramachandran, sudhanshu arora, arka bhat-
tacharyya, shirshanka das, et al. ground: a data context service. in cidr,
2017.
[25] fred hohman, kanit wongsuphasawat, mary beth kery, and kayur patel. un-
derstanding and visualizing data iteration in machine learning. in proceedings of
the 2020 chi conference on human factors in computing systems, pages 1–13, 2020.
[26] kenneth holstein, jennifer wortman vaughan, hal daumé, miro dudik, and
hanna wallach. improving fairness in machine learning systems: what do
industry practitioners need? in proceedings of the 2019 chi conference on human
factors in computing systems, chi ’19, page 1–16, new york, ny, usa, 2019.
association for computing machinery.
[27] chip huyen. machine learning tools landscape v2 (+84 new tools), dec 2020.
[28] meenu mary john, helena holmström olsson, and jan bosch. towards mlops: a
framework and maturity model. in 2021 47th euromicro conference on software
engineering and advanced applications (seaa), pages 1–8. ieee, 2021.
[29] sean kandel, andreas paepcke, joseph hellerstein, and jeffrey heer. wrangler:
interactive visual specification of data transformation scripts. in proceedings of
the sigchi conference on human factors in computing systems, pages 3363–3372,
2011.
[30] sean kandel, andreas paepcke, joseph m. hellerstein, and jeffrey heer. enter-
prise data analysis and visualization: an interview study. ieee transactions on
visualization and computer graphics, 18(12):2917–2926, 2012.
[31] daniel kang, deepti raghavan, peter bailis, and matei zaharia. model assertions
for debugging machine learning.
[32] mary beth kery, amber horvath, and brad a myers. variolite: supporting
exploratory programming by data scientists. in chi, volume 10, pages 3025453–
3025626, 2017.
[33] miryung kim, thomas zimmermann, robert deline, and andrew begel. data
scientists in software teams: state of the art and challenges. ieee transactions on
software engineering, 44(11):1024–1038, 2017.
[34] dominik kreuzberger, niklas kühl, and sebastian hirschl. machine learning
operations (mlops): overview, definition, and architecture, 2022.
[35] po-ming law, sana malik, fan du, and moumita sinha. designing tools for
semi-automated detection of machine learning biases: an interview study. arxiv
preprint arxiv:2003.07680, 2020.
[36] angela lee, doris xin, doris lee, and aditya parameswaran. demystifying a
dark art: understanding real-world machine learning model development, 2020.
[37] leonardo leite, carla rocha, fabio kon, dejan milojicic, and paulo meirelles.
a survey of devops concepts and challenges. acm computing surveys (csur),
52(6):1–35, 2019.
[38] zhiqiu lin, jia shi, deepak pathak, and deva ramanan. the clear benchmark:
continual learning on real-world imagery. in thirty-fifth conference on neural
information processing systems datasets and benchmarks track (round 2), 2021.
[39] mike loukides. what is devops? "" o’reilly media, inc."", 2012.
[40] lucy ellen lwakatare, terhi kilamo, teemu karvonen, tanja sauvola, ville
heikkilä, juha itkonen, pasi kuvaja, tommi mikkonen, markku oivo, and casper
lassenius. devops in practice: a multiple case study of five companies. informa-
tion and software technology, 114:217–230, 2019.
[41] lucy ellen lwakatare, aiswarya raj, j. bosch, helena holmström olsson, and
ivica crnkovic. a taxonomy of software engineering challenges for machine
learning systems: an empirical investigation. in xp, 2019.
[42] stephen macke, hongpu gong, doris jung-lin lee, andrew head, doris xin,
and aditya parameswaran. fine-grained lineage for safer notebook interactions.
proc. vldb endow., 14(6):1093–1101, sep 2021.
[43] michael a. madaio, luke stark, jennifer wortman vaughan, and hanna wallach.
co-designing checklists to understand organizational challenges and opportuni-
ties around fairness in ai. in proceedings of the 2020 chi conference on human
factors in computing systems, chi ’20, page 1–14, new york, ny, usa, 2020.
association for computing machinery.
[44] sasu mäkinen, henrik skogström, eero laaksonen, and tommi mikkonen. who
needs mlops: what data scientists seek to accomplish and how can mlops help?
in 2021 ieee/acm 1st workshop on ai engineering-software engineering for ai
(wain), pages 109–112. ieee, 2021.
[45] mlreef. global mlops and ml tools landscape: mlreef, feb 2021.
[46] jose g. moreno-torres, troy raeder, rocío alaiz-rodríguez, nitesh v. chawla,
and francisco herrera. a unifying view on dataset shift in classification. pattern
recognition, 45(1):521–530, 2012.
[47] dennis muiruri, lucy ellen lwakatare, jukka k nurminen, and tommi mikko-
nen. practices and infrastructures for ml systems–an interview study in finnish
organizations. 2022.
[48] michael muller. curiosity, creativity, and surprise as analytic tools: grounded
theory method. in ways of knowing in hci, pages 25–48. springer, 2014.
[49] michael muller, ingrid lange, dakuo wang, david piorkowski, jason tsay, q vera
liao, casey dugan, and thomas erickson. how data science workers work with
data: discovery, capture, curation, design, creation. in proceedings of the 2019
chi conference on human factors in computing systems, pages 1–15, 2019.
[50] mohammad hossein namaki, avrilia floratou, fotis psallidas, subru krishnan,
ashvin agrawal, yinghui wu, yiwen zhu, and markus weimer. vamsa: auto-
mated provenance tracking in data science scripts. in proceedings of the 26th
acm sigkdd international conference on knowledge discovery & data mining,
pages 1542–1551, 2020.
[51] yaniv ovadia, emily fertig, j. ren, zachary nado, d. sculley, sebastian nowozin,
joshua v. dillon, balaji lakshminarayanan, and jasper snoek. can you trust
your model’s uncertainty? evaluating predictive uncertainty under dataset shift.
in neurips, 2019.
[52] andrei paleyes, raoul-gabriel urma, and neil d. lawrence. challenges in de-
ploying machine learning: a survey of case studies. acm comput. surv., apr
2022. just accepted.
[53] samir passi and steven j jackson. trust in data science: collaboration, translation,
and accountability in corporate data science projects. proceedings of the acm on
human-computer interaction, 2(cscw):1–28, 2018.
[54] neoklis polyzotis, sudip roy, steven euijong whang, and martin zinkevich. data
management challenges in production machine learning. in proceedings of the
2017 acm international conference on management of data, pages 1723–1726,
2017.
[55] neoklis polyzotis, sudip roy, steven euijong whang, and martin zinkevich.
data lifecycle challenges in production machine learning: a survey. sigmod
record, 47(2):12, 2018.
[56] luisa pumplun, mariska fecho, nihal wahl, felix peters, peter buxmann, et al.
adoption of machine learning systems for medical diagnostics in clinics: qualita-
tive interview study. journal of medical internet research, 23(10):e29301, 2021.
[57] stephan rabanser, stephan günnemann, and zachary chase lipton. failing
loudly: an empirical study of methods for detecting dataset shift. in neurips,
2019.
[58] alexander ratner, stephen h bach, henry ehrenberg, jason fries, sen wu, and
christopher ré. snorkel: rapid training data creation with weak supervision. in
proceedings of the vldb endowment. international conference on very large data
shreya shankar∗, rolando garcia∗, joseph m. hellerstein, aditya g. parameswaran
bases, volume 11, page 269. nih public access, 2017.
[59] cedric renggli, luka rimanic, nezihe merve gürel, bojan karlaš, wentao wu, and
ce zhang. a data quality-driven view of mlops. arxiv preprint arxiv:2102.07750,
2021.
[60] e. rezig et al. dagger: a data (not code) debugger. in cidr, 2020.
[61] philip russom et al. big data analytics. tdwi best practices report, fourth quarter,
19(4):1–34, 2011.
[62] nithya sambasivan, shivani kapania, hannah highfill, diana akrong, praveen
paritosh, and lora m aroyo. “everyone wants to do the model work, not the data
work”: data cascades in high-stakes ai. in proceedings of the 2021 chi conference
on human factors in computing systems, pages 1–15, 2021.
[63] sebastian schelter et al. automating large-scale data quality verification. in
pvldb’18, 2018.
[64] d. sculley, gary holt, daniel golovin, eugene davydov, todd phillips, diet-
mar ebner, vinay chaudhary, michael young, jean-françois crespo, and dan
dennison. hidden technical debt in machine learning systems. in nips, 2015.
[65] shreya shankar, bernease herman, and aditya g. parameswaran. rethinking
streaming machine learning evaluation. arxiv, abs/2205.11473, 2022.
[66] shreya shankar, stephen macke, sarah chasins, andrew head, and aditya
parameswaran. bolt-on, compact, and rapid program slicing for notebooks.
proc. vldb endow., sep 2023.
[67] shreya shankar and aditya g. parameswaran. towards observability for produc-
tion machine learning pipelines. arxiv, abs/2108.13557, 2022.
[68] james p spradley. the ethnographic interview. waveland press, 2016.
[69] steve nunez. why ai investments fail to deliver, 2022. [online; accessed 15-
september-2022].
[70] anselm strauss and juliet corbin. grounded theory methodology: an overview.
1994.
[71] masashi sugiyama et al. covariate shift adaptation by importance weighted
cross validation. in jmlr, 2007.
[72] justin talbot, bongshin lee, ashish kapoor, and desney s. tan. ensemblematrix:
interactive visualization to support machine learning with multiple classifiers.
in proceedings of the sigchi conference on human factors in computing systems,
chi ’09, page 1283–1292, new york, ny, usa, 2009. association for computing
machinery.
[73] damian a tamburri. sustainable mlops: trends and challenges. in 2020 22nd in-
ternational symposium on symbolic and numeric algorithms for scientific computing
(synasc), pages 17–23. ieee, 2020.
[74] manasi vartak. modeldb: a system for machine learning model management. in
hilda ’16, 2016.
[75] dakuo wang, justin d. weisz, michael muller, parikshit ram, werner geyer,
casey dugan, yla tausczik, horst samulowitz, and alexander gray. human-ai
collaboration in data science: exploring data scientists’ perceptions of automated
ai. proc. acm hum.-comput. interact., 3(cscw), nov 2019.
[76] joyce weiner. why ai/data science projects fail: how to avoid project pitfalls.
synthesis lectures on computation and analytics, 1(1):i–77, 2020.
[77] wikipedia contributors. mlops — wikipedia, the free encyclopedia, 2022. [online;
accessed 15-september-2022].
[78] olivia wiles, sven gowal, florian stimberg, sylvestre-alvise rebuffi, ira ktena,
krishnamurthy dvijotham, and ali taylan cemgil. a fine-grained analysis on
distribution shift. arxiv, abs/2110.11328, 2021.
[79] doris xin, hui miao, aditya parameswaran, and neoklis polyzotis. production
machine learning pipelines: empirical analysis and optimization opportunities.
in proceedings of the 2021 international conference on management of data, pages
2639–2652, 2021.
[80] doris xin, eva yiwei wu, doris jung-lin lee, niloufar salehi, and aditya
parameswaran. whither automl? understanding the role of automation in ma-
chine learning workflows. in proceedings of the 2021 chi conference on human
factors in computing systems, chi ’21, new york, ny, usa, 2021. association
for computing machinery.
[81] m. zaharia et al. accelerating the machine learning lifecycle with mlflow. ieee
data eng. bull., 41:39–45, 2018.
[82] amy x zhang, michael muller, and dakuo wang. how do data science workers
collaborate? roles, workflows, and tools. proceedings of the acm on human-
computer interaction, 4(cscw1):1–23, 2020.
[83] marvin zhang, henrik marklund, abhishek gupta, sergey levine, and chelsea
finn. adaptive risk minimization: a meta-learning approach for tackling group
shift. corr, abs/2007.02931, 2020.
operationalizing machine learning: an interview study
a
semi-structured interview
questions
in the beginning of each interview, we explained the purpose of
the interview—to better understand processes within the organi-
zation for validating changes made to production ml models, ide-
ally through stories of ml deployments. we then kickstarted the
information-gathering process with a question to build rapport
with the interviewee, such as tell us about a memorable previous ml
model deployment. this question helped us isolate an ml pipeline or
product to discuss. we then asked a series of open-ended questions:
(1) nature of ml task
• what is the ml task you are trying to solve?
• is it a classification or regression task?
• are the class representations balanced?
(2) modeling and experimentation ideas
• how do you come up with experiment ideas?
• what models do you use?
• how do you know if an experiment idea is good?
• what fraction of your experiment ideas are good?
(3) transition from development to production
• what processes do you follow for promoting a model from
the development phase to production?
• how many pull requests do you make or review?
• what do you look for in code reviews?
• what automated tests run at this time?
(4) validation datasets
• how did you come up with the dataset to evaluate the
model on?
• do the validation datasets ever change?
• does every engineer working on this ml task use the same
validation datasets?
(5) monitoring
• do you track the performance of your model?
• if so, when and how do you refresh the metrics?
• what information do you log?
• do you record provenance?
• how do you learn of an ml-related bug?
(6) response
• what historical records (e.g., training code, training set)
do you inspect in the debugging process?
• what organizational processes do you have for responding
to ml-related bugs?
• do you make tickets (e.g., jira) for these bugs?
• how do you react to these bugs?
• when do you decide to retrain the model?
b
interview transcripts
histograms of the number of codes and sentences in the interview
transcripts are shown in figures 3a and 3b, respectively.
c
codes
across the interview transcripts, we had a total of 1766 coded seg-
ments, with exactly 600 unique codes. we organized codes into
hierarchies. table 3 shows the most frequently occurring codes,
ordered by the number of distinct interviews the codes appeared
in (not the raw number of occurrences across all documents). fig-
ure 4 displays the top five correlated codes for each top-level or
parent code. two codes are correlated if they occur within twenty
sentences of each other.
d
mlops tool stack
table 2 shows common tools used by mles across layers of the
stack and tasks in the production ml lifecycle.
shreya shankar∗, rolando garcia∗, joseph m. hellerstein, aditya g. parameswaran
data collection
experimentation
evaluation and deployment
monitoring and response
run
know what data is avail-
able and where it lives
prototype ideas and track
results
catch errors in training
(e.g., overfitting)
track ml metrics over
time
data catalogs, amund-
sen, aws glue, hive
metastores
weights & biases, mlflow, train/test set parameter
configs, a/b test tracking tools
dashboards, sql, met-
ric functions and window
sizes
pipeline
regularly scheduled, pos-
sibly outsourced
ad-hoc or user-triggered,
hyperparameter search
scheduled refresh of hold-
out validation sets
scheduled computation
of metrics and triggered
alerts
in-house or outsourced
annotators
automl
github actions, travis ci,
prediction serving tools,
kafka, flink
prometheus, aws cloud-
watch
airflow, kubeflow, argo, tensorflow extended (tfx), vertex ai, dbt
component
sourcing, labeling, clean-
ing
feature generation and
selection, model training
running model on hold-
out validation set, model
compression or rewrite,
model serialization
data validation, ml met-
ric computation, tracing
predictions
data cleaning tools
tensorflow, mllib, py-
torch, scikit-learn, xg-
boost
c++, onnx, octoml,
tvm, joblib, pickle
scikit-learn metric func-
tions, great expectations,
deequ
python, pandas, spark, sql
infrastructure
velocity
velocity
validate early
versioning
annotation
schema,
cleaning criteria configs
jupyter notebook setups,
gpus
edge devices, cpus
logging and observabil-
ity services (e.g., data-
dog)
cloud (e.g., aws, gcp), compute clusters, storage (e.g., aws s3, snowflake), docker, kubernetes
table 2: primary goals and tools for each layer in the mlops stack and routine task in the ml engineering workflow.
40
60
80
100
120
140
160
number of coded segments
0
1
2
3
4
number of interviews
(a) histogram of number of coded segments in each interview.
400
450
500
550
600
650
number of sentences
0
1
2
3
4
number of interviews
(b) histogram of number of sentences in each interview.
figure 3: interview transcript statistics. each histogram has 10 equally-spaced buckets.
operationalizing machine learning: an interview study
parent code
code
# coded segments
# transcripts
1
known challenges
data drift/shift/skew
15
10
2
monitoring and response (+)
live monitoring
21
9
3
python (+)
jupyter
16
8
4
evaluation and deployment
build the infrastructure
16
8
5
data pipeline
data iteration, fresh data
15
8
6
fast & simple
high iteration speed, agile, rapid cycles
24
7
7
production bugs
debugging and bugs
18
7
8
tests
ab testing
14
7
9
software development
pull request
13
7
10
data pipeline
pipeline on a schedule
13
7
11
operations
model training & retraining
12
7
12
data ingest
automated featurization
9
7
13
evaluation and deployment
ci/cd
8
7
14
trends
per-customer model and many customers
15
6
15
known challenges
feedback delay
13
6
16
evaluation and deployment
metrics and validation
12
6
17
metrics and validation (+)
accuracy
11
6
18
models
deep learning
9
6
19
sandboxing
offline demonstration of value
7
6
20
apps & use-cases
ranking
7
6
table 3: top 20 codes, ordered by the number of distinct transcripts the codes were mentioned in, descending.
shreya shankar∗, rolando garcia∗, joseph m. hellerstein, aditya g. parameswaran
figure 4: correlated codes for each top-level code. each edge is weighted by the occurrence count for its pair of codes.
"
23642.pdf;"
references 
[1] rschmelzer, “global ai adoption trends & forecast 2020,” cognilytica, 22-jan-2020. [online]. 
available: https://www.cognilytica.com/2020/01/22/global-ai-adoption-trends-forecast-2020/. 
 
 
 
 
 
 
 
 
[2] algorithmia, “the state of enterprise ml 2020”, available: https://algorithmia.com/state-of-ml.  
[3] a. dyck, r. penners and h. lichter, ""towards definitions for release engineering and devops,"" 
2015 ieee/acm 3rd international workshop on release engineering, florence, 2015, pp. 3-3, 
doi: 10.1109/releng.2015.10. 
[4] i. buchanan, “history of devops.” [online]. available: https://www.atlassian.com/devops/what-
is-devops/history-of-devops. 
[5] jha, pratibha & khan, rizwan. (2018). a review paper on devops: beginning and more to 
know. international journal of computer applications. 180. 16-20. 10.5120/ijca2018917253. 
[6] “what is devops? devops explained: microsoft azure,” devops explained | microsoft azure. 
[online]. available: https://azure.microsoft.com/en-us/overview/what-is-devops/. 
[7] d. m. kersten, “project to product: a cambrian explosion of devops tools,” tasktop blog, 07-
feb-2019. [online]. available: https://www.tasktop.com/blog/a-cambrian-explosion-of-devops-
tools/. 
[8] “sate of devops 2019 | google cloud,” google. [online]. available: 
https://cloud.google.com/devops/state-of-devops.  
[9] sculley, d & holt, gary & golovin, daniel & davydov, eugene & phillips, todd & ebner, 
dietmar & chaudhary, vinay & young, michael & dennison, dan. (2015). hidden technical 
debt in machine learning systems. nips. 2494-2502. 
[10] 
y. roh, g. heo and s. e. whang, ""a survey on data collection for machine learning: a 
big data - ai integration perspective,"" in ieee transactions on knowledge and data 
engineering, doi: 10.1109/tkde.2019.2946162. 
[11] 
breck, eric & cai, shanqing & nielsen, eric & salib, michael & sculley, d.. (2017). the 
ml test score: a rubric for ml production readiness and technical debt reduction. 1123-1132. 
10.1109/bigdata.2017.8258038. 
[12] 
“ml-ops.org,” ml ops: machine learning operations, 25-nov-2020. [online]. available: 
https://ml-ops.org/content/mlops-principles.  
[13] 
k. hazelwood et al., ""applied machine learning at facebook: a datacenter 
infrastructure perspective,"" 2018 ieee international symposium on high performance computer 
architecture (hpca), vienna, 2018, pp. 620-629, doi: 10.1109/hpca.2018.00059. 
[14] 
katie o'leary, & makoto uchida, “common problems with creating machine learning 
pipelines from existing code” 2020 
[15] 
“mlops: continuous delivery and automation pipelines in machine learning,” google. 
[online]. available: https://cloud.google.com/solutions/machine-learning/mlops-continuous-
delivery-and-automation-pipelines-in-machine-learning. 
[16] 
s. 3 and r. merritt, “what is mlops?” the official nvidia blog, 23-oct-2020. 
[online]. available: https://blogs.nvidia.com/blog/2020/09/03/what-is-mlops/.  
[17] 
“design scalable ml architectures to deliver ai-based systems,” gartner. [online]. 
available: https://www.gartner.com/en/webinars/26551/design-scalable-ml-architectures-to-
deliver-ai-based-systems.  
[18] 
p. by j. baer, “the winding road to better machine learning infrastructure through 
tensorflow extended and kubeflow,” spotify engineering, 06-jul-2020. [online]. available: 
https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-
infrastructure-through-tensorflow-extended-and-kubeflow/. 
 
 
 
 
 
 
 
 
[19] 
googlecloudplatform, “introduction to kubeflow,” youtube, 28-mar-2020. [online]. 
available: https://www.youtube.com/watch?v=ctzardgbiww.  
[20] 
lf ai &amp; data landscape. [online]. available: https://landscape.lfai.foundation/.  
[21] 
ferreira leite, leonardo & rocha, carla & kon, fabio & milojicic, dejan & meirelles, 
paulo. (2019). a survey of devops concepts and challenges. 
[22] 
lee, jae-nam. (2001). the impact of knowledge sharing, organizational capability, and 
partnership quality on is outsourcing success. information & management. 38. 323-335. 
10.1016/s0378-7206(00)00074-4. 
[23] 
alteryx, “alteryx/featuretools,” github. [online]. available: 
https://github.com/featurelabs/featuretools. 
[24] 
blue-yonder, “blue-yonder/tsfresh,” github. [online]. available: 
https://github.com/blue-yonder/tsfresh. 
[25] 
google, “google/ml-metadata,” github. [online]. available: 
https://github.com/google/ml-metadata. 
[26] 
odpi, “odpi/egeria,” github. [online]. available: https://github.com/odpi/egeria. 
[27] 
project jupyter. [online]. available: https://jupyter.org/.  
[28] 
jupyter, “jupyter/nbdime,” github. [online]. available: 
https://github.com/jupyter/nbdime. 
[29] 
kubernetes, “kubernetes/kubernetes,” github. [online]. available: 
https://github.com/kubernetes/kubernetes.  
[30] 
apache, “apache/ambari,” github. [online]. available: 
https://github.com/apache/ambari. 
[31] 
ansible, “ansible/ansible,” github. [online]. available: 
https://github.com/ansible/ansible. 
[32] 
hashicorp, “hashicorp/terraform,” github. [online]. available: 
https://github.com/hashicorp/terraform.  
[33] 
t. m. mitchell, “machine learning,” amazon, 2017. [online]. available: 
https://docs.aws.amazon.com/machine-learning/latest/dg/training-ml-models.html.  
[34] 
“build automation,” wikipedia, 02-sep-2020. [online]. available: 
https://en.wikipedia.org/wiki/build_automation. 
[35] 
“machine learning workloads: why you need a modern infrastructure,” 
insidebigdata, 13-aug-2019. [online]. available: 
https://insidebigdata.com/2019/08/08/infrastructure-machine-learning-workloads/. 
[36] 
m. langer, z. he, w. rahayu and y. xue, ""distributed training of deep learning 
models: a taxonomic perspective,"" in ieee transactions on parallel and distributed systems, 
vol. 31, no. 12, pp. 2802-2818, 1 dec. 2020, doi: 10.1109/tpds.2020.3003307. 
[37] 
studioml, “studioml/studio,” github. [online]. available: 
https://github.com/studioml/studio. 
[38] 
quic, “quic/aimet,” github. [online]. available: https://github.com/quic/aimet.  
[39] 
horovod, “horovod/horovod,” github. [online]. available: 
https://github.com/horovod/horovod. 
 
 
 
 
 
 
 
 
[40] 
apache, “apache/airflow,” github. [online]. available: 
https://github.com/apache/airflow. 
[41] 
mlflow, “mlflow/mlflow,” github. [online]. available: 
https://github.com/mlflow/mlflow.  
[42] 
nguyen, giang & dlugolinsky, stefan & bobak, martin & tran, viet & lopez garcia, 
alvaro & heredia, ignacio & malík, peter & hluchý, ladislav. (2019). machine learning and 
deep learning frameworks and libraries for large-scale data mining: a survey. artificial 
intelligence review. 52. 77-124. 10.1007/s10462-018-09679-z. 
[43] 
seldonio, “seldonio/seldon-core,” github. [online]. available: 
https://github.com/seldonio/seldon-core. 
[44] 
bentoml, “bentoml/bentoml,” github. [online]. available: 
https://github.com/bentoml/bentoml. 
[45] 
cortexlabs, “cortexlabs/cortex,” github. [online]. available: 
https://github.com/cortexlabs/cortex. 
[46] 
eric breck et al., “data validation for machine learning”, 2019 in proceedings of sysml. 
[47] 
whylabs, “whylabs/whylogs-python,” github. [online]. available: 
https://github.com/whylabs/whylogs-python. 
[48] 
tensorflow, “tensorflow/tfx,” github. [online]. available: 
https://github.com/tensorflow/tfx. 
[49] 
mlops.community, slack. [online]. available: https://mlops-
community.slack.com/archives/c7dr1e5nc/p1602868762405100.  
[50] 
metaflow. [online]. available: https://metaflow.org/. 
[51] 
demetrios brinkmann, “mlops #11 machine learning at scale in mercado libre with 
carlos de la torre - mlops.community,” spotify, 16-may-2020. [online]. available: 
https://open.spotify.com/episode/5gjl3uihzb2yhxxtlwygg4?si=wvma2y7krlsi5me6zl72_w 
[52] 
“home,” open-source leader in ai and ml. [online]. available: https://www.h2o.ai/. 
[53] 
mljar. [online]. available: https://mljar.com/. 
[54] 
kubeflow, 15-aug-2020. [online]. available: 
https://www.kubeflow.org/docs/about/kubeflow/. 
[55] 
community, kubeflow, “kubeflow/community,” github. [online]. available: 
https://github.com/kubeflow/community/blob/master/member_organizations.yaml. 
[56] 
d. hudgeon and r. nichol, “machine learning for business: using amazon sagemaker 
and jupyter,” amazon, 2020. [online]. available: https://aws.amazon.com/sagemaker. 
[57] 
neo-ai, “neo-ai/neo-ai-dlr,” github. [online]. available: https://github.com/neo-ai/neo-
ai-dlr.  
[58] 
“introduction to ai platform &nbsp;|&nbsp; google cloud,” google. [online]. 
available: https://cloud.google.com/ai-platform/docs/technical-overview. 
[59] 
“mlops machine learning operations,” algorithmia. [online]. available: 
https://algorithmia.com/mlops. 
[60] 
“the state of kubernetes 2020,” vmware. [online]. available: 
https://k8s.vmware.com/state-of-kubernetes-2020/. 
 
 
 
 
 
 
 
 
[61] 
“kubeflow versioning policies,” kubeflow, 11-nov-2020. [online]. available: 
https://www.kubeflow.org/docs/reference/version-policy/. 
[62] 
kubeflow, “release v1.1.0 · kubeflow/kubeflow,” github. [online]. available: 
https://github.com/kubeflow/kubeflow/releases/tag/v1.1.0. 
[63] 
katib, kubeflow, “kubeflow/katib,” github. [online]. available: 
https://github.com/kubeflow/katib. 
[64] 
metadata, kubeflow, “kubeflow/metadata,” github. [online]. available: 
https://github.com/kubeflow/metadata. 
[65] 
“introduction to feast,” kubeflow, 23-oct-2020. [online]. available: 
https://www.kubeflow.org/docs/components/feature-store/overview/. 
[66] 
fairing, kubeflow, “kubeflow/fairing,” github. [online]. available: 
https://github.com/kubeflow/fairing. 
[67] 
examples, kubeflow, “kubeflow/examples,” github. [online]. available: 
https://github.com/kubeflow/examples. 
[68] 
xin, d., ma, l., song, s., and parameswaran, a., “how developers iterate on machine 
learning workflows -- a survey of the applied machine learning literature”, arxiv e-prints, 
2018. 
[69] 
“data version control · dvc,” dvc. [online]. available: https://dvc.org/. 
[70] 
kubeflow-kale, “kubeflow-kale/kale,” github. [online]. available: 
https://github.com/kubeflow-kale/kale. 
[71] 
paleyes, andrei & urma, raoul-gabriel & lawrence, neil. (2020). challenges in 
deploying machine learning: a survey of case studies. 
"
2826-Article Text-4116-2-10-20220219.pdf;"
references 
 
1. 
alla s, adari sk. what is mlops? in beginning 
mlops with mlflow. springer. 2021;79–124.  
2. 
renggli c, rimanic l, gurel nm, karla b, s, 
wu w, zhang c, zurich e. a data quality-
driven view of mlops. 2021;2. [online].  
available:https://arxiv.org/abs/ 2102.07750v1  
3. 
ruf p, madan m, reich c, ould-abdeslam d. 
demystifying mlops and presenting a recipe for 
the selection of open-source tools. applied 
sciences. 2021;11(9):8861. [online].  
available:https://www.mdpi.com/2076-
3417/11/ 
 
 
 
 
gujjar and kumar; ajoair, 13(3): 19-23, 2022 
 
 
 
23 
 
19/8861/htmhttps://www.mdpi.com/2076-
3417/11/19/8861  
4. 
klaise j, looveren av, cox c, vacanti g, 
coca a. monitoring and explainability of 
models in production. 2020;7. [online].  
available:https://arxiv.org/abs/2007.06299v1  
5. 
tamburri da. “sustainable mlops: trends and 
challenges,” 
proceedings 
- 
2020 
22nd 
international symposium on symbolic and 
numeric algorithms for scientific computing, 
synasc. 2020; 9:17–23.  
6. 
praveen 
gujjar, 
prasanna 
kumar 
hr. 
sentimental analysis for running text in email 
conversation. 
international 
journal 
of 
computer science and engineering (ijcse). 
2020;9(4):67-68.  
7. 
manjunatha t, praveen gujjar. performance 
analysis of indian information technology 
companies 
using 
dupont 
model. 
iup                
journal of management research. 2018; 
17(4):7-14.  
8. 
gift 
n, 
deza 
a. 
practical 
mlops: 
operationalizing machine learning models. 
o’reilly media, inc; 2020. 
9. 
treveil m, team d. introducing mlops how to 
scale machine learning in the enterprise. 
2020;185. [online].  
available:https://www.oreilly.com/library/ 
view/introducing-mlops/9781492083283/ 
10. 
adari sask. beginning mlops with mlflow 
deploy models in aws sagemaker, google 
cloud, and microsoft azure; 2021. [online].  
available:https://doi.org/10.1007/ 978-1-4842-
6549-9 
11. 
armbrust m, das t, sun l, yavuz b, zhu s, 
murthy m, et al. “delta lake,” proceedings of 
the vldb endowment. 2020;13(8):3411–
3424. [online].  
available:https://dl.acm.org/doi/abs/ 
10.14778/3415478.3415560 
12. 
mlflow. mlflow.  
available:https://mlflow.org/  
(accessed on 10 february 2022). 
__________________________________________________________________________________________ 
© copyright mb international media and publishing house. all rights reserved. 
 
"
3402942.3402953.pdf;"
references
[1] lea albaugh, april grow, chenxi liu, james mccann, gillian smith, and jennifer
mankoff. 2016. threadsteading: playful interaction for textile fabrication devices.
in 2016 chi extended abstracts. 285–288.
[2] avery alder. 2013. the quiet year. buriedwithoutceremony.com/the-quiet-year.
[3] botnik. 2017. predictive writer. botnik.org/apps/writer.
[4] kate compton and michael mateas. 2015. casual creators. in proc. international
conference on computational creativity. 228–235.
[5] mirjam palosaari eladhari. 2018. re-tellings: the fourth layer of narrative as an
instrument for critique. in proc. icids. springer, 65–78.
[6] mirjam p eladhari, anne sullivan, gillian smith, and josh mccoy. 2011. ai-based
game design: enabling new playable experiences. technical report. uc santa cruz
baskin school of engineering, santa cruz, ca.
[7] richard evans and emily short. 2013. versu—a simulationist storytelling system.
ieee tciaig 6, 2 (2013), 113–130.
[8] jacob garbe. 2018.
simulation of history and recursive narrative scaffold-
ing.
project.jacobgarbe.com/simulation-of-history-and-recursive-narrative-
scaffolding.
[9] max kreminski, devi acharya, nick junius, elisabeth oliver, kate compton,
melanie dickinson, cyril focht, stacey mason, stella mazeika, and noah wardrip-
fruin. 2019. cozy mystery construction kit: prototyping toward an ai-assisted
collaborative storytelling mystery game. in proc. fdg.
[10] max kreminski, melanie dickinson, michael mateas, and noah wardrip-fruin.
2020. why are we like this?: exploring writing mechanics for an ai-augmented
storytelling game. in proc. electronic literature organization.
[11] max kreminski, melanie dickinson, and noah wardrip-fruin. 2019. felt: a simple
story sifter. in proc. icids. springer, 267–281.
[12] max kreminski, ben samuel, edward melcer, and noah wardrip-fruin. 2019.
evaluating ai-based games through retellings. in proc. aiide, vol. 15. 45–51.
[13] max kreminski and noah wardrip-fruin. 2019. generative games as storytelling
partners. in proc. fdg.
[14] antonios liapis, georgios n yannakakis, constantine alexopoulos, and phil
lopes. 2016. can computers foster human users’ creativity? theory and praxis of
mixed-initiative co-creativity. digital culture & education 8, 2 (2016), 136–153.
[15] enrique manjavacas, folgert karsdorp, ben burtenshaw, and mike kestemont.
2017. synthetic literature: writing science fiction in a co-creative process. in proc.
computational creativity in natural language generation (cc-nlg). 29–37.
[16] chris martens and matthew a hammer. 2017. languages of play: towards
semantic foundations for game interfaces. in proc. fdg. 32–41.
[17] james r meehan. 1977. tale-spin, an interactive program that writes stories.
in proc. ijcai. 91–98.
[18] nikita prokopov. 2014. datascript. github.com/tonsky/datascript.
[19] aaron reed. 2018. archives of the sky. archivesofthesky.textories.com.
[20] ben robbins. 2011. microscope: a fractal role-playing game of epic histories.
lamemage.com/microscope.
[21] melissa roemmele and andrew s gordon. 2015. creative help: a story writing
assistant. in proc. icids. springer, 81–92.
[22] james ryan. 2018. curating simulated storyworlds. ph.d. dissertation. uc santa
cruz.
[23] james owen ryan, michael mateas, and noah wardrip-fruin. 2015. open design
challenges for interactive emergent narrative. in proc. icids. springer, 14–26.
[24] james owen ryan, adam summerville, michael mateas, and noah wardrip-fruin.
2015. toward characters who observe, tell, misremember, and lie. in proc. aiide.
[25] ben samuel, michael mateas, and noah wardrip-fruin. 2016. the design of
writing buddy: a mixed-initiative approach towards computational story collab-
oration. in proc. icids. springer, 388–396.
[26] ben samuel, james ryan, adam j summerville, michael mateas, and noah
wardrip-fruin. 2016. bad news: an experiment in computationally assisted
performance. in proc. icids. springer, 108–120.
[27] robin sloan. 2016. writing with the machine. robinsloan.com/notes/writing-
with-the-machine.
[28] ingibergur sindri stefnisson and david thue. 2018. mimisbrunnur: ai-assisted
authoring for interactive storytelling. in proc. aiide. 236–242.
[29] noah wardrip-fruin. 2009. expressive processing: digital fictions, computer
games, and software studies. mit press.
"
ajassp.2021.152.164.pdf;"
references 
allad, r. (2016). moving from a mobile first to an ai 
first world. https://unionstreetmedia.com/moving-
from-a-mobile-first-to-an-ai-first-world/ 
ameisen, e. (2020). building machine learning powered 
applications: going from idea to product. "" o'reilly 
media, inc."". 
bajpai, s. (2020). analyzing resume using natural 
language processing machine learning and django. 
international journal for research in applied science 
and engineering technology, 8(5), 2037-2039. 
https://doi.org/10.22214/ijraset.2020.5333 
bankar, s. (2018). cloud computing using amazon web 
services aws. international journal of trend in 
scientific research and development, 2156-2157. 
https://doi.org/10.31142/ijtsrd14583 
binge,  s. (2020). the importance of good software 
architecture. 
https://www.sitepen.com/blog/the-
importance-of-good-software-architecture 
chen, l. (2015). continuous delivery: huge benefits, but 
challenges too. ieee software, 32(2), 50-54. 
https://doi.org/10.1109/ms.2015.27 
chen, z., cao, y., liu, y., wang, h., xie, t., & liu, x. 
(2020, november). a comprehensive study on 
challenges in deploying deep learning based 
software. in proceedings of the 28th acm joint 
meeting 
on 
european 
software 
engineering 
conference and symposium on the foundations of 
software engineering (pp. 750-762). 
esmaeilzadeh, a. 2017. a test driven approach to 
develop 
web-based 
machine 
learning 
applications. 
unlv 
theses, 
dissertations, 
professional papers and capstones. 3127. 
kennedy ochilo hadullo and daniel makini getuno / american journal of applied sciences 2021, volume 18: 152.164 
doi: 10.3844/ajassp.2021.152.164 
 
163 
fitzgerald, b., & stol, k. j. (2017). continuous software 
engineering: a roadmap and agenda. journal of 
systems 
and 
software, 
123, 
176-189. 
https://doi.org/10.1016/j.jss.2015.06.063 
grayson, 
j. 
e. 
(2000). 
python 
and 
tkinter 
programming. 
manning 
publications 
co. 
greenwich. 
http://117.239.19.55:8080/jspui/handle/12345678
9/230 
geron, a. (2019). hands-on machine learning with scikit-
learn, keras and tensorflow: concepts, tools and 
techniques to build intelligent systems. o'reilly 
media. isbn-10:1492032611. 
hull, j. c. (2020). machine learning in business: an 
introduction to the world of data science. 
independently 
published. 
https://www.amazon.com/machine-learning-
business-introduction-science/dp/b088b8162s 
ieee cs. (2000). recommended practice for architectural 
description 
for 
software-intensive 
systems. 
https://doi.org/10.1109/ieeestd.2000.91944 
jaxenter. (2018). ml trends in stack overflow developer 
survey 2018. https://jaxenter.com/ml-trends-stack-
overflow-145870.html 
keshari, k. (2020, december 02). top 10 applications of 
machine learning: machine learning applications 
in 
daily 
life. 
retrieved 
from. 
https://www.edureka.co/blog/machine-learning-
applications 
jordon, w. (2019). python django web development: 
the ultimate django web framework guide for 
beginners. 
independently 
published. 
https://www.amazon.com/python-django-web-
development-framework/dp/1688542817 
kruchten, p., nord, r. l., ozkaya, i., & visser, j. (2012). 
technical debt in software development. acm 
sigsoft software engineering notes, 37(5), 36-38. 
https://doi.org/10.1145/2347696.2347698 
li, z., avgeriou, p., & liang, p. (2015). a systematic 
mapping study on technical debt and its management. 
journal of systems and software, 101, 193-220. 
https://doi.org/10.1016/j.jss.2014.12.027 
mcclendon, l., & meghanathan, n. (2015). using 
machine learning algorithms to analyze crime 
data. machine learning and applications: an 
international 
journal, 
2(1), 
1-12. 
https://doi.org/10.5121/mlaij.2015.2101 
mcgovern, j., ambler, s. w., stevens, m. e., linn, j., jo, 
e. k., & sharan, v. (2004). a practical guide to 
enterprise architecture. prentice hall professional. 
isbn-10: 0131412752. 
mikut, r., & reischl, m. (2011). data mining tools. 
wiley interdisciplinary reviews: data mining and 
knowledge 
discovery, 
1(5), 
431-443. 
https://doi.org/10.1002/widm.24 
miller, j. (2019). hands-on machine learning with ibm 
watson: leverage ibm watson to implement 
machine learning techniques and algorithms using 
python. 
packt 
publishing 
ltd. 
https://www.amazon.com/hands-machine-
learning-ibm-watson/dp/1789611857 
moroney, l. (2020). ai and machine learning for coders. 
o'reilly 
media, 
incorporated. 
https://www.oreilly.com/library/view/ai-and-
machine/9781492078180/ 
murad, d. f. (2020). systematic literature review (slr) 
approach. https://doi.org/10.31219/osf.io/v7239. 
o'leary, k., & uchida, m. (2020). common problems 
with creating machine learning pipelines from 
existing code. https://storage.googleapis.com/pub-
tools-public-publication-
data/pdf/b50bc83882bbd29c50250d1e59fbc3afda3f
b5e5.pdf 
oppegaard, s. m. n. (2021). regulating flexibility: 
uber’s 
platform 
as 
a 
technological 
work 
arrangement. nordic journal of working life 
studies. https://doi.org/10.18291/njwls.122197 
pathak, n. (2017). artificial intelligence for. net: 
speech, language and search: building smart 
applications with microsoft cognitive services 
apis. apress. isbn-10: 1484229495. 
plonski, p. (2019). december 31. deploy machine 
learning 
models 
with 
django. 
https://www.deploymachinelearning.com/ 
ranjeetsingh, s. s. (2014). microsoft windows azure: 
developing applications for highly available 
storage of cloud service. international journal of 
science and research (ijsr), 4(12), 662-665. 
https://doi.org/10.21275/v4i12.nov151864 
redapt marketing. (2019). why do ml projects fail? 
https://www.redapt.com/blog/why-90-of-machine-
learning-models-never-make-it-to-
production#:~:text=during%20a%20panel%20at%20l
ast,actually%20make%20it%20into%20production 
runyu, xu. (2020). a design pattern for deploying 
machine 
learning 
models 
to 
production. 
https://csusm-
dspace.calstate.edu/bitstream/handle/10211.3/21717
6/xurunyu_summer2020.pdf?sequence=1 
sanderson, d. (2012). programming google app engine. 
sebastopol, 
ca: 
o'reilly. 
https://www.oreilly.com/library/view/programming-
google-app/9781449314095/ 
sculley, d., holt, g., golovin, d., davydov, e., phillips, 
t., ebner, d., ... & dennison, d. (2015). hidden 
technical debt in machine learning systems. 
advances in neural information processing systems, 
28, 
2503-2511. 
http://papers.nips.cc/paper/5656-
hidden-technical-debt-in-machine-learning-systems 
kennedy ochilo hadullo and daniel makini getuno / american journal of applied sciences 2021, volume 18: 152.164 
doi: 10.3844/ajassp.2021.152.164 
 
164 
sharma, r. (2020). study of supervised learning and 
unsupervised learning. international journal for 
research in applied science and engineering 
technology, 
8(6), 
588-593. 
https://doi.org/10.22214/ijraset.2020.6095 
schröer, c., kruse, f., & gómez, j. m. (2021). a systematic 
literature review on applying crisp-dm process 
model. procedia computer science, 181, 526-534. 
https://doi.org/10.1016/j.procs.2021.01.199 
singh, p. (2021). deploy machine learning models to 
production: with flask, streamlit, docker and 
kubernetes on google cloud platform. apress. 
http://103.7.177.7/handle/123456789/207519 
stack overflow. (2020). we <3 people who code. 
https://stackoverflow.com/never 
make 
it 
into 
production? 
https://venturebeat.com/2019/07/19/why-do-87-of-
data-science-projects-never-make-it-into-
production/ 
wang, q. (2019). machine learning applications in 
operations management and digital marketing 
(doctoral dissertation, universiteit van amsterdam). 
https://abs.uva.nl/binaries/content/assets/subsites/am
sterdam-business-
school/research/dissertations/thesis-q.-wang---abs-
2019.pdf 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
washizaki, h., uchida, h., khomh, f., & guéhéneuc, y. g. 
(2019, december). studying software engineering 
patterns for designing machine learning systems. 
in 2019 10th international workshop on empirical 
software engineering in practice (iwesep) (pp. 
49-495). ieee. 
 
https://ieeexplore.ieee.org/abstract/document/8945075/ 
zazworka, n., shaw, m. a., shull, f., & seaman, c. 
(2011, may). investigating the impact of design debt 
on software quality. in proceedings of the 2nd 
workshop on managing technical debt (pp. 17-23). 
https://doi.org/10.1145/1985362.1985366 
zhang, d., & tsai, j. j. (eds.). (2005). machine learning 
applications in software engineering (vol. 16). world 
scientific. 
https://doi.org/10.1142/9789812569271_0001 
zhang, t., gao, c., ma, l., lyu, m., & kim, m. (2019, 
october). an empirical study of common challenges 
in developing deep learning applications. in 2019 
ieee 30th international symposium on software 
reliability engineering (issre) (pp. 104-115). 
ieee. https://doi.org/10.1109/issre.2019.00020 
"
applsci-11-08861-v2.pdf;"
references
1.
sculley, d.; holt, g.; golovin, d.; davydov, e.; phillips, t.; ebner, d.; chaudhary, v.; young, m.; crespo, j.f.; dennison, d. hidden
technical debt in machine learning systems. in advances in neural information processing systems; cortes, c., lawrence, n., lee,
d., sugiyama, m., garnett, r., eds.; curran associates, inc.: cambridge, ma, usa, 2015; volume 28.
2.
goyal, a. machine learning operations. in international journal of information technology insights & transformations [issn:
2581-5172 (online)]; eureka journals: pune, india 2020; volume 4.
3.
raj, e.; westerlund, m.; espinosa-leal, l. reliable fleet analytics for edge iot solutions. arxiv 2021, arxiv:2101.04414.
4.
rai, r.k. intricacies of unstructured data. eai endorsed trans. scalable inf. syst. 2017. [crossref]
5.
mohammadi, b.; fathy, m.; sabokrou, m. image/video deep anomaly detection: a survey. arxiv 2021, arxiv:2103.01739.
6.
shrivastava, s.; patel, d.; zhou, n.; iyengar, a.; bhamidipaty, a. dqlearn: a toolkit for structured data quality learning.
in proceedings of the 2020 ieee international conference on big data (big data), atlanta, ga, usa, 10–13 december 2020;
pp. 1644–1653.
7.
tamburri, d.a. sustainable mlops: trends and challenges. in proceedings of the 2020 22nd international symposium on
symbolic and numeric algorithms for scientiﬁc computing (synasc), timisoara, romania, 1–4 september 2020; pp. 17–23.
8.
fursin, g.; guillou, h.; essayan, n. codereef: an open platform for portable mlops, reusable automation actions and
reproducible benchmarking. arxiv 2020, arxiv:2001.07935.
9.
granlund, t.; kopponen, a.; stirbu, v.; myllyaho, l.; mikkonen, t. mlops challenges in multi-organization setup: experiences
from two real-world cases. arxiv 2021, arxiv:2103.08937.
10.
zhao, y. machine learning in production: a literature review. 2021. available online: https://staff.fnwi.uva.nl/a.s.z.belloum/
literaturestudies/reports/2021-literaturestudy-report-yizhen.pdf (accessed on 27 july 2021).
11.
muralidhar, n.; muthiah, s.; butler, p.; jain, m.; yu, y.; burne, k.; li, w.; jones, d.; arunachalam, p.; mccormick, h.s.; et al. using
antipatterns to avoid mlops mistakes. arxiv 2021, arxiv:2107.00079.
appl. sci. 2021, 11, 8861
37 of 39
12.
silva, l.c.; zagatti, f.r.; sette, b.s.; dos santos silva, l.n.; lucrédio, d.; silva, d.f.; de medeiros caseli, h. benchmarking
machine learning solutions in production. in proceedings of the 2020 19th ieee international conference on machine learning
and applications (icmla), miami, fl, usa, 14–17 december 2020; pp. 626–633.
13.
sureddy, m.r.; yallamula, p. a framework for monitoring data warehousing applications. int. res. j. eng. technol. 2020,
7, 7023–7029.
14.
shivakumar, s.k., web performance monitoring and infrastructure planning. in modern web performance optimization: methods,
tools, and patterns to speed up digital platforms; apress: berkeley, ca, usa, 2020; pp. 175–212. [crossref]
15.
sebastian-coleman, l. chapter 4—data quality and measurement. in measuring data quality for ongoing improvement; sebastian-
coleman, l., ed.; mk series on business intelligence; morgan kaufmann: boston, ma, usa, 2013; pp. 39–53. doi:10.1016/b978-0-
12-397033-6.00004-3. [crossref]
16.
schelter, s.; lange, d.; schmidt, p.; celikel, m.; biessmann, f.; grafberger, a. automating large-scale data quality veriﬁcation.
proc. vldb endow. 2018, 11, 1781–1794. [crossref]
17.
taleb, i.; serhani, m.a.; dssouli, r. big data quality: a survey. in proceedings of the 2018 ieee international congress on big
data (bigdata congress), san francisco, ca, usa, 2–7 july 2018; pp. 166–173.
18.
barrak, a.; eghan, e.e.; adams, b. on the co-evolution of ml pipelines and source code - empirical study of dvc projects. in
proceedings of the 2021 ieee international conference on software analysis, evolution and reengineering (saner), honolulu,
hi, usa, 9–12 march 2021; pp. 422–433. [crossref]
19.
ramasubramanian, k.; singh, a. machine learning model evaluation. in machine learning using r; apress: berkeley, ca, usa;
2017; pp. 425–464. [crossref]
20.
mehmood, h.; kostakos, p.; cortes, m.; anagnostopoulos, t.; pirttikangas, s.; gilman, e. concept drift adaptation techniques in
distributed environment for real-world data streams. smart cities 2021, 4, 349–371. [crossref]
21.
hutter, f.; kotthoff, l.; vanschoren, j. (eds.) automated machine learning: methods, systems, challenges; springer: berlin, germany,
2018; in press. available online: http://automl.org/book (accessed on 27 july 2021).
22.
zöller, m.a.; huber, m.f. survey on automated machine learning. arxiv 2019, arxiv:1904.12054.
23.
peng, g.; lacagnina, c.; downs, r.r.; ramapriyan, h.; ivánová, i.; ganske, a.; jones, d.; bastin, l.; wyborn, l.; bastrakova,
i.; et al. international community guidelines for sharing and reusing quality information of individual earth science datasets.
osf preprints, 16 april 2021. available online: https://osf.io/xsu4p (accessed on 27 august 2021).
24.
raj, e. engineering mlops: rapidly build, test, and manage production-ready machine learning life cycles at scale; packt publishing:
birmingham, uk, 2021.
25.
wang, d.; liao, q.v.; zhang, y.; khurana, u.; samulowitz, h.; park, s.; muller, m.; amini, l. how much automation does a data
scientist want? arxiv 2021, arxiv:2101.03970.
26.
aws mlops framework. available online: https://aws.amazon.com/solutions/implementations/aws-mlops-framework/
(accessed on 14 september 2021).
27.
sharma, s. the devops adoption playbook: a guide to adopting devops in a multi-speed it enterprise; john wiley & sons: hoboken,
nj, usa, 2017.
28.
karamitsos, i.; albarhami, s.; apostolopoulos, c. applying devops practices of continuous automation for machine learning.
information 2020, 11, 363. [crossref]
29.
mäkinen, s.; skogström, h.; laaksonen, e.; mikkonen, t. who needs mlops: what data scientists seek to accomplish and
how can mlops help? arxiv 2021, arxiv:2103.08942.
30.
treveil, m.; omont, n.; stenac, c.; lefevre, k.; phan, d.; zentici, j.; lavoillotte, a.; miyazaki, m.; heidmann, l. introducing
mlops; o’reilly media: sebastopol, ca, usa, 2020.
31.
lu, j.; liu, a.; dong, f.; gu, f.; gama, j.; zhang, g. learning under concept drift: a review. ieee trans. knowl. data eng. 2018,
31, 2346–2363. [crossref]
32.
baylor, d.; haas, k.; katsiapis, k.; leong, s.; liu, r.; menwald, c.; miao, h.; polyzotis, n.; trott, m.; zinkevich, m. continuous
training for production ml in the tensorflow extended (tfx) platform. in proceedings of the 2019 usenix conference on
operational machine learning (opml 19), santa clara, ca, usa, 20 may 2019; pp. 51–53.
33.
google. mlops: continuous delivery and automation pipelines in machine learning. available online: https://cloud.google.
com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning/ (accessed on 3 may 2021).
34.
maydanchik, a. data quality assessment; technics publications: basking ridge, nj, usa, 2007.
35.
verheul, i.; imming, m.; ringerma, j.; mordant, a.; ploeg, j.l.v.d.; pronk, m. data stewardship on the map: a study of tasks and
roles in dutch research institutes. 2019. available online: https://zenodo.org/record/2669150#.yuw2bh0rvpy (accessed on
27 august 2021).
36.
wende, k. a model for data governance–organising accountabilities for data quality management. in proceedings of the data
stewardship on the map: a study of tasks and roles in dutch research institutes, toowoomba, australia, 5–7 december 2007.
37.
pergl, r.; hooft, r.; suchánek, m.; knaisl, v.; slifka, j. “data stewardship wizard”: a tool bringing together researchers, data
stewards, and data experts around data management planning. data sci. j. 2019, 18, 59. [crossref]
38.
peng, g.; ritchey, n.a.; casey, k.s.; kearns, e.j.; privette, j.a.; saunders, d.; jones, p.; maycock, t.; ansari, s. scientiﬁc
stewardship in the open data and big data era-roles and responsibilities of stewards and other major product stakeholders.
2016. available online: https://www.dlib.org/dlib/may16/peng/05peng.html (accessed on 27 august 2021).
appl. sci. 2021, 11, 8861
38 of 39
39.
mons, b. data stewardship for open science: implementing fair principles; crc press: boca raton, fl, usa, 2018.
40.
mons, b.; neylon, c.; velterop, j.; dumontier, m.; da silva santos, l.o.b.; wilkinson, m.d. cloudy, increasingly fair; revisiting
the fair data guiding principles for the european open science cloud. inf. serv. use 2017, 37, 49–56. [crossref]
41.
zubair, n.; hebbar, k.; simmhan, y. characterizing iot data and its quality for use. arxiv 2019, arxiv:1906.10497.
42.
gudivada, v.; apon, a.; ding, j. data quality considerations for big data and machine learning: going beyond data cleaning and
transformations. int. j. adv. softw. 2017, 10, 1–20.
43.
dong, x.l.; rekatsinas, t. data integration and machine learning: a natural synergy. in proceedings of the 2018 international
conference on management of data, houston, tx, usa, 10–15 june 2018; pp. 1645–1650.
44.
kißkalt, d.; mayr, a.; lutz, b.; rögele, a.; franke, j. streamlining the development of data-driven industrial applications by
automated machine learning. procedia cirp 2020, 93, 401–406. [crossref]
45.
lee, y.; scolari, a.; chun, b.g.; weimer, m.; interlandi, m. from the edge to the cloud: model serving in ml. net. ieee data
eng. bull. 2018, 41, 46–53.
46.
wang, z.; bovik, a.c.; sheikh, h.r.; simoncelli, e.p. image quality assessment: from error visibility to structural similarity. ieee
trans. image process. 2004, 13, 600–612. [crossref] [pubmed]
47.
li, c.; bovik, a.c. content-partitioned structural similarity index for image quality assessment. signal process. image commun.
2010, 25, 517–526. [crossref]
48.
wang, z.; li, q. information content weighting for perceptual image quality assessment. ieee trans. image process. 2010,
20, 1185–1198. [crossref] [pubmed]
49.
li, c.; bovik, a.c. three-component weighted structural similarity index. in proceedings of the image quality and system
performance vi. international society for optics and photonics, 19–21 january 2009, san jose, ca, usa; volume 7242, p. 72420q.
50.
de freitas zampolo, r.; seara, r. a comparison of image quality metric performances under practical conditions. in proceedings
of the ieee international conference on image processing 2005, genoa, italy, 11–14 september 2005; volume 3, pp. 1192–1195.
[crossref]
51.
sheikh, h.r.; bovik, a.c. image information and visual quality. ieee trans. image process. 2006, 15, 430–444. [crossref] [pubmed]
52.
zhang, l.; zhang, l.; mou, x.; zhang, d. fsim: a feature similarity index for image quality assessment. ieee trans. image
process. 2011, 20, 2378–2386. [crossref] [pubmed]
53.
xue, w.; zhang, l.; mou, x.; bovik, a.c. gradient magnitude similarity deviation: a highly efﬁcient perceptual image quality
index. ieee trans. image process. 2013, 23, 684–695. [crossref]
54.
egiazarian, k.; astola, j.; ponomarenko, n.; lukin, v.; battisti, f.; carli, m. new full-reference quality metrics based on hvs. in
proceedings of the second international workshop on video processing and quality metrics, scottsdale, az, usa, 22–24 january
2006; volume 4.
55.
larson, e.c.; chandler, d.m. most apparent distortion: full-reference image quality assessment and the role of strategy. j. electron.
imaging 2010, 19, 011006.
56.
lee, d.; plataniotis, k.n. towards a full-reference quality assessment for color images using directional statistics. ieee trans.
image process. 2015, 24, 3950–3965.
57.
zhang, l.; shen, y.; li, h. vsi: a visual saliency-induced index for perceptual image quality assessment. ieee trans. image
process. 2014, 23, 4270–4281. [crossref]
58.
mattson, p.; cheng, c.; coleman, c.; diamos, g.; micikevicius, p.; patterson, d.; tang, h.; wei, g.y.; bailis, p.; bittorf, v.; et al.
mlperf training benchmark. arxiv 2019, arxiv:1910.01500.
59.
mlﬂow. mlﬂow. available online: https://mlﬂow.org/ (accessed on 27 july 2021).
60.
polyaxon—machine learning at scale. available online: https://polyaxon.com/ (accessed on 27 july 2021).
61.
kedro: a python framework for creating reproducible, maintainable and modular data science code. available online:
https://github.com/quantumblacklabs/kedro (accessed on 27 july 2021).
62.
baylor, d.; breck, e.; cheng, h.t.; fiedel, n.; foo, c.y.; haque, z.; haykal, s.; ispir, m.; jain, v.; koc, l.; et al tfx: a tensorﬂow-
based production-scale machine learning platform. in proceedings of the 23rd acm sigkdd international conference on
knowledge discovery and data mining, halifax, ns, canada, 13–17 august 2017; pp. 1387–1395.
63.
zenml. available online: https://zenml.io/ (accessed on 27 july 2021).
64.
h2o: tfully open source, distributed in-memory machine learning platform. available online: https://www.h2o.ai/products/
h2o/ (accessed on 2 september 2021).
65.
kubeﬂow: the machine learning toolkit for kubernetes. available online: https://www.kubeﬂow.org/ (accessed on 27 july 2021).
66.
flyte: the workﬂow automation platform for complex, mission-critical data and ml processes at scale. available online:
https://ﬂyte.org/ (accessed on 27 july 2021).
67.
apache airﬂow, a platform created by the community to programmatically author, schedule and monitor workﬂows. available
online: https://airﬂow.apache.org/ (accessed on 27 july 2021).
68.
dvc: open-source version control system for machine learning projects. available online: https://dvc.org/ (accessed on
27 july 2021).
69.
the data foundation for machine learning. available online: https://www.pachyderm.com/ (accessed on 27 july 2021).
70.
quilt. available online: https://quiltdata.com/ (accessed on 27 july 2021).
71.
great expectations. available online: https://greatexpectations.io/ (accessed on 27 july 2021).
appl. sci. 2021, 11, 8861
39 of 39
72.
git large file storage (lfs). available online: https://git-lfs.github.com/ (accessed on 27 july 2021).
73.
continuous machine learning (cml). available online: https://cml.dev/ (accessed on 27 july 2021).
74.
github actions. available online: https://github.com/features/actions (accessed on 27 july 2021).
75.
circleci. available online: https://circleci.com/ (accessed on 27 july 2021).
76.
gocd. available online: https://www.gocd.org/ (accessed on 27 july 2021).
77.
cortex. available online: https://www.cortex.dev/ (accessed on 27 july 2021).
78.
seldon core. available online: https://github.com/seldonio/seldon-core (accessed on 27 july 2021).
79.
bentoml. available online: https://github.com/bentoml/bentoml (accessed on 27 july 2021).
80.
prometheus—monitoring system and time series database. available online: https://prometheus.io/ (accessed on 27 july 2021).
81.
kibana. available online: https://www.elastic.co/kibana/ (accessed on 27 july 2021).
82.
grafana: the open observability platform. available online: https://grafana.com (accessed on 27 july 2021).
83.
lable studio. available online: https://labelstud.io/ (accessed on 27 july 2021).
84.
make sense. available online: https://www.makesense.ai/ (accessed on 27 july 2021).
85.
tao, x.; zhang, d.; ma, w.; liu, x.; xu, d. automatic metallic surface defect detection and recognition with convolutional neural
networks. appl. sci. 2018, 8, 1575. [crossref]
86.
ruff, l.; kauffmann, j.r.; vandermeulen, r.a.; montavon, g.; samek, w.; kloft, m.; dietterich, t.g.; müller, k.r. a unifying
review of deep and shallow anomaly detection. in proceedings of the ieee, xiamen, china, 28–30 july 2021. [crossref]
87.
ultralytics. yolov5. available online: https://github.com/ultralytics/yolov5 (accessed on 22 july 2021).
88.
bergstra, j.; yamins, d.; cox, d. making a science of model search: hyperparameter optimization in hundreds of dimensions for
vision architectures. in proceedings of the international conference on machine learning, atlanta, ga, usa, 16–21 june 2013;
pp. 115–123.
89.
ahmed, m.; hashmi, k.a.; pagani, a.; liwicki, m.; stricker, d.; afzal, m.z. survey and performance analysis of deep learning
based object detection in challenging environments. sensors 2021, 21, 5116. [crossref]
"
A_canopy_approach_to_nitrogen_fertilizer20160131-3248-ehh8b2-with-cover-page-v2.pdf;"
references
akaike, h., 1973. information theory and an extension of the maximum likeli-
hood principle. in: petov, b.n., csaki, f. (eds.), second international sym-
posium on information theory. akademia kiado, budapest, pp. 267–281.
allison, m.f., armstrong, m.j., jaggard, k.w., todd, a.d., milford, g.f.j.,
1996. an analysis of the agronomic, economic and environmental effects of
applying n fertilizer to sugar beet (beta vulgaris). j. agric. sci., camb. 127,
475–486.
andrieu, b., allirand, j.m., jaggard, k.w., 1997. ground cover and leaf area
index of maize and sugar beet crops. agronomie 17, 315–321.
armstrong, m.j., milford, g.f.j., pocock, t.o., last, p.j., day, w., 1986. the
dynamics of nitrogen uptake and its remobilization during the growth of
sugar beet. j. agric. sci., camb. 107, 145–154.
boyd, d.a., tinker, p.b.h., draycott, a.p., last, p.j., 1970. nitrogen require-
ment of sugar beet grown on mineral soils. j. agric. sci., camb. 74, 37–40.
brown, k.f., biscoe, p.v., 1985. fibrous root growth and water use of sugar
beet. j. agric. sci., camb. 105, 79–691.
brown, k.f., messem, a.b., dunham, r.j., biscoe, p.v., 1987. effect of drought
on growth and water use of sugar beet. j. agric. sci. camb. 109, 421–435.
critchley, c.s., 2001. a physiological explanation for the canopy nitrogen
requirement of winter wheat. ph.d. thesis. university of nottingham.
foulkes, m.j., sylvester-bradley, r., scott, r.k., 1998. evidence of differences
between winter wheat cultivars in acquisition of soil mineral nitrogen and
uptake and utilization of applied fertilizer nitrogen. j. agric. sci., camb.
130, 29–44.
gillett, a.g., crout, n.m.j., stokes, d.t., sylvester-bradley, r., scott, r.k.,
1999. simple winter wheat green area index model under uk conditions. j.
agric. sci., camb. 132, 263–271.
gummerson, r.j., 1986. the effect of constant temperatures and osmotic poten-
tials on the germination of sugar-beet. j. exp. bot. 37, 729–741.
hodge, c.a.h., burton, r.g.o., corbett, w.m., evans, r., seale, r.s., 1984.
soils and their use in eastern england. soil survey of england and wales,
harpenden, uk, 450 pp.
icumsa, 1979. in: schneider, f. (ed.), sugar analysis. international commis-
sion for uniform methods of sugar analysis, peterborough, uk.
jaggard, k.w., limb, m., proctor, g., 1995. sugar beet: a grower’s guide. sugar
beet research and education committee, london, 111 pp.
last, p.j., draycott, a.p., messem, a.b., webb, d.j., 1983. effects of nitrogen
fertilizer and irrigation on sugar beet at broom’s barn, 1973–1978. j. agric.
sci., camb. 101, 185–205.
lord, e.i., vaughan, j., 1987. optimising nitrogen applications for the produc-
tion of malting barley. aspects appl. biol. 15, 319–335.
malnou, c.s., 2003. a canopy approach to nitrogen recommendation for the
sugar beet crop. ph.d. thesis. university of nottingham.
m.a.f.f., 1986. the analysis of agricultural soils: reference book 427.
hmso, london.
m.a.f.f., 2000. fertiliser recommendations for agricultural and horticultural
crops. reference book 209, seventh ed. hmso, london.
milford, g.f.j., pocock, t.o., riley, j., 1985a. an analysis of leaf growth in
sugar beet. i. leaf appearance and expansion in relation to temperature under
controlled conditions. ann. appl. biol. 106, 163–172.
milford, g.f.j., pocock, t.o., riley, j., 1985b. an analysis of leaf growth in
sugar beet. ii. leaf appearance in ﬁeld crops. ann. appl. biol. 106, 173–
185.
milford, g.f.j., pocock, t.o., jaggard, k.w., biscoe, p.v., armstrong, m.j.,
last, p.j., goodman, p.j., 1985c. an analysis of leaf growth in sugar beet.
iii. the expansion of the leaf canopy in relation to temperature and nitrogen.
ann. appl. biol. 107, 335–347.
neeteson, j.j., zwetsloot, h.j.c., 1989. an analysis of the response of the
response of sugar beet and potatoes to fertilizer nitrogen and soil mineral
nitrogen. neth. j. agric. sci. 37, 129–141.
pocock, t.o., milford, g.f.j., armstrong, m.j., 1990. storage root quality in
sugar-beet in relation to nitrogen uptake. j. agric. sci., camb. 115, 355–362.
russell, r.s., 1977. plant root systems: their function and interaction with
the soil. mcgraw-hill book company ltd., uk, pp. 62–89.
scott, r.k., jaggard, k.w., 1993. crop physiology and agronomy. in: cooke,
d.a., scott, r.k. (eds.), the sugar beet crop. chapman & hall, london,
pp. 179–237.
steven, m.d., biscoe, p.v., jaggard, k.w., 1983. estimation of sugar beet pro-
ductivity from reﬂectance in the red and near-infrared spectral bands. int. j.
remote sens. 4, 325–334.
steven, m.d., biscoe, p.v., jaggard, k.w., parutu, j., 1986. foliage cover and
radiation intercepted. field crop res. 13, 75–87.
sylvester-bradley, r., scott, r.k., stokes, d.t., clare, r.w., 1997. the sig-
niﬁcance of crop canopies for n nutrition. aspects appl. biol. 50, 103–
116.
sylvester-bradley, r., dampney, p.m.r., murray, a.w.a., 1984. the response
of winter wheat to nitrogen. in: needham, p., archer, j.r., sylvester-bradley,
r., goodlass, g. (eds.), the nitrogen requirement of cereals. hmso, lon-
don, pp. 151–176.
touraine,b.,daniel-vedele,f.,forde,b.,2001.nitrateuptakeanditsregulation.
in: lea, p.j., morot-gaudry, j.f. (eds.), plant nitrogen, inra editions.
springer, paris, pp. 1–36.
webb, j., seeney, f.m., sylvester-bradley, r., 1998. the response to fertilizer
nitrogen of cereals grown on sandy soils. j. agric. sci., camb. 130, 271–286.
werker, a.r., jaggard, k.w., 1997. modelling asymmetrical growth curves that
rise and then fall: applications to foliage dynamics of sugar beet (beta vul-
garis l.). ann. bot. 79, 657–665.
"
Best Practice_ 5 Schritte zur Minimum Viable Enterprise Architecture - cio.de.pdf;
C1-4.pdf;"
references
[be19]
ben zid, i.; parekh, m.; waedt, k.; lou, x.ȷ the application of articial
intelligence for cyber security in industry ».0. in (draude, c.; lange, m.;
sick, b., eds.)ȷ informatik 2019ȷ 50 jahre gesellschaft für informatik –
informatik für gesellschaft (workshop-beitrčge). gesellschaft für informatik
e.v., bonn, pp. 255–260, 2019.
[ch19]
chircu, a.; sultanow, e.; baum, d.; koch, c.; seßler, m.ȷ visualization and
machine learning for data center management. in (draude, c.; lange, m.;
sick, b., eds.)ȷ informatik 2019ȷ 50 jahre gesellschaft für informatik –
informatik für gesellschaft (workshop-beitrčge). gesellschaft für informatik
e.v., bonn, pp. 2«–«5, 2019.
[ch20]
chircu, a.; sultanow, e.; hain, t.; merscheid, t.; özcan, o.ȷ real-time «d
visualization of queues with embedded ml-based prediction of item pro-
cessing for a product information management system. in (zimmermann, a.;
howlett, r. j.; jain, l. c., eds.)ȷ human centred intelligent systems. springer
singapore, singapore, pp. «»7–«58, 2020.
[ha18]
hazelwood, k.; bird, s.; brooks, d.; chintala, s.; diril, u.; dzhulgakov, d.;
fawzy, m.; jia, b.; jia, y.; kalro, a.; law, j.; lee, k.; lu, j.; noordhuis, p.;
smelyanskiy, m.; xiong, l.; wang, x.ȷ applied machine learning at facebookȷ
a datacenter infrastructure perspective. inȷ 2018 ieee international sym-
posium on high performance computer architecture (hpca). pp. 620–629,
2018.
[hd17]
hermann, j.; del balso, m.ȷ meet michelangeloȷ uber’s machine learning
platform, sept. 5, 2017, urlȷ https://eng.uber.com/michelangelo-
machine-learning-platform/.
[sc15]
sculley, d.; holt, g.; golovin, d.; davydov, e.; phillips, t.; ebner, d.;
chaudhary, v.; young, m.; crespo, j.-f.; dennison, d.ȷ hidden technical debt
in machine learning systems. in (cortes, c.; lawrence, n. d.; lee, d. d.;
sugiyama, m.; garnett, r., eds.)ȷ advances in neural information processing
systems 28. curran associates, inc., pp. 250«–2511, 2015, urlȷ http :
//papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-
learning-systems.pdf.
180 yannick martel et al.
[sww19]
sato, d.; wider, a.; windheuser, c.ȷ continuous delivery for machine learn-
ing, martinfowler.com, sept. 19, 2019, urlȷ https://martinfowler.com/
articles/cd4ml.html.
[wh19]
white, a.ȷ our top data and analytics predicts for 2019, gartner blog
network, jan. «, 2019, urlȷ https://blogs.gartner.com/andrew_white/
2019/01/03/our-top-data-and-analytics-predicts-for-2019.
software architecture best practices for enterprise ai 181
"
cidr2021_paper26.pdf;"
references
[1] sandro ackermann et al. using transfer learning to detect
galaxy mergers. mnras, 2018.
[2] johannes beck et al. sensing social media signals for
cryptocurrency news. in companion proceedings of www
2019, 2019.
[3] eric breck et al. data validation for machine learning. in
sysml, 2019.
[4] ivan girardi et al. patient risk assessment and warning
symptom detection using deep attention-based neural
networks. in the ninth international workshop on health
text mining and information analysis, 2018.
[5] nina glaser et al. radiogan–translations between different
radio surveys with generative adversarial networks. mnras,
2019.
[6] ruoxi jia et al. efﬁcient task-speciﬁc data valuation for
nearest neighbor algorithms. pvldb, 2019.
[7] ruoxi jia et al. an empirical and comparative analysis of
data valuation with scalable algorithms. arxiv, 2019.
[8] ruoxi jia et al. towards efﬁcient data valuation based on the
shapley value. aistats, 2019.
[9] mohammad reza karimi, nezihe merve gürel, bojan
karlaš, johannes rausch, ce zhang, and andreas krause.
online active model selection for pre-trained classiﬁers,
2020.
[10] bojan karlas et al. building continuous integration services
for machine learning. in kdd, 2020.
[11] bojan karlaš et al. nearest neighbor classiﬁers over
incomplete information: from certain answers to certain
predictions. arxiv, 2020.
[12] bojan karlaš et al. ease.ml in action: towards multi-tenant
declarative learning services. vldb demo, 2018.
[13] tim kraska. northstar: an interactive data science system.
pvldb, 2018.
[14] sanjay krishnan et al. activeclean: interactive data cleaning
for statistical modeling. pvldb, 2016.
[15] tian li et al. ease.ml: towards multi-tenant resource sharing
for machine learning workloads. in pvldb, 2018.
[16] akshay naresh modi et al. tfx: a tensorﬂow-based
production-scale machine learning platform. in kdd 2017,
2017.
[17] supun nakandala et al. incremental and approximate
inference for faster occlusion-based deep cnn explanations.
in sigmod, 2019.
[18] supun nakandala et al. cerebro: a data system for
optimized deep learning model selection. pvldb, 2020.
[19] alexander ratner et al. snorkel: rapid training data creation
with weak supervision. pvldb, 2017.
[20] johannes rausch et al. docparser: hierarchical structure
parsing of document renderings. aaai, 2021.
[21] theodoros rekatsinas et al. holoclean: holistic data repairs
with probabilistic inference. pvldb, 2017.
[22] cedric renggli et al. continuous integration of machine
learning models with ease.ml/ci: towards a rigorous yet
practical treatment. in sysml, 2019.
[23] cedric renggli et al. ease.ml/ci and ease.ml/meter in action:
towards data management for statistical generalization. in
vldb demo, 2019.
[24] cedric renggli et al. ease.ml/snoopy in action: towards
automatic feasibility analysis for machine learning
application development. in vldb demo, 2020.
[25] giuseppe russo et al. control, generate, augment: a scalable
framework for multi-attribute text generation. arxiv, 2020.
[26] lia f sartori et al. a model for agn variability on multiple
time-scales. mnras: letters, 2018.
[27] lia f sartori et al. a forward modeling approach to agn
variability–method description and early applications. the
astrophysical journal, 2019.
[28] kevin schawinski et al. generative adversarial networks
recover features in astrophysical images of galaxies beyond
the deconvolution limit. mnras letters, 2017.
[29] kevin schawinski et al. exploring galaxy evolution with
generative models. astronomy & astrophysics, 2018.
[30] dominic stark et al. psfgan: a generative adversarial network
system for separating quasar point sources and host galaxy
light. mnras, 2018.
[31] min su et al. generative adversarial networks as a tool to
recover structural information from cryo-electron
microscopy data. biorxiv, 2018.
[32] dan suciu, dan olteanu, christopher ré, and christoph
koch. probabilistic databases. synthesis lectures on data
management. 2011.
[33] manasi vartak et al. modeldb: a system for machine
learning model management. in hilda, 2016.
[34] renzhi wu et al. zeroer: entity resolution using zero labeled
examples. in sigmod, 2020.
[35] weiyuan wu et al. complaint-driven training data debugging
for query 2.0. in sigmod, 2020.
[36] chen yu et al. automl from service provider’s perspective:
multi-device, multi-tenant model selection with gp-ei.
aistats, 2019.
[37] m. zaharia et al. accelerating the machine learning lifecycle
with mlﬂow. ieee data eng. bull., 2018.
[38] ce zhang et al. deepdive: declarative knowledge base
construction. commun. acm, 2017.
"
CSE 21-135 Nazir.pdf;"
references 
 
[1]  s. amershi, a. begel, c. bird, r. deline, h. c. gall, e. kamar, n. nagappan, b. 
nushi and t. zimmermann, “software engineering for machine learning: a case 
study,” proceedings of the 41st international conference on software engineering: 
software engineering in practice, icse (seip) 2019, montreal, qc, canada,, may 
25-31, 2019.  
[2]  h. washizaki, h. uchida, f. khomh and y. gueheneuc, “studying software 
engineering patterns for designing machine learning systems,” 2019 10th 
international workshop on empirical software engineering in practice (iwesep), 
no. 30 december 2019, 13-14 dec. 2019.  
[3]  a. apostolos, c. sofia and s. loannis, “research state of the art on gof design 
patterns: a mapping study,” journal of systems and software, p. 1945–1964, july, 
2013.  
[4]  j. growin, d. weynys and t. holvoet, “design patterns for multi-agent systems: a 
systematic literature review,” agent-oriented software engineering: reflections 
on architecture's, methodologie's, language's, and framework's, pp. 79-99, 02 
2014.  
[5]  a. hany, w. abedelmoez and s. hamdi, “software engineering using artificial 
intelligence techniques: current state and open problems,” march, 2012.  
[6]  s. shirwaikar, “an exploratory study of the security design pattern landscape and 
their classification,” international journal of secure software engineering, vol. 7, 
pp. 26-43, 07 2016.  
[7]  y. watanabe, h. washizaki, k. sakamoto, d. saito, k. honda, n. tsuda, y. 
fukazawa and n. yoshioka, “preliminary systematic literature review of machine 
learning system development process,” 10 2019.  
[8]  b. yazdi, m. bafandeh, a. rasoolzadegan and a. ghavidel, “the state of the art on 
design patterns: a systematic mapping of the literature,” journal of systems and 
software, vol. 125, pp. 93-118, 2017.  
[9]  h. liu, s. eksmo, j. risberg and r. hebig, “emerging and changing tasks in the 
development process for machine learning systems,” vol. 125–134, 2020.  
[10]  t. al-naeem, f. t. dabous, f. a. rabhi and b. benatallah, “formulating the 
architectural design of enterprise applications as a search problem,” proceedings 
of the 2005 australian conference on software engineering, p. 282–291, 2005.  
[11]  e. m. saleh, o. sallabi and h. a. darbi, “a multi-agent system to support design 
pattern recommendation,” 2020.  
[12]  s. schelter, b. f, t. januschowski, d. salinas and s. s. g. seufert, “on challenges 
in machine learning model management,” bulletin of the ieee computer society 
technical committee on data engineering, pp. 1-11, 2018.  
 
64 
 
[13]  p. zdun and u. avgeriou, “architectural patterns revisited - a pattern language,” 
europlop’ 2005, tenth european conference on pattern languages of programs, 
irsee, germany, pp. 431-470, july 6-10, 2005,.  
[14]  h. washizaki, h. takeuchi, f. khomh, n. natori, t. doi and s. okuda, 
“practitioners’ insights on machine-learning software engineering design patterns: a 
preliminary study},” 2020 ieee international conference on software maintenance 
and evolution (icsme), pp. 797-799, 2020.  
[15]  h. washizaki, n. yoshioka, a. hazeyama, t. kato, h. kaiya, s. ogata, t. okubo 
and e. fernandez, “landscape of iot patterns, proceedings of the 1st international 
workshop on software engineering research &amp; practices for the internet of 
things,” ieee press, 2019.  
[16]  j. wang, g. li and y. pu, “a scenario-based architecture for reliability design of 
artificial intelligent software,” pp. 6-9, 2010.  
[17]  r. mayer and h.-a. jacobsen, “scalable deep learning on distributed 
infrastructures: challenges, techniques, and tools,” acm computing surveys 
(csur), vol. 53, pp. 1-37, 02 2020.  
[18]  z. a. x. x. a. l. d. a. m. g. c. wan, “how does machine learning change 
software development practices?,” ieee transactions on software engineering, 
pp. 1-1, 2019.  
[19]  h. muccini and k. vaidhyanathan, “software architecture for ml-based systems: 
what exists and what lies ahead,” 03 2021.  
[20]  h. yokoyama, “machine learning system architectural pattern for improving 
operational stability,” 2019 ieee international conference on software 
architecture companion (icsa-c), pp. 267-274, 2019.  
[21]  b. kitchenham and s. charters, “guidelines for performing systematic literature 
reviews in software engineering,” vol. 2, 01 2007.  
[22]  p. regnell, h. martin and a. rainer, “case study research in software engineering 
– guidelines and examples,” 2012.  
[23]  d. amaratunga, d. baldry, m. sarshar and r. newton, “quantitative and qualitative 
research in the built environment: application of 'mixed' research approach,” vol. 
51, pp. 17-31, 02 2002.  
[24]  c. okoli, “a guide to conducting a standalones systematics literatures reviews,” 
communications of the association for information systems, vol. 37, 2015.  
[25]  o. ulrika, l. kidd, y. wengström and n. rowa-dewar, “combining qualitative and 
quantitative research within mixed method research designs: a methodological 
review,” international journal of nursing studies, 11 2010.  
[26]  t. meline, “selecting studies for systemic review: inclusion and exclusion 
criteria,” contemporary issues in communication science and disorders, vol. 33, 
pp. 21-27, 03 2006.  
[27]  c. wohlin, “guidelines for snowballing in systematic literature studies and a 
replication in software engineering,” in proceedings of the 18th international 
conference on evaluation and assessment in software engineering, pp. 1-10, 2014.  
 
65 
 
[28]  j. webster and r. walton t., “analyzing the past to prepare for the future: writing a 
literature review.,” mis quarterly, pp. xiii-xxiii, 2002.  
[29]  t. a. c. m. a. f. b. a. j. i. punter, “conducting on-line surveys in software 
engineering,” 2003 international symposium on empirical software engineering, 
2003. isese 2003. proceedings., pp. 80-88, 2003.  
[30]  k. dillenburger, “survey research methods, floyd j. fowler jnr, london, 
thousand oaks, ca, sage publications, 3rd edn, 2002, pp. ix + 179, cloth isbn 0 
7619 2190 7, pound38.00, paper isbn 0 7619 2191 5, pound14.99,” british journal 
of social work - brit j soc work, vol. 32, pp. 390-391, 2002.  
[31]  f. shull, j. singer and d. i. sjøberg, “guide to advanced empirical software,” 
2007.  
[32]  g. hackett, “survey research methods,” personnel guidance journal, vol. 59, p. 9, 
1981.  
[33]  c. wohlin, r. per, h. matin, m. c. o, r. björn and w. anders, “empirical 
strategies."" in experimentation in software engineering,” no. springer, berlin, 
heidelberg, pp. 9-36, 2012.  
[34]  x. zhou, y. jin, h. zhang, s. li and x. huang, “a map of threats to validity of 
systematic literature reviews in software engineering,” proceedings of the 23rd asia-
pacific software engineering conference, pp. 153-160, dec, 2016.  
[35]  m. kassab, j. defranco and p. laplante, “software testing: the state of the 
practice,” ieee software, pp. 46-52, 2017.  
[36]  a. nguyen-duc, i. sundb, n. elizamary, c. tayana, a. iftekhar and a. pekka, “a 
multiple case study of artificial intelligent system development in industry,” 
ease '20: evaluation and assessment in software engineering trondheim norway, 
p. 1–10, 04 2020.  
[37]  j. mcgovern, s. w. ambler, m. stevens, j. linn, e. k. jo and v. sharan, “the 
practical guide to enterprise architecture,” 2003.  
 
 
 
 
 
 
 
66 
 
 
 
 
 
 
appendix a 
 
primary studies for slr: 
s1)  j. wang, g. li and y. pu, ""a scenario-based architecture for reliability design of 
artificial intelligent software,"" international conference on computational intelligence 
and security, pp. 6-9, 2010.  
 
s2)  a. serban, k. van der blom, h. hoos and j. visser, ""adoption and effects of soft-
ware engineering best practices in machine learning,"" in: proceedings of the 14th inter-
national symposium on empirical software engineering and measurement, pp. 1-12, 
2020.  
 
s3)  j. schleier-smith, ""an architecture for agile machine learning in real-time ap-
plications,"" 15: proceedings of the 21th acm sigkdd international conference on 
knowledge discovery and data mining, p. 2059–2068, 2015.  
 
s4)  z. wan, x. xia, d. lo and g. c. murphy, ""how does machine learning change 
software development practices?,"" ieee transactions on software engineering, pp. 1-1, 
2019.  
 
s5) j. musil, a. musil and s. biffl, ""introduction and challenges of environment archi-
tectures for collective intelligence systems,"" agent environments for multi-agent sys-
tems iv, vol. 9068, pp. 76-94, 2015.  
 
s6) h. yokoyama, ""machine learning system architectural pattern for improving op-
erational stability,"" ieee international conference on software architecture workshops 
(icsaw), pp. 267-274, 2019.  
 
s7) a. musil, j. musil and s. biffl, ""major variants of the sis architecture pattern for 
collective intelligence systems,"" proceedings of the 21st european conference on pattern 
languages of programs, pp. 1-11, 2016.  
 
s8) s. e. a. amershi, ""software engineering for machine learning: a case study,"" 2019 
ieee/acm 41st international conference on software engineering: software engineering 
in practice (icse-seip), pp. 291-300, 2019.  
 
s9) h. washizaki, h. takeuchi, f. khomh, n. d. t. natori and s. okuda, ""practitioners’ 
insights on machine-learning software engineering design patterns: a preliminary study,"" 
2020 ieee international conference on software maintenance and evolution (icsme), 
pp. 797-799, 2020.  
 
s10) r. mayer and h. jacobsen, ""scalable deep learning on distributed infrastructures: 
challenges, techniques, and tools,"" acm computing surveys (csur), vol. 53, no. 1, 
pp. 1-37, 2020.  
 
67 
 
 
s11) h. muccini and k. vaidhyanathan, ""software architecture for ml-based systems: 
what exists and what lies ahead,"" arxiv:2103.07950, pp. 1-8, 2021.  
 
s12) a. ahmad and m. babar, ""software architectures for robotic systems: a systematic 
mapping study,"" journal of systems and software, vol. 122, pp. 16-39, 2016.  
 
s13) h. washizaki, h. uchida, f. khomh and y. guéhéneuc, ""studying software engi-
neering patterns for designing machine learning systems,"" 2019 10th international 
workshop on empirical software engineering in practice (iwesep), pp. 49-495, 2019.  
 
s14) m. scheerer, j. klamroth, r. reussner and b. beckert, ""towards classes of archi-
tectural dependability assurance for machine-learning-based systems,"" seams '20: pro-
ceedings of the ieee/acm 15th international symposium on software engineering for 
adaptive and self-managing systems, pp. 31-37, 2020.  
 
s15) c. castellanos, b. pérez, d. correal and c. varela, ""a model-driven architectural 
design method for big data analytics applications,"" 2020 ieee international confer-
ence on software architecture companion (icsa-c), pp. 89-94, 2020.  
 
s16)  a. biondi, f. nesti, g. cicero, d. casini and g. buttazzo, ""a safe, secure, and pre-
dictable software architecture for deep learning in safety-critical systems,"" ieee em-
bedded systems letters, vol. 12, no. 3, pp. 78-82, 2020.  
 
s17)  a. panousopoulou, s. farrens, k. fotiadou, a. woiselle, g. tsagkatakis, j. starck 
and p. tsakalide, ""a distributed learning architecture for scientific imaging problems,"" 
arxiv:1809.05956 , pp. 1-40, 2018.  
 
s18)  m. e. a. jin, ""an anomaly detection algorithm for microservice architecture 
based on robust principal component analysis,"" ieee access, vol. 8, pp. 226397-
226408, 2020.  
 
s19)  p. alarcon, m. gomez, j. campos, s. aguilar, s. romero and p. serrahima, ""a ho-
listic approach for intelligent automated control technology for balanced automation 
systems,"" the international federation for information , pp. 301-308, 1995.  
 
s20)  l. spalazzi, m. paolanti and e. frontoni, ""an offline parallel architecture for foren-
sic multimedia classification,"" multimedia tools and applications, 2021.  
 
s21)  a. e. a. berquand, ""artificial intelligence for the early design phases of space mis-
sions,"" 2019 ieee aerospace conference, pp. 1-20, 2019.  
 
s22)  a. serban, ""designing safety critical software systems to manage inherent uncer-
tainty,"" 2019 ieee international conference on software architecture companion (icsa-
c), pp. 246-249, 2019.  
 
 
68 
 
s23)  v. indumathi and g. nasira, ""fault tolerance in job scheduling 
through fault management framework using soa in grid,"" 
ictact journal on soft computing, vol. 07, no. 02, pp. 1-5, 2017.  
 
s24)  l. li, j. wang and c. xu, ""flsim: an extensible and reusable simulation frame-
work for federated learning,"" icst institute for computer sciences, social informatics 
and telecommunications engineering, pp. 350-369, 2021.  
 
s25)  o. fomin and o. derevianchenko, ""improvement of the quality of cutting tools 
states recognition using cloud technologies,"" advances in design, simulation and 
manufacturing iii. dsmie 2020, pp. 243-252, 2020.  
 
s26)  e. kusmenko, s. nickels, s. pavlitskaya, b. rumpe and t. t, ""modeling and train-
ing of neural processing systems,"" 2019 acm/ieee 22nd international conference on 
model driven engineering languages and systems (models), pp. 283-293, 2019.  
 
s27)  m. e. a. möstl, ""platform-centric self-awareness as a key enabler for controlling 
changes in cps,"" proceedings of the ieee, vol. 106, no. 9, pp. 1543-1567, 2018.  
 
s28)  a. muzaffar, s. mir, m. latif, w. butt and a. f, ""software architecture of a mo-
bile robot,"" international conference on computational science and computational intel-
ligence, pp. 1-6, 2015.  
 
s29)  b. vinayagasundaram and s. srivatsa, ""software quality in artificial intelligence 
system,"" information technology journal, vol. 6, pp. 835-842, 2007.  
 
s30)  a. serban, e. poll and j. visser, ""towards using probabilistic models to design 
software systems with inherent uncertainty,"" european conference on software archi-
tecture ecsa 2020 software architecture , pp. 89-97, 2020.  
 
s31)  e. buccio, a. lorenzet, m. melucci and f. neresini, ""unveiling latent states be-
hind social indicators"".  
 
s32)  m. bhat, k. shumaiev, k. koch, u. hohenstein, a. biesdorf and f. matthes, ""an 
expert recommendation system for design decision making: who should be involved in 
making a design decision?,"" 2018 ieee international conference on software architec-
ture (icsa), pp. 85-8509, 2018.  
 
s33)  h. venthur, s. dähne, j. hönhe, h. heller and b. blankertz, ""wyrm: a brain-com-
puter interface toolbox in python,"" neuroinformatics, vol. 13, p. 471–486, 2015.  
 
s34)  d. baylor and et al., ""tfx: a tensorflow-based production-scale machine learn-
ing platform,"" proceedings of the 23rd acm sigkdd international conference on 
knowledge discovery and data mining, p. 1387–1395, 2017.  
 
 
69 
 
s35)  a. anjos, m. günther, t. de freitas pereira, p. m. a. korshunov and s. marcel, 
""continuously reproducing toolchains in pattern recognition and machine learning ex-
periments,"" icml 2017 rml program chairs, 2017.  
 
s36)  b. burns and d. oppenheimer, ""design patterns for container-based distributed sys-
tems,"" usenix} association, denver, 2016. 
 
s37)  p. raut and n. borkar, ""machine learning algorithms:trends, perspectives and 
prospects,"" international journal of engineering science and computing, vol. 7, no. 3, pp. 
4884-4891, 2017.  
 
s38)  d. e. a. sculley, ""hidden technical debt in machine learning systems,"" advances 
in neural information processing systems, curran associatesd, inc.,, pp. 1-9, 2015.  
 
s39)  s. schelter, b. f, t. januschowski, d. salinas and s. s. g. seufert, ""on challenges 
in machine learning model management,"" bulletin of the ieee computer society tech-
nical committee on data engineering, pp. 1-11, 2018.  
 
s40)  m. e. a. abadi, ""tensorflow: a system for large-scale machine learning,"" the 
advance computing systems association, pp. 1-21, 2016.  
 
s41)  n. chervyakov, p. lyakhov, m. deryabin, n. nagornov, m. valueva and g. val-
uev, ""residue number system-based solution for reducing the hardware cost of a con-
volutional neural network,"" neurocomputing, vol. 407, pp. 439-453, 2020. 
 
 
 
 
70 
 
 
 
 
 
 
appendix b 
interview invitation:  
interview invitation  
invitation to participate in our research project titled 
""software architecture design challenges for machine 
learning systems"" 
hello 
 
my name is roger nazir. i am doing my master thesis in software engineering under the 
guidance of associate professor patrizio pelliccione from chalmers university of technology, 
sweden. this research aims to investigate architectural design decisions for machine learning 
(ml) systems. this study aims to help developers to have a comprehensive and order classifi-
cation of common challenges, best practices, and main software architecture (sa) design deci-
sions of ml systems from the available studies. it will also highlight the ml software design 
complexity and common ml design techniques. the study will help developers/designers to 
learn the best ml design practice to minimize the challenges in creating large-scale ml solu-
tions. i hope this study will offer some essential contributions to future research work and pre-
sent a platform to assist young architects by suggesting appropriate architecture designs. 
initially, i am conducting a systemic literature review with snowballing. then through 
an in-depth interview, i will collect the practitioner's opinions, which will be compared to 
the results of the systematic literature review. 
 
i need your time for interviewing a video call, which takes 30-40 minutes. your partici-
pation in this study based on your knowledge and experience will be valuable to our re-
search. the interview session will only be recorded after getting permission from the 
interviewee. no such details will be added in the study that will point to any personal in-
formation of the interviewee. we kindly invite you to give your opinions if you are willing 
to participate. please suggest a day and time that suits you, and i will do my best to be 
available. if you have any questions, please do not hesitate to ask. i appreciate any help 
you can provide. 
 
regards, 
roger nazir 
 
supervisor: 
associate professor patrizio pelliccione 
department of computer science and engineering software engineering 
chalmers university of technology 
patrizio.pelliccione@cse.gu.se 
 
71 
 
 
 
 
72 
 
questions:  
 
1. can you please introduce yourself and describe your job role in this company? 
 
2. since how many years you are working in this company? 
 
3. have you published any thesis in the machine learning domain?  
 
4. can you please share your experience in your current position? 
 
5. do you have any experience in the previous company which is developing machine 
learning system? if so, then what was your old experience? 
 
6. is your company is service-based or product-based? 
 
7. what software development model do you practice in your company in general, like an 
agile, waterfall, etc.? 
 
8. could you please share your experience with the interesting machine learning project 
you have recently worked on? 
 
9. in your working experience, how many software architecture design techniques of ma-
chine learning you worked with?  
 
10. which common software architecture design technique of machine learning you found 
being used in most companies through your experience?  
 
11. according to your experience, which are your best software architecture design tech-
nique for machine learning, and what are the benefits of using them? 
 
12. do you have any recommendations for software architecture design techniques of ma-
chine learning systems?  
 
13. which would be the best practice that could be useful/helpful in applying software ar-
chitecture designing of machine learning systems?  
 
14. what are the most common software architecture design challenges in machine learn-
ing systems? 
 
15. what are the main architectural decisions on software architecture design of different 
machine learning systems? 
 
 
"
elb-12-11-05.pdf;"
references
[1] y. g. gordienko, s. stirenko, y. p. kochura, o. alienin, m. novotarskiy and n. gordienko, deep
learning for fatigue estimation on the basis of multimodal human-machine interactions, arxiv.org,
arxiv: 1801.06048, 2017.
[2] e. frank and m. a. hall, data mining: practical machine learning tools and techniques, morgan
kaufmann, 2011.
[3] s. sonnenburg, m. l. braun, c. s. ong, s. bengio, l. bottou, g. holmes, y. lecun, k.-r. m¨uller,
f. pereira and c. e. rasmussen, the need for open source software in machine learning, journal of
machine learning research, vol.8, no.10, pp.2443-2466, 2007.
[4] d. a. tamburri and r. kazman, general methods for software architecture recovery: a potential
approach and its evaluation, empirical software engineering, vol.23, no.3, pp.1457-1489, 2018.
[5] m. akour, h. al sghaier and o. al qasem, the eﬀectiveness of using deep learning algorithms
in predicting students achievements, indonesian journal of electrical engineering and computer
science, vol.19, no.1, pp.387-393, 2020.
[6] m. akour and m. alenezi, test suites eﬀectiveness evolution in open source systems: empirical
study, indonesian journal of electrical engineering and computer science, vol.19, no.2, pp.1085-
1092, 2020.
[7] o. al qasem, m. akour and m. alenezi, the inﬂuence of deep learning algorithms factors in software
fault prediction, ieee access, vol.8, pp.63945-63960, 2020.
[8] h. alsghaier and m. akour, software fault prediction using particle swarm algorithm with genetic
algorithm and support vector machine classiﬁer, software: practice and experience, vol.50, no.4,
pp.407-427, 2020.
[9] o. al qasem and m. akour, software fault prediction using deep learning algorithms, international
journal of open source software and processes (ijossp), vol.10, no.4, 2019.
[10] m. akour, o. al qasem, h. alsghaier and k. al-radaideh, the eﬀectiveness of using deep learning
algorithms in predicting daily activities, international journal of advanced trends in computer
science and engineering, vol.8, no.5, pp.2231-2235, 2019.
[11] m. alenezi and i. abunadi, quality of open source systems from product metrics perspective, inter-
national journal of computer science issues (ijcsi), vol.12, no.5, p.143, 2015.
1026
m. alenezi and m. akour
[12] f. a. fontana, v. lenarduzzi, r. roveda and d. taibi, are architectural smells independent from
code smells? an empirical study, journal of systems and software, vol.154, pp.139-156, 2019.
[13] i. t. bowman, r. c. holt and n. v. brewster, linux as a case study: its extracted software
architecture, proc. of the 1999 international conference on software engineering, pp.555-563, 1999.
[14] b. grone, a. kn¨opfel and r. kugel, architecture recovery of apache 1.3 – a case study, international
conference on software engineering research and practice, 2002.
[15] a. e. hassan and r. c. holt, architecture recovery of web applications, proc. of the 24th interna-
tional conference on software engineering, pp.349-359, 2002.
[16] i. ivkovic and m. w. godfrey, architecture recovery of dynamically linked applications: a case
study, proc. of the 10th international workshop on program comprehension, pp.178-184, 2002.
[17] w. eixelsberger, m. kalan, m. ogris, h. beckman, b. bellay and h. gall, recovery of architectural
structure: a case study, international workshop on architectural reasoning for embedded systems,
pp.89-96, 1998.
[18] w. eixelsberger, recovery of a reference architecture: a case study, proc. of the 3rd international
workshop on software architecture, pp.33-36, 1998.
[19] g. granchelli, m. cardarelli, p. di francesco, i. malavolta, l. iovino and a. di salle, towards
recovering the software architecture of microservice-based systems, ieee international conference
on software architecture workshops (icsaw), pp.46-53, 2017.
[20] x. zhang, m. persson, m. nyberg, b. mokhtari, a. einarson, h. linder, j. westman, d. chen
and m. t¨orngren, experience on applying software architecture recovery to automotive embedded
systems, software evolution week – ieee conference on software maintenance, reengineering, and
reverse engineering (csmr-wcre), pp.379-382, 2014.
[21] j. garcia, i. ivkovic and n. medvidovic, a comparative analysis of software architecture recovery
techniques, the 28th ieee/acm international conference on automated software engineering
(ase), pp.486-496, 2013.
[22] v. tzerpos and r. c. holt, accd: an algorithm for comprehension-driven clustering, proc. of the
7th working conference on reverse engineering, pp.258-267, 2000.
[23] j. garcia, d. popescu, c. mattmann, n. medvidovic and y. cai, enhancing architectural recovery
using concerns, the 26th ieee/acm international conference on automated software engineering
(ase2011), pp.552-555, 2011.
[24] b. s. mitchell and s. mancoridis, on the automatic modularization of software systems using the
bunch tool, ieee trans. software engineering, vol.32, no.3, pp.193-208, 2006.
[25] p. andritsos and v. tzerpos, information-theoretic software clustering, ieee trans. software en-
gineering, vol.31, no.2, pp.150-165, 2005.
[26] o. maqbool and h. babri, hierarchical clustering for software architecture recovery, ieee trans.
software engineering, vol.33, no.11, pp.759-780, 2007.
[27] k. sartipi, k. kontogiannis and f. mavaddat, a pattern matching framework for software architec-
ture recovery and restructuring, proc. of the 8th international workshop on program comprehension
(iwpc2000), pp.37-47, 2000.
[28] s. kalyanasundaram, k. ponnambalam, a. singh, b. j. stacey and r. munikoti, metrics for software
architecture: a case study in the telecommunication domain, proc. of ieee canadian conference
on electrical and computer engineering, vol.2, pp.715-718, 1998.
[29] m. alenezi, software architecture quality measurement stability and understandability, international
journal of advanced computer science and applications (ijacsa), vol.7, no.7, pp.550-559, 2016.
[30] m. staron and w. meding, a portfolio of internal quality metrics for software architects, interna-
tional conference on software quality, pp.57-69, 2017.
[31] a. abu hassan and m. alshayeb, a metrics suite for uml model stability, software & systems
modeling, vol.18, no.1, pp.557-583, 2019.
[32] t. al hamed and m. alenezi, business continuity management & disaster recovery capabilities in
saudi arabia ict businesses, international journal of hybrid information technology, vol.9, no.11,
pp.99-126, 2016.
[33] m. alenezi and m. zarour, modularity measurement and evolution in object-oriented open-source
projects, proc. of the international conference on engineering & mis, pp.1-7, 2015.
[34] d. singh, an optimizating the software metrics for uml structural and behaviourl diagrams using
metrics tool, infocomp journal of computer science, vol.18, no.1, pp.9-19, 2019.
"
Francesco_Maffezzoli_Thesis.pdf;
Gusenbauern and Haddaway_2019_Which academic search systems are suitable.pdf;"
references
1. price djs. little science, big science. new york: columbia
univ. press; 1963 columbia paperback.
2. larsen po, von ins m. the rate of growth in scientific publica-
tion and the decline in coverage provided by science citation
gusenbauer and haddaway
213
index. scientometrics. 2010;84(3):575-603. https://doi.org/10.
1007/s11192-010-0202-z
3. eden d. from the editors: replication, meta-analysis, scientific
progress, and amj's publication policy. amj. 2002;45(5):841-
846. https://doi.org/10.5465/amj.2002.7718946
4. naisbitt j, aburdene p. megatrends 2000: ten new directions
for the 1990's. 1st ed. new york: morrow; 1990.
5. cooper hm. research synthesis and meta-analysis: a step-by-
step approach. applied social research methods series. fifth
ed. 2 los angeles: sage; 2017.
6. littell jh. conceptual and practical classification of research
reviews and other evidence synthesis products; 2018.
7. kostoff rn, shlesinger mf. cab: citation-assisted background.
scientometrics.
2005;62(2):199-212.
https://doi.org/10.1007/
s11192-005-0014-8
8. littell jh, corcoran j, pillai v. systematic reviews and meta-
analysis: oxford university press, usa; 2008. https://books.
google.at/books?id=upsrdaaaqbaj.
9. rethlefsen
ml,
farrell
am,
osterhaus
trzasko
lc,
brigham tj. librarian co-authors correlated with higher qual-
ity reported search strategies in general internal medicine sys-
tematic reviews. j clin epidemiol. 2015;68(6):617-626. https://
doi.org/10.1016/j.jclinepi.2014.11.025
10. bandara w, furtmueller e, gorba, gorbacheva e, miskon s,
beekhuyzen j. achieving rigor in literature reviews: insights
from qualitative data analysis and tool-support. communica-
tions of the association for information systems. 2015;37(8):154-
204. http://aisel.aisnet.org/cais/vol37/iss1/8
11. meert d, torabi n, costella j. impact of librarians on reporting
of the literature searching component of pediatric systematic
reviews. j med libr assoc. 2016;104(4):267-277. https://doi.org/
10.3163/1536-5050.104.4.004
12. koffel
jb.
use
of
recommended
search
strategies
in
systematic reviews and the impact of librarian involvement:
a cross-sectional survey of recent authors. plos one. 2015;10(5):
1-13. https://doi.org/10.1371/journal.pone.0125931
13. livoreil b, glanville j, haddaway nr, et al. systematic
searching for environmental evidence using multiple tools and
sources. environ evid. 2017;6:1-14. https://doi.org/10.1186/
s13750-017-0099-6
14. higgins jpt, green s. cochrane handbook for systematic
reviews of interventions; 2011; version 5.1.0.
15. kugley s, wade a, thomas j, et al. searching for studies:
guidelines on information retrieval for campbell systematic
reviews; 2016; 1.
16. pullin a, frampton g, livoreil b, petrokofsky g. guidelines
and standards for evidence synthesis in environmental man-
agement. version 5.0; 2018.
17. hug se, braendle mp. the coverage of microsoft academic:
analyzing the publication output of a university. scientometrics.
2017;113(3):1551-1571.
https://doi.org/10.1007/s11192-017-
2535-3
18. khabsa m, giles cl. the number of scholarly documents on
the public web. plos one. 2014;9(5):1-6. https://doi.org/10.
1371/journal.pone.0093949
19. falagas me, pitsouni ei, malietzis ga, pappas g. comparison
of pubmed, scopus, web of science, and google scholar:
strengths and weaknesses. faseb journal. 2008;22(2):338-342.
https://doi.org/10.1096/fj.07-9492lsf
20. orduña-malea e, ayllón jm, martín-martín a, delgado l-ce.
methods for estimating the size of google scholar. sci-
entometrics.
2015;104(3):931-949.
https://doi.org/10.1007/
s11192-015-1614-6
21. harzing a-w. a longitudinal study of google scholar coverage
between 2012 and 2013. scientometrics. 2014;98(1):565-575.
https://doi.org/10.1007/s11192-013-0975-y
22. meier jj, conkling tw. google scholar's coverage of the engi-
neering literature: an empirical study. the journal of academic
librarianship.
2008;34(3):196-201.
https://doi.org/10.1016/j.
acalib.2008.03.002
23. turnbull d, berryman j. relevant search: with applications for
solr and elasticsearch. shelter island new york: manning pub-
lications co; 2016.
24. levay p, ainsworth n, kettle r, morgan a. identifying evi-
dence for public health guidance: a comparison of citation
searching with web of science and google scholar. res synth
methods. 2016;7(1):34-45. https://doi.org/10.1002/jrsm.1158
25. boeker m, vach w, motschall e. google scholar as replace-
ment for systematic literature searches: good relative recall and
precision are not enough. bmc med res methodol. 2013;13:1-
12. https://doi.org/10.1186/1471-2288-13-131
26. hjørland b. classical databases and knowledge organization: a
case for boolean retrieval and human decision-making during
searches. j assn inf sci tec. 2015;66(8):1559-1575. https://doi.
org/10.1002/asi.23250
27. weber
k.
search
engine
bias.
in:
lewandowski
d,
ed. handbuch internet-suchmaschinen 2. aka verlag heidel-
berg; 2011:265-285.
28. vaughan l, thelwall m. search engine coverage bias: evidence
and possible causes. information processing & management.
2004;40(4):693-707.
https://doi.org/10.1016/s0306-4573(03)
00063-3
29. carmines eg, zeller ra. reliability and validity assessment.
quantitative applications in the social sciences. beverly hills,
london: sage publications; 1979 no.07–017.
30. jacsó p. google scholar: the pros and the cons. online informa-
tion review. 2005;29(2):208-214. https://doi.org/10.1108/14684
520510598066
31. jacsó p. google scholar revisited. online information review.
2008;32(1):102-114.
https://doi.org/10.1108/1468452081086
6010
32. bethel a, rogers m. a checklist to assess database-hosting plat-
forms for designing and running searches for systematic
reviews. health info libr j. 2014;31(1):43-53. https://doi.org/10.
1111/hir.12054
33. bramer wm, giustini d, kramer b, anderson p. the compara-
tive recall of google scholar versus pubmed in identical
searches for biomedical systematic reviews: a review of
searches used in systematic reviews. syst rev. 2013;2:1-9.
https://doi.org/10.1186/2046-4053-2-115
34. bramer wm, giustini d, kramer bmr. comparing the cover-
age, recall, and precision of searches for 120 systematic reviews
in embase, medline, and google scholar: a prospective
study. syst rev. 2016;5:1-9. https://doi.org/10.1186/s13643-016-
0215-7
35. mowshowitz a, kawaguchi a. measuring search engine bias.
information processing & management. 2005;41(5):1193-1205.
https://doi.org/10.1016/j.ipm.2004.05.005
214
gusenbauer and haddaway
36. moher d, liberati a, tetzlaff j, altman dg. preferred
reporting items for systematic reviews and meta-analyses: the
prisma statement. plos med. 2009;6(7):1-6. https://doi.org/
10.1371/journal.pmed.1000097
37. gusenbauer m. google scholar to overshadow them all? com-
paring the sizes of 12 academic search engines and biblio-
graphic databases. scientometrics. 2019;118(1):177-214. https://
doi.org/10.1007/s11192-018-2958-5
38. ortega jl. academic search engines: a quantitative outlook.
chandos information professional series. oxford, uk: chandos
publishing/elsevier; 2014.
39. schöpfel
j,
farace
dj.
grey
literature.
in:
bates
mj,
maack mn, eds. encyclopedia of library and information sci-
ences. 3rd ed. / edited by boca raton, fla: crc london: tay-
lor & francis; 2010:2029–2039.marcia j. bates and mary niles
maack
40. sampson m, mcgowan j. inquisitio validus index medicus: a
simple method of validating medline systematic review
searches. res synth methods. 2011;2(2):103-109. https://doi.org/
10.1002/jrsm.40
41. rogers m, bethel a, abbott r. locating qualitative studies in
dementia on medline, embase, cinahl, and psycinfo: a
comparison of search strategies. res synth methods. 2017;9(2):
579-586. https://doi.org/10.1002/jrsm.1280
42. rader t, mann m, stansfield c, cooper c, sampson m.
methods for documenting systematic review searches: a discus-
sion of common issues. res synth methods. 2014;5(2):98-115.
https://doi.org/10.1002/jrsm.1097
43. o'mara-eves a, brunton g, mcdaid d, kavanagh j, oliver s,
thomas j. techniques for identifying cross-disciplinary and
‘hard-to-detect’ evidence for systematic review. res synth
methods. 2014;5(1):50-59. https://doi.org/10.1002/jrsm.1094
44. atkinson
km,
koenka
ac,
sanchez
ce,
moshontz
h,
cooper h. reporting standards for literature searches and
report inclusion criteria: making research syntheses more
transparent and easy to replicate. res synth methods. 2015;6(1):
87-95. https://doi.org/10.1002/jrsm.1127
45. mahood q, van eerd d, irvin e. searching for grey literature
for systematic reviews: challenges and benefits. res synth
methods. 2014;5(3):221-234. https://doi.org/10.1002/jrsm.1106
46. bar-ilan j. on the overlap, the precision and estimated recall of
search engines. a case study of the query “erdos”. sci-
entometrics.
1998;42(2):207-228.
https://doi.org/10.1007/
bf02458356
47. kumar bts, prakash jn. precision and relative recall of search
engines: a comparative study of google and yahoo. singapore
journal of library and information management. 2009;38:
124-137.
48. shafi sm, rather r. precision and recall of five search engines
for retrieval of scholarly information in the field of biotechnol-
ogy. webology. 2005;2(2):1-7.
49. usmani ta, pant d, bhatt ak. a comparative study of google
and bing search engines in context of precision and relative
recall parameter. international journal on computer science
and engineering (ijcse). 2012;4(1):21-34.
50. giustini d, boulos mnk. google scholar is not enough to
be
used
alone
for
systematic
reviews.
online
j
public
health inform. 2013;5(2):1-10. https://doi.org/10.5210/ojphi.
v5i2.4623
51. bramer wm. variation in number of hits for complex searches
in google scholar. journal of the medical library association.
2016;104(2):143-145.
https://doi.org/10.3163/1536-5050.104.
2.009
52. brophy j, bawden d. is google enough? comparison of an
internet search engine with academic library resources. aslib
proceedings. 2005;57(6):498-512. https://doi.org/10.1108/00012
530510634235
53. gehanno j-f, rollin l, darmoni s. is the coverage of google
scholar enough to be used alone for systematic reviews. bmc
medical informatics and decision making. 2013;13(7):1-5.
54. sturm b, sunyaev a. if you want your research done right, do
you have to do it all yourself? developing design principles for
systematic literature search systems. in: maedche a, vom
brocke j, hevner a, eds. designing the digital transformation;
2017:138–146.
55. chu h, rosenthal m. search engines for the world wide web:
a comparative study and evaluation methodology. j. am. soc.
inf. sci. 1996;33:127-135.
56. biolcati-rinaldi f, molteni f, salini s. assessing the reliability
and validity of google scholar indicators. the case of social sci-
ences in italy. in: bonaccorsi a, ed. the evaluation of research
in social sciences and humanities: lessons from the italian
experience. cham: springer international publishing; 2018:
295-319.
57. haddaway nr, collins am, coughlin d, kirk s. the role of
google scholar in evidence reviews and its applicability to grey
literature searching. plos one. 2015;10(9):1-17. https://doi.org/
10.1371/journal.pone.0138237
58. aune d, giovannucci e, boffetta p, et al. fruit and vegetable
intake and the risk of cardiovascular disease, total cancer and
all-cause mortality—a systematic review and dose–response
meta-analysis of prospective studies. international journal of
epidemiology.
2017;46(3):1029-1056.
https://doi.org/10.1093/
ije/dyw319
59. barnett dw, barnett a, nathan a, van cauwenberg j, cerin e.
built environmental correlates of older adults' total physical
activity and walking: a systematic review and meta-analysis.
international journal of behavioral nutrition and physical
activity.
2017;14(1):1-24.
https://doi.org/10.1186/s12966-017-
0558-z
60. baur d, gladstone bp, burkert f, et al. effect of antibiotic
stewardship on the incidence of infection and colonisation with
antibiotic-resistant bacteria and clostridium difficile infection: a
systematic review and meta-analysis. lancet infectious diseases.
2017;17(9):990-1001.
https://doi.org/10.1016/s1473-3099(17)
30325-0
61. bediou b, adams dm, mayer re, tipton e, green cs,
bavelier d. meta-analysis of action video game impact on per-
ceptual, attentional, and cognitive skills. psychological bulletin.
2018;144(1):77-110. https://doi.org/10.1037/bul0000130
62. bethel ma, patel ra, merrill p, et al. cardiovascular outcomes
with glucagon-like peptide-1 receptor agonists in patients with
type 2 diabetes: a meta-analysis. lancet diabetes & endocrinol-
ogy. 2018;6(2):105-113. https://doi.org/10.1016/s2213-8587(17)
30412-6
63. bourne rra, flaxman sr, braithwaite t, et al. magnitude,
temporal trends, and projections of the global prevalence of
blindness
and
distance
and
near
vision
impairment:
a
gusenbauer and haddaway
215
systematic review and meta-analysis. lancet global health.
2017;5(9):e888-e897.
https://doi.org/10.1016/s2214-109x(17)
30293-0
64. brunoni ar, chaimani a, moffa ah, et al. repetitive trans-
cranial magnetic stimulation for the acute treatment of major
depressive episodes a systematic review with network meta-
analysis. jama psychiatry. 2017;74(2):143-152. https://doi.org/
10.1001/jamapsychiatry.2016.3644
65. carlbring p, andersson g, cuijpers p, riper h, hedman-
lagerlof e. internet-based vs. face-to-face cognitive behavior
therapy for psychiatric and somatic disorders: an updated sys-
tematic review and meta-analysis. cognitive behaviour therapy.
2018;47(1):1-18. https://doi.org/10.1080/16506073.2017.1401115
66. chu dk, kim lh-y, young pj, et al. mortality and morbidity
in acutely ill adults treated with liberal versus conservative oxy-
gen therapy (iota): a systematic review and meta-analysis.
lancet.
2018;391(10131):1693-1705.
https://doi.org/10.1016/
s0140-6736(18)30479-3
67. cipriani a, furukawa ta, salanti g, et al. comparative effi-
cacy and acceptability of 21 antidepressant drugs for the acute
treatment of adults with major depressive disorder: a system-
atic review and network meta-analysis. lancet. 2018;391
(10128):1357-1366.
https://doi.org/10.1016/s0140-6736(17)
32802-7
68. stacey d, légaré f, lewis k, et al. decision aids for people fac-
ing health treatment or screening decisions. cochrane database
syst
rev.
2017;4:1-343.
https://doi.org/10.1002/14651858.
cd001431.pub5
69. oxford wordlist. oxford university press; 2008.
70. fieschi m, coiera e, li ycj. medinfo: ios press; 2004. https://
books.google.at/books?id=bs2xdt7iufgc.
71. meho li, yang k. impact of data sources on citation counts
and rankings of lis faculty: web of science versus scopus and
google scholar. j. am. soc. inf. sci. 2007;58(13):2105-2125.
https://doi.org/10.1002/asi.20677
72. martín-martín
a,
orduna-malea
e,
thelwall
m,
lópez-
cózar ed. google scholar, web of science, and scopus: a sys-
tematic comparison of citations in 252 subject categories. jour-
nal of informetrics. 2018;12(4):1160-1177. https://doi.org/10.
31235/osf.io/42nkm
73. bakkalbasi n, bauer k, glover j, wang l. three options for
citation tracking: google scholar, scopus and web of science.
biomed digit libr. 2006;3(7):1-8. https://doi.org/10.1186/1742-
5581-3-7
74. haddaway nr, macura b, whaley p, pullin as. roses
reporting standards for systematic evidence syntheses: pro
forma, flow-diagram and descriptive summary of the plan and
conduct of environmental systematic reviews and systematic
maps.
environ
evid.
2018;7(7):1-8.
https://doi.org/10.1186/
s13750-018-0121-7
75. white rw, roth ra. exploratory search: beyond the query-
response paradigm. morgan & claypool; 2009.
76. jansen bj, spink a. in: langendoerfer p, droegehorn o, eds.
ic'2003an analysis of web documents retrieved and viewed;
2003:65-69.
77. jansen bj, spink a. how are we searching the world wide
web?: a comparison of nine search engine transaction logs.
information processing & management. 2006;42(1):248-263.
https://doi.org/10.1016/j.ipm.2004.10.007
78. athukorala
k,
głowacka
d,
jacucci
g,
oulasvirta
a,
vreeken j. is exploratory search different?: a comparison of
information search behavior for exploratory and lookup tasks.
j assn inf sci tec. 2016;67(11):2635-2651. https://doi.org/10.
1002/asi.23617
79. hemminger bm, lu d, vaughan ktl, adams sj. information
seeking behavior of academic scientists. j. am. soc. inf. sci.
2007;58(14):2205-2225. https://doi.org/10.1002/asi.20686
80. athukorala k, hoggan e, lehtiö a, ruotsalo t, jacucci g.
information-seeking behaviors of computer scientists: chal-
lenges for electronic literature search tools. proc. am. soc. info.
sci.
tech.
2013;50(1):1-11.
https://doi.org/10.1002/meet.
14505001041
81. nicholas d, boukacem-zeghmouri c, rodríguez-bravo b, et al.
where and how early career researchers find scholarly infor-
mation. learned publishing. 2017;30(1):19-29. https://doi.org/
10.1002/leap.1087
82. niu x, hemminger bm. a study of factors that affect the
information-seeking behavior of academic scientists. j. am.
soc. inf. sci. 2012;63(2):336-353. https://doi.org/10.1002/asi.
21669
83. sapa r, krakowska m, janiak m. information seeking behav-
iour of mathematicians: scientists and students. information
research: an international electronic journal. 2014;19(4):1-11.
84. fast kv, campbell dg. “i still like google”: university student
perceptions of searching opacs and the web. proceedings of
the american society for information science and technology.
2004;41(1):138-146. https://doi.org/10.1002/meet.1450410116
85. kuiper e, volman m, terwel j. students' use of web literacy
skills and strategies: searching, reading and evaluating web
information. information research. 2008;13(3):1-18.
86. ingwersen p. cognitive perspectives of information retrieval
interaction: elements of a cognitive ir theory. journal of docu-
mentation. 1996;52(1):3-50. https://doi.org/10.1108/eb026960
87. wellings s, casselden b. an exploration into the information-
seeking behaviours of engineers and scientists. journal of
librarianship and information science. 2017;9(2):1-12. https://
doi.org/10.1177/0961000617742466
88. rowlands i, nicholas d, williams p, et al. the google genera-
tion: the information behaviour of the researcher of the future.
ap.
2008;60(4):290-310.
https://doi.org/10.1108/00012530810
887953
89. kingsley k, galbraith gm, herring m, stowers e, stewart t,
kingsley kv. why not just google it? an assessment of infor-
mation literacy skills in a biomedical science curriculum. bmc
med educ. 2011;11(17):1-8. https://doi.org/10.1186/1472-6920-
11-17
90. kurbano�glu s, boustany j, špiranec s, grassian e, mizrachi d,
roy l, eds. information literacy: moving toward sustainability:
third
european
conference,
ecil
2015,
tallinn,
estonia,
october 19–22, 2015: revised selected papers. cham, heidelberg,
new york: springer; 2015. communications in computer and
information science; 552.
91. kurbano�glu s, boustany j, špiranec s, et al. (eds). search
engine
literacy:
information
literacy
in
the.
workplace:
springer international publishing; 2018.
92. brindesi h, monopoli m, kapidakis s. information seeking and
searching habits of greek physicists and astronomers: a case
study
of
undergraduate
students.
procedia
-
social
and
216
gusenbauer and haddaway
behavioral sciences. 2013;73:785-793. https://doi.org/10.1016/j.
sbspro.2013.02.119
93. kirschner pa, de bruyckere p. the myths of the digital native
and the multitasker. teaching and teacher education. 2017;67:
135-142. https://doi.org/10.1016/j.tate.2017.06.001
94. georgas h. google vs. the library (part ii): student search
patterns and behaviors when using google and a federated
search tool. portal: libraries and the academy. 2014;14(4):
503-532.
95. halevi g, moed h, bar-ilan j. suitability of google scholar as a
source of scientific information and as a source of data for sci-
entific
evaluation:
review
of
the
literature.
journal
of
informetrics.
2017;11(3):823-834.
https://doi.org/10.1016/j.joi.
2017.06.005
supporting information
additional supporting information may be found online
in the supporting information section at the end of this
article.
how to cite this article: gusenbauer m,
haddaway nr. which academic search systems
are suitable for systematic reviews or meta-
analyses? evaluating retrieval qualities of google
scholar, pubmed, and 26 other resources. res syn
meth. 2020;11:181–217. https://doi.org/10.1002/
jrsm.1378
gusenbauer and haddaway
217
"
hi.pdf;"
references
[amir et al., 2016] ofra amir, ece kamar, andrey kolobov,
and barbara grosz.
interactive teaching strategies for
agent training. in ijcai, 2016.
[clouse, 1996] jeffery allen clouse. on integrating appren-
tice learning and reinforcement learning. 1996.
[doroudi et al., 2016] shayan doroudi, ece kamar, emma
brunskill, and eric horvitz. toward a learning science for
complex crowdsourcing tasks. in chi, 2016.
[ipeirotis et al., 2010] panagiotis g ipeirotis, foster provost,
and jing wang. quality management on amazon mechan-
ical turk. in proceedings of the acm sigkdd workshop
on human computation, 2010.
[kamar and horvitz, 2013] ece kamar and eric horvitz.
light at the end of the tunnel: a monte carlo approach
to computing value of information. in aamas, 2013.
[kamar and horvitz, 2015] ece kamar and eric horvitz.
planning for crowdsourcing hierarchical tasks. in aamas,
2015.
[kamar et al., 2012] ece kamar, severin hacker, and eric
horvitz. combining human and machine intelligence in
large-scale crowdsourcing. in aamas, 2012.
[kamar et al., 2013] ece kamar, ashish kapoor, and eric
horvitz. lifelong learning for acquiring the wisdom of
the crowd. in ijcai, 2013.
[kamar et al., 2015] ece kamar, ashish kapoor, and eric
horvitz.
identifying and accounting for task-dependent
bias in crowdsourcing. in hcomp, 2015.
[mao et al., 2013a] andrew mao, ece kamar, yiling chen,
eric horvitz, megan e schwamb, chris j lintott, and ar-
fon m smith. volunteering versus work for pay: incentives
and tradeoffs in crowdsourcing. in hcomp, 2013.
[mao et al., 2013b] andrew mao, ece kamar, and eric
horvitz. why stop now? predicting worker engagement
in online crowdsourcing. in hcomp, 2013.
[mitchell et al., 2014] margaret mitchell, wa redmond,
dan bohus, and ece kamar.
crowdsourcing language
generation templates for dialogue systems. in inlg and
sigdial 2014, 2014.
[segal et al., 2016] avi segal, ya’akov gal, ece kamar, eric
horvitz, alex bowyer, and grant miller.
intervention
strategies for increasing engagement in crowdsourcing:
platform, predictions, and experiments. in ijcai, 2016.
[torrey and taylor, 2013] lisa torrey and matthew taylor.
teaching on a budget: agents advising agents in reinforce-
ment learning. in aamas, 2013.
[wang et al., 2012] wei yu wang, dan bohus, ece kamar,
and eric horvitz. crowdsourcing the acquisition of natural
language corpora: methods and observations. in spoken
language technology workshop (slt), 2012 ieee, 2012.
"
IC2E_Edge_MLOps.pdf;"
references
[1] k. bilal, o. khalid, a. erbad, and s. khan, “potentials, trends, and
prospects in edge technologies: fog, cloudlet, mobile edge, and micro
data centers,” computer networks, vol. 130, 10 2017.
[2] y. wu, y. wu, and s. wu, an outlook of a future smart city in taiwan
from post–internet of things to artiﬁcial intelligence internet of things.
elsevier, 01 2019, pp. 263–282.
[3] w. shi, j. cao, q. zhang, y. li, and l. xu, “edge computing: vision
and challenges,” ieee internet of things journal, vol. 3, pp. 1–1, 10
2016.
[4] g. agarwal, “what is edge ai and how it ﬁlls the cracks of iot, 2019.
[5] m. westerlund, “a study of eu data protection regulation and appropri-
ate security for digital services and platforms,” ph.d. dissertation, ˚abo
akademi university, 2018.
[6] j. koneˇcn´y, h. b. mcmahan, f. x. yu, p. richtarik, a. t. suresh, and
d. bacon, “federated learning: strategies for improving communication
efﬁciency,” in nips workshop on private multi-party machine learning,
2016.
[7] m. beck, m. werner, s. feld, and t. schimper, “mobile edge computing:
a taxonomy,” in the sixth international conference on advances in
future internet (afin 2014), 01 2014.
[8] j. wickstr¨om, m. westerlund, and g. pulkkis, “smart contract based dis-
tributed iot security: a protocol for autonomous device management,”
in proceedings of 21st acm/ieee international symposium on cluster,
cloud and grid computing (ccgrid 2021) (forthcoming), 2021.
[9] k. sato, “what is ml ops? best practices for devops ml,”
2018, presentation at google cloud next’18. [online]. available:
https://cloud.withgoogle.com/next18/sf/sessions/session/192579
[10] e. raj, engineering mlops.
birmingham, united kingdom: packt,
2021.
[11] s. teerapittayanon, b. mcdanel, and h.-t. kung, “distributed deep
neural networks over the cloud, the edge and end devices,” in 2017
ieee 37th international conference on distributed computing systems
(icdcs).
ieee, 2017, pp. 328–339.
[12] d. a. tamburri, “sustainable mlops: trends and challenges,” in 2020
22nd international symposium on symbolic and numeric algorithms for
scientiﬁc computing (synasc), 2020, pp. 17–23.
[13] s. b. calo, m. touna, d. c. verma, and a. cullen, “edge computing ar-
chitecture for applying ai to iot,” in 2017 ieee international conference
on big data (big data), 2017, pp. 3012–3016.
[14] j. chen and x. ran, “deep learning with edge computing: a review,”
proceedings of the ieee, vol. 107, no. 8, pp. 1655–1674, 2019.
[15] c. wu, d. brooks, k. chen, d. chen, s. choudhury, m. dukhan,
k. hazelwood, e. isaac, y. jia, b. jia, t. leyvand, h. lu, y. lu, l. qiao,
b. reagen, j. spisak, f. sun, a. tulloch, p. vajda, x. wang, y. wang,
b. wasti, y. wu, r. xian, s. yoo, and p. zhang, “machine learning
at facebook: understanding inference at the edge,” in 2019 ieee
international symposium on high performance computer architecture
(hpca), 2019, pp. 331–344.
[16] s. wang, c. ding, n. zhang, x. liu, a. zhou, j. cao, and x. s. shen, “a
cloud-guided feature extraction approach for image retrieval in mobile
edge computing,” ieee transactions on mobile computing, 2019.
[17] y. lee, a. scolari, b.-g. chun, m. weimer, and m. interlandi, “from
the edge to the cloud: model serving in ml. net.” ieee data eng. bull.,
vol. 41, no. 4, pp. 46–53, 2018.
[18] b. sudharsan, j. g. breslin, and m. i. ali, “edge2train: a framework to
train machine learning models (svms) on resource-constrained iot edge
devices,” in proceedings of the 10th international conference on the
internet of things, 2020, pp. 1–8.
[19] gartner,
integrate
devops
and
artiﬁcial
intelligence
to
accelerate
it
solution
delivery
and
business
value,
2017,
https://www.gartner.com/doc/3787770/integrate-devops-artiﬁcial-
intelligence-accelerate.
[20] j. hermann and m. d. balso, meet michelangelo: ubers machine
learning platform, 2017, https://eng.uber.com/michelangelo.
[21] m. ali and y. lee, “crm sales prediction using continuous time-evolving
classiﬁcation,” in aaai, 2018.
[22] j. ereth, “dataops-towards a deﬁnition.” lwda, vol. 2191, pp. 104–112,
2018.
[23] a. r. munappy, d. i. mattos, j. bosch, h. h. olsson, and a. dakkak,
“from ad-hoc data analytics to dataops,” in proceedings of the inter-
national conference on software and system processes, 2020, pp. 165–
174.
[24] p. agrawal and n. rawat, “devops, a new approach to cloud de-
velopment testing,” in 2019 international conference on issues and
challenges in intelligent computing techniques (icict), vol. 1, 2019,
pp. 1–4.
[25] h. tian, m. yu, and w. wang, “continuum: a platform for cost-aware,
low-latency continual learning,” in proceedings of the acm symposium
on cloud computing, ser. socc ’18. new york, ny, usa: association
for computing machinery, 2018, p. 26–40.
[26] j. moon, s. kum, and s. lee, “a heterogeneous iot data analysis
framework with collaboration of edge-cloud computing: focusing on
indoor pm10 and pm2. 5 status prediction,” sensors, vol. 19, no. 14, p.
3038, 2019.
[27] a. bhattacharjee, y. barve, s. khare, s. bao, z. kang, a. gokhale, and
t. damiano, “stratum: a bigdata-as-a-service for lifecycle management
of iot analytics applications,” in 2019 ieee international conference on
big data (big data), 2019, pp. 1607–1612.
[28] t. hastie, r. tibshirani, and j. friedman, the elements of statistical
learning, ser. springer series in statistics.
new york, ny, usa:
springer new york inc., 2001.
[29] j. bergstra and y. bengio, “random search for hyper-parameter op-
timization,” j. mach. learn. res., vol. 13, no. null, p. 281–305, feb.
2012.
[30] e. raj, m. westerlund, and l. espinosa-leal, “reliable ﬂeet analytics
for edge iot solutions,” cloud computing 2020: the eleventh inter-
national conference on cloud computing, grids, and virtualization,
p. 55, 2020.
[31] r. akkiraju, v. sinha, a. xu, j. mahmud, p. gundecha, z. liu,
x. liu, and j. schumacher, “characterizing machine learning process:
a maturity framework,” arxiv preprint arxiv:1811.04871, 2018.
[32] a. al-fuqaha, m. guizani, m. mohammadi, m. aledhari, and
m. ayyash, “internet of things: a survey on enabling technologies,
protocols, and applications,” ieee communications surveys tutorials,
vol. 17, no. 4, pp. 2347–2376, 2015.
[33] r. wieringa, design science methodology for information systems and
software engineering.
springer, 2014, 10.1007/978-3-662-43839-8.
[34] r. j. hyndman and g. athanasopoulos, forecasting : principles and
practice, 2nd ed.
otexts.com, 2014.
[35] d. f. andrews, “a robust method for multiple linear regression,”
technometrics, vol. 16, no. 4, pp. 523–531, 1974.
[36] a. akusok, k.-m. bj¨ork, y. miche, and a. lendasse, “high-
performance extreme learning machines: a complete toolbox for big data
applications,” ieee access, vol. 3, pp. 1011–1025, 2015.
[37] a. liaw, m. wiener et al., “classiﬁcation and regression by randomfor-
est,” r news, vol. 2, no. 3, pp. 18–22, 2002.
[38] microsoft, “yearly running cost per rasbpi - raspberry pi forums,”
https://www.raspberrypi.org/forums/viewtopic.php?t=18043, (accessed
on 04/12/2021).
[39] “pricing
-
windows
virtual
machines
—
microsoft
azure,”
https://azure.microsoft.com/en-us/pricing/details/virtual-
machines/windows/, (accessed on 04/12/2021).
[40] p. kumari and p. kaur, “a survey of fault tolerance in cloud computing,”
journal of king saud university - computer and information sciences,
2018.
[41] c. wang, c. gill, and c. lu, “frame: fault tolerant and real-time
messaging for edge computing,” in 2019 ieee 39th international
conference on distributed computing systems (icdcs), 2019, pp. 976–
985.
[42] a. javed, j. robert, k. heljanko, and k. fr¨amling, “iotef: a federated
edge-cloud architecture for fault-tolerant iot applications,” journal of
grid computing 18, vol. 18, 2020.
[43] d. sculley, g. holt, d. golovin, e. davydov, t. phillips, d. ebner,
v. chaudhary, m. young, j.-f. crespo, and d. dennison, “hidden
technical debt in machine learning systems,” in proceedings of the 28th
international conference on neural information processing systems -
volume 2, 2015, p. 2503–2511.
[44] s. parikh, d. dave, r. patel, and n. doshi, “security and privacy issues
in cloud, fog and edge computing,” procedia computer science, vol.
160, pp. 734–739, 2019.
[45] a. alwarafy, k. a. al-thelaya, m. abdallah, j. schneider, and
m. hamdi, “a survey on security and privacy issues in edge-computing-
assisted internet of things,” ieee internet of things journal, vol. 8,
no. 6, pp. 4004–4022, 2021.
[46] p. voigt and a. von dem bussche, “the eu general data protection regu-
lation (gdpr),” a practical guide, 1st ed., cham: springer international
publishing, 2017.
[47] q. yang, y. liu, t. chen, and y. tong, “federated machine learning:
concept and applications,” acm trans. intell. syst. technol., vol. 10,
no. 2, jan. 2019. [online]. available: https://doi.org/10.1145/3298981
view publication stats
view publication stats
"
machine-learning-in-production-from-experimented-ml-model-to-system (1).pdf;"
references 
1. https://www.jeremyjordan.me/evaluating-a-machine-learn-
ing-model/
2. https://www.analyticsvidhya.com/blog/2021/05/ma-
chine-learning-model-evaluation/
3. https://madewithml.com/courses/mlops/pipelines/
4. derakhshan, b., mahdiraji, a. r., rabl, t., & markl, v. 
(2019). continuous deployment of machine learning pipe-
lines. in edbt (pp. 397-408).
5. xin, d., miao, h., parameswaran, a., & polyzotis, n. (2021). 
production machine learning pipelines: empirical analysis 
and optimization opportunities. in proceedings of the 2021 
international conference on management of data (pp. 2639-
2652).
6. sarajlić, a., malod-dognin, n., yaveroğlu, ö. n., & pržulj, 
n. (2016). graphlet-based characterization of directed net-
works. scientific reports, 6(1), 1-14.
7. liu, x., chen, y. z. j., lui, j. c., & avrachenkov, k. (2020). 
learning to count: a deep learning framework for graphlet 
count estimation. network science, 9(s1), s23-s60.
8. https://www.mathworks.com/help/deeplearning/quantization.
html?searchhighlight=c&s_tid=doc_srchtitle
9. software engineering for machine learning: a case study 
(2019) amershi et al. (microsoft) 
10. a machine learning pipeline to optimally utilize limited 
samples in predictive modeling 
11. tensorflow-serving: flexible, high-performance ml serv-
ing 
12. jin, w. (2020). research on machine learning and its algo-
rithms and development. in journal of physics: conference 
series (vol. 1544, no. 1, p. 012003). iop publishing.
13. j. schoenberg. metric spaces and completely monotone func-
tions. the annals of mathematics, 39(4):811, 1938 
14. https://www.mathworks.com/help/deeplearning/ug/quantiza-
tion-of-deep-neural-networks.html
15. https://www.mathworks.com/help/deeplearning/ug/quantiza-
tion-workflow-prerequisites.html
16. https://www.mathworks.com/help/gpucoder/ug/code-genera-
tion-for-quantized-deep-learning-networks.html
"
Martin-Martin et al_2018_Google scholar web of science.pdf;"
references 
amara, n., & landry, r. (2012). counting citations in the field of business and management: 
why use google scholar rather than the web of science. scientometrics, 93(3), 553–581. 
https://doi.org/10.1007/s11192-012-0729-2 
bakkalbasi, n., bauer, k., glover, j., & wang, l. (2006). three options for citation tracking: 
google scholar, scopus and web of science. biomedical digital libraries, 3(1), 7. 
https://doi.org/10.1186/1742-5581-3-7 
bar-ilan, j. (2010). citations to the “introduction to informetrics” indexed by wos, scopus and 
google scholar. scientometrics, 82(3), 495–506. https://doi.org/10.1007/s11192-010-
0185-9 
chavarro, d., ràfols, i., & tang, p. (2018). to what extent is inclusion in the web of science an 
indicator of journal ‘quality’? research evaluation, 27(2), 106–118. 
https://doi.org/10.1093/reseval/rvy001 
clarivate analytics. (2015). web of science & google scholar collaboration. retrieved june 5, 
2018, from http://wokinfo.com/googlescholar/ 
clarivate analytics. (2017). emerging sources citation index backfile (2005-2014). retrieved 
from https://clarivate.com/wp-content/uploads/2017/10/m255-crv_sar_esci-infographic-
fa.pdf 
damerau, f. j. (1964). a technique for computer detection and correction of spelling errors. 
communications of the acm, 7(3), 171–176. https://doi.org/10.1145/363958.363994 
de groote, s. l., & raszewski, r. (2012). coverage of google scholar, scopus, and web of 
science: a case study of the h-index in nursing. nursing outlook, 60(6), 391–400. 
https://doi.org/10.1016/j.outlook.2012.04.007 
de solla price, d. (1976). a general theory of bibliometric and other cumulative advantage 
processes. journal of the american society for information science, 27(5), 292–306. 
https://doi.org/10.1002/asi.4630270505 
de winter, j. c. f., zadpoor, a. a., & dodou, d. (2014). the expansion of google scholar 
versus web of science: a longitudinal study. scientometrics, 98(2), 1547–1565. 
https://doi.org/10.1007/s11192-013-1089-2 
delgado lópez-cózar, e., orduna-malea, e., & martín-martín, a. (2018). google scholar as a 
data source for research assessment. in w. glaenzel, h. moed, u. schmoch, & m. 
thelwall (eds.), springer handbook of science and technology indicators. springer. 
delgado lópez-cózar, e., robinson-garcía, n., & torres-salinas, d. (2014). the google 
scholar experiment: how to index false papers and manipulate bibliometric indicators. 
22 
 
journal of the association for information science and technology, 65(3), 446–454. 
https://doi.org/10.1002/asi.23056 
dowle, m., srinivasan, a., gorecki, j., chirico, m., stetsenko, p., short, t., … parsonage, h. 
(2018). data.table: extension of “data.frame.” retrieved from https://cran.r-
project.org/package=data.table 
else, h. (2018, april 11). how i scraped data from google scholar. nature. 
https://doi.org/10.1038/d41586-018-04190-5 
elsevier. (2018). scopus source list (april 2018). retrieved from 
https://www.elsevier.com/__data/assets/excel_doc/0015/91122/ext_list_april_2018_2017_
metrics.xlsx 
halevi, g., moed, h., & bar-ilan, j. (2017). suitability of google scholar as a source of scientific 
information and as a source of data for scientific evaluation—review of the literature. 
journal of informetrics, 11(3), 823–834. https://doi.org/10.1016/j.joi.2017.06.005 
harzing, a.-w. (2013). a longitudinal study of google scholar coverage between 2012 and 
2013. scientometrics, 98(1), 565–575. https://doi.org/10.1007/s11192-013-0975-y 
harzing, a.-w., & alakangas, s. (2016). google scholar, scopus and the web of science: a 
longitudinal and cross-disciplinary comparison. scientometrics, 106(2), 787–804. 
https://doi.org/10.1007/s11192-015-1798-9 
harzing, a. w. (2007). publish or perish. retrieved from http://www.harzing.com/pop.htm 
harzing, a. w. k., & van der wal, r. (2008). google scholar as a new source for citation 
analysis. ethics in science and environmental politics, 8(1), 61–73. 
https://doi.org/10.3354/esep00076 
jacimovic, j., petrovic, r., & zivkovic, s. (2010). a citation analysis of serbian dental journal 
using web of science, scopus and google scholar. stomatoloski glasnik srbije, 57(4), 
201–211. https://doi.org/10.2298/sgs1004201j 
jacsó, p. (2010). metadata mega mess in google scholar. online information review, 34(1), 
175–191. https://doi.org/10.1108/14684521011024191 
kousha, k., & thelwall, m. (2007). google scholar citations and google web/url citations: a 
multi-discipline exploratory analysis. journal of the american society for information 
science and technology, 58(7), 1055–1065. https://doi.org/10.1002/asi.20584 
kousha, k., & thelwall, m. (2008). sources of google scholar citations outside the science 
citation index: a comparison between four science disciplines. scientometrics, 74(2), 
273–294. https://doi.org/10.1007/s11192-008-0217-x 
larsson, j., godfrey, a. j. r., kelley, t., eberly, d. h., gustafsson, p., & huber, e. (2018). 
eulerr: area-proportional euler and venn diagrams with circles or ellipses. retrieved from 
https://cran.r-project.org/package=eulerr 
lasda bergman, e. m. (2012). finding citations to social work literature: the relative benefits 
of using web of science, scopus, or google scholar. the journal of academic 
librarianship, 38(6), 370–379. https://doi.org/10.1016/j.acalib.2012.08.002 
levenshtein, v. i. (1966). binary codes capable of correcting deletions, insertions, and 
reversals. in soviet physics doklady (vol. 10, pp. 707–710). 
martín-martín, a., & delgado lópez-cózar, e. (2016). reading web of science data into r. 
retrieved from https://github.com/alberto-martin/read.wos.r 
23 
 
martín-martín, a., orduna-malea, e., & delgado lópez-cózar, e. (2018). coverage of highly-
cited documents in google scholar, web of science, and scopus: a multidisciplinary 
comparison. scientometrics, 116(3), 2175–2188. https://doi.org/10.1007/s11192-018-
2820-9 
meho, l. i., & yang, k. (2007). impact of data sources on citation counts and rankings of lis 
faculty: web of science versus scopus and google scholar. journal of the american 
society for information science and technology, 58(13), 2105–2125. 
https://doi.org/10.1002/asi.20677 
minasny, b., hartemink, a. e., mcbratney, a., & jang, h.-j. (2013). citations and the h index of 
soil researchers and journals in the web of science, scopus, and google scholar. peerj, 
1, e183. https://doi.org/10.7717/peerj.183 
mingers, j., & lipitakis, e. a. e. c. g. (2010). counting the citations: a comparison of web of 
science and google scholar in the field of business and management. scientometrics, 
85(2), 613–625. https://doi.org/10.1007/s11192-010-0270-0 
moed, h. f., bar-ilan, j., & halevi, g. (2016). a new methodology for comparing google 
scholar and scopus. journal of informetrics, 10(2), 533–551. 
https://doi.org/10.1016/j.joi.2016.04.017 
mongeon, p., & paul-hus, a. (2016). the journal coverage of web of science and scopus: a 
comparative analysis. scientometrics, 106(1), 213–228. https://doi.org/10.1007/s11192-
015-1765-5 
ooms, j., & sites, d. (2018). cld2: google’s compact language detector 2. retrieved from 
https://cran.r-project.org/package=cld2 
orduna-malea, e., ayllón, j. m., martín-martín, a., & delgado lópez-cózar, e. (2015). methods 
for estimating the size of google scholar. scientometrics, 104(3), 931–949. 
https://doi.org/10.1007/s11192-015-1614-6 
orduña-malea, e., martín-martín, a., ayllón, j. m., & delgado lópez-cózar, e. (2016). la 
revolución google scholar : destapando la caja de pandora académica. granada: 
universidad de granada. 
orduna-malea, e., martín-martín, a., & delgado lópez-cózar, e. (2018). classic papers: using 
google scholar to detect the highly-cited documents. in 23rd international conference on 
science and technology indicators. leiden. https://doi.org/10.31235/osf.io/zkh7p 
pauly, d., & stergiou, k. (2005). equivalence of results from two citation analyses: thomson 
isi’s citation index and google’s scholar service. ethics in science and environmental 
politics, 9, 33–35. https://doi.org/10.3354/esep005033 
prins, a. a. m., costas, r., van leeuwen, t. n., & wouters, p. f. (2016). using google scholar 
in research evaluation of humanities and social science programs: a comparison with web 
of science data. research evaluation, 25(3), 264–270. 
https://doi.org/10.1093/reseval/rvv049 
r core team. (2014). r: a language and environment for statistical computing. vienna, 
austria. retrieved from http://www.r-project.org/ 
rahimi, s., & chandrakumar, v. (2014). a comparison of citation coverage of traditional and 
web citation databases in medical science. malaysian journal of library and information 
science, 19(3), 1–11. retrieved from 
http://jice.um.edu.my/index.php/mjlis/article/view/1779 
sud, p., & thelwall, m. (2014). evaluating altmetrics. scientometrics, 98(2), 1131–1143. 
https://doi.org/10.1007/s11192-013-1117-2 
24 
 
tijssen, r., nederhof, a., van leeuwen, t., hollanders, h., kanerva, m., & van den berg, p. 
(2010). wetenschaps- en technologie- indicatoren 2010. retrieved from 
http://nowt.merit.unu.edu/docs/nowt-wti_2010.pdf 
van der loo, m., van der laan, j., r core team, logan, n., & muir, c. (2018). stringdist: 
approximate string matching and string distance functions. retrieved from https://cran.r-
project.org/package=stringdist 
van leeuwen, t. n., moed, h. f., tijssen, r. j. w., visser, m. s., & van raan, a. f. j. (2001). 
language biases in the coverage of the science citation index and its consequences for 
international comparisons of national research performance. scientometrics, 51(1), 335–
346. https://doi.org/10.1023/a:1010549719484 
van noorden, r. (2014, november 7). google scholar pioneer on search engine’s future. 
nature. https://doi.org/10.1038/nature.2014.16269 
walker, a., & braglia, l. (2018). openxlsx: read, write and edit xlsx files. retrieved from 
https://cran.r-project.org/package=openxlsx 
wickham, h. (2016). ggplot2: elegant graphics for data analysis. springer-verlag new york. 
retrieved from http://ggplot2.org 
wildgaard, l. (2015). a comparison of 17 author-level bibliometric indicators for researchers in 
astronomy, environmental science, philosophy and public health in web of science and 
google scholar. scientometrics, 104(3), 873–906. https://doi.org/10.1007/s11192-015-
1608-4 
yang, k., & meho, l. i. (2007). citation analysis: a comparison of google scholar, scopus, and 
web of science. proceedings of the american society for information science and 
technology, 43(1), 1–15. https://doi.org/10.1002/meet.14504301185 
 
"
master_Stening_Joel_2022.pdf;"
references
[1] t. zonta, c. a. da costa, r. da rosa righi, m. j. de lima, e. s. da trindade,
and g. p.
li, “predictive maintenance in the industry 4.0: a systematic
literature review,” computers industrial engineering, vol. 150, p. 106889, 2020.
[2] t. p. carvalho, f. a. a. m. n. soares, r. vita, r. da p. francisco, j. p.
basto, and s. g. s. alcalá, “a systematic literature review of machine learning
methods applied to predictive maintenance,” computers industrial engineering,
vol. 137, p. 106024, 2019.
[3] i. a. t. hashem, i. yaqoob, n. b. anuar, s. mokhtar, a. gani, and s. ullah
khan, “the rise of “big data” on cloud computing: review and open research
issues,” information systems, vol. 47, pp. 98–115, 2015.
[4] “data science
big data analytics : discovering, analyzing, visualizing and
presenting data,” 2015 - 2015.
[5] ishwarappa and j. anuradha, “a brief introduction on big data 5vs characteris-
tics and hadoop technology,” procedia computer science, vol. 48, pp. 319–324,
2015. international conference on computer, communication and convergence
(iccc 2015).
[6] l. r. nair, s. d. shetty, and s. d. shetty, “applying spark based machine
learning model on streaming big data for health status prediction,” computers
electrical engineering, vol. 65, pp. 393–399, 2018.
[7] l. igual, introduction to data science a python approach to concepts, tech-
niques and applications. undergraduate topics in computer science, cham:
springer international publishing, 1st ed. 2017. ed., 2017.
[8] e. alpaydin, introduction to machine learning.
adaptive computation and
machine learning series, cambridge, massachusetts: mit press, third edition. ed.,
2014.
[9] s. alla, beginning mlops with mlflow : deploy models in aws sagemaker,
google cloud, and microsoft azure. berkeley, california: apress, 1st ed. 2021. ed.,
2021.
[10] p. kadlec, b. gabrys, and s. strandt, “data-driven soft sensors in the process
industry,” computers & chemical engineering, vol. 33, no. 4, pp. 795–814, 2009.
[11] s. g. soares, ensemble learning methodologies for soft sensor development in
industrial processes. phd thesis, 2015.
[12] m. a. hall et al., “correlation-based feature selection for machine learning,”
1999.
[13] a. muller, “introduction to machine learning with python,” 2016.
47
[14] b. lin, b. recke, t. m. schmidt, j. k. knudsen, and s. b. jørgensen, “data-
driven soft sensor design with multiple-rate sampled data: a comparative study,”
industrial & engineering chemistry research, vol. 48, no. 11, pp. 5379–5387,
2009.
[15] a. k. jardine, d. lin, and d. banjevic, “a review on machinery diagnostics and
prognostics implementing condition-based maintenance,” mechanical systems
and signal processing, vol. 20, no. 7, pp. 1483–1510, 2006.
[16] s. weidman, “deep learning from scratch : building with python from first
principles,” 2019.
[17] g. ciaburro, “hands-on machine learning on google cloud platform,” 2018.
[18] s. bai, j. z.
kolter, and v. koltun, “an empirical evaluation of generic
convolutional and recurrent networks for sequence modeling,” arxiv preprint
arxiv:1803.01271, 2018.
[19] c. pelletier, g. i. webb, and f. petitjean, “temporal convolutional neural
network for the classification of satellite image time series,” remote sensing,
vol. 11, no. 5, p. 523, 2019.
[20] j. liu, h. zhu, y. liu, h. wu, y. lan, and x. zhang, “anomaly detection for
time series using temporal convolutional networks and gaussian mixture model,”
in journal of physics: conference series, vol. 1187, p. 042111, iop publishing,
2019.
[21] j. an and s. cho, “variational autoencoder based anomaly detection using
reconstruction probability,” special lecture on ie, vol. 2, no. 1, pp. 1–18, 2015.
[22] c. zhou and r. c. paffenroth, “anomaly detection with robust deep autoen-
coders,” in proceedings of the 23rd acm sigkdd international conference on
knowledge discovery and data mining, pp. 665–674, 2017.
[23] g. bonaccorso, hands-on unsupervised learning with python : implement ma-
chine learning and deep learning models using scikit-learn, tensorflow, and more.
birmingham ;: packt publishing ltd, 1st edition ed., 2019.
[24] l. breiman, “bagging predictors,” machine learning, vol. 24, no. 2, pp. 123–140,
1996.
[25] l. prechelt, “early stopping-but when?,” in neural networks: tricks of the
trade, pp. 55–69, springer, 1998.
[26] y. lan, y. c. soh, and g.-b. huang, “ensemble of online sequential extreme
learning machine,” neurocomputing, vol. 72, no. 13, pp. 3391–3395, 2009. hybrid
learning machines (hais 2007) / recent developments in natural computation
(icnc 2007).
48
[27] h. hapke, “building machine learning pipelines : automating model life cycles
with tensorflow,” 2020.
[28] m. treveil, introducing mlops : how to scale machine learning in the enterprise.
beijing: o’reilly, 1st edition ed., 2021.
[29] o. e. gundersen, s. shamsaliei, and r. j. isdahl, “do machine learning
platforms provide out-of-the-box reproducibility?,” future generation computer
systems, vol. 126, pp. 34–47, 2022.
[30] k. m. lee, j. yoo, s.-w. kim, j.-h. lee, and j. hong, “autonomic machine
learning platform,” international journal of information management, vol. 49,
pp. 491–501, 2019.
[31] o. tomarchio, d. calcaterra, and g. d. modica, “cloud resource orchestration
in the multi-cloud landscape: a systematic review of existing frameworks,” j.
cloud comput., vol. 9, p. 49, 2020.
[32] m.-g. avram, “advantages and challenges of adopting cloud computing from
an enterprise perspective,” procedia technology, vol. 12, pp. 529–534, 2014.
[33] “water-cooled
oil-free
magnetic
bearing
chillers
by
arctic.”
https:
//www.trane.com/commercial/north-america/us/en/products-systems/
chillers/water-cooled-chillers/oil-free-magnetic-bearing.html
accessed on 06/01/2022.
[34] hvac fundamentals (3rd edition). fairmont press, inc., 2016.
[35] “operations guide tracer adaptiview™ display for gear-driven water-cooled cen-
travac™ (cvgf) chillers.” https://www.trane.com/webcache/ctv-svu02a-en_
03012009.pdf accessed on 03/03/2022.
[36] “tensorflow
extended.”
https://www.tensorflow.org/tfx
accessed
on
05/01/2022.
[37] “apache airflow.” https://airflow.apache.org/ accessed on 05/01/2022.
[38] “kubeflow documentation.” https://www.kubeflow.org/docs/ accessed on
03/03/2022.
[39] j. de ruiter, “data pipelines with apache airflow.,” 2021.
[40] “flyte
launch
blogpost.”
https://eng.lyft.com/
introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59
accessed on 05/01/2022.
[41] “flyte.” https://flyte.org/ accessed on 05/01/2022.
[42] m. butcher, learning helm : managing apps on kubernetes.
sebastopol,
california: o’reilly media, incorporated, 1st edition ed., 2021.
49
[43] “vertex
launch
post.”
https://cloud.google.com/blog/products/
ai-machine-learning/google-cloud-launches-vertex-ai-unified-platform-for-mlops
accessed on 03/03/2022.
[44] “google cloud platform, vertex ai.” https://cloud.google.com/vertex-ai
accessed on 05/01/2022.
[45] “sagemaker
launch
post.”
https://techcrunch.com/2017/11/29/
aws-releases-sagemaker-to-make-it-easier-to-build-and-deploy-machine-learning-models/
?guccounter=1&guce_referrer=ahr0chm6ly9lbi53awtpcgvkaweub3jnlw&
guce_referrer_sig=aqaaabvurkwaofgloe-ur9rnagrle3mne_
knvcx9wbtnajynwr9rjcw4o0xg_zndy3eqaiqjnqxhfoog2k5ujn-monnmcnzbahl9o9ad5amtywiqothwwn-rr05pnrwms97tk0sbvvmglflwqnrfyxvq4aso-63wkoymhndubdhg8vrq
accessed on 03/03/2022.
[46] “amazon
web services,
amazon
sagemaker.” https://aws.amazon.com/
sagemaker/ accessed on 05/01/2022.
[47] “azure
machine
learning
platform
launch
post.”
https://techcrunch.com/2015/02/18/
microsoft-officially-launches-azure-machine-learning-big-data-platform/
accessed on 03/03/2022.
[48] “microsoft azure, machine learning.” https://azure.microsoft.com/en-us/
services/machine-learning/ accessed on 05/01/2022.
[49] “aws sagemaker, introduction to feature store.” https://docs.aws.amazon.
com/sagemaker/latest/dg/feature-store-introduction-notebook.html
accessed on 03/03/2022.
[50] r. a. mccain, value solutions in cooperative games. hackensack, n.j: world
scientific pub., 2013.
[51] c. molnar, interpretable machine learning. lulu.com, 2020.
[52] s. kandel, j. heer, c. plaisant, j. kennedy, f. van ham, n. h. riche, c. weaver,
b. lee, d. brodbeck, and p. buono, “research directions in data wrangling:
visualizations and transformations for usable and credible data,” information
visualization, vol. 10, no. 4, pp. 271–288, 2011.
[53] “aws sagemaker, introduction to amazon data wrangler.” https://docs.
aws.amazon.com/sagemaker/latest/dg/data-wrangler-getting-started.
html accessed on 03/03/2022.
50
a
evaluate.py
1 import
json
2 import
pathlib
3 import
tarfile
4 import
logging
5 import os
6 import
codecs
7
8 import
numpy as np
9 import
tensorflow
10
11 logger = logging.getlogger ()
12 logger.setlevel(logging.info)
13 logger.addhandler(logging.streamhandler ())
14
15 def
evaluate_score (model ,train_data ,anomaly_signal_test ,
anomaly_data_test ):
16
#get
treshold
based on healthy
data
17
train_prediction =model.predict(train_data)
18
train_mae = np.mean(np.abs( train_prediction - train_data), axis
=1)
19
train_thsld = np.max(train_mae)
20
21
#get anomaly
test data and
anomaly
test
signal
22
signal_mean = np.mean(np.abs( anomaly_signal_test ), axis =1)
23
anomalies_bools = signal_mean != 0
24
25
#predict
anomalies in test data
26
test_prediction =model.predict( anomaly_data_test )
27
test_mae = np.mean(np.abs( test_prediction - anomaly_data_test ),
axis =1)
28
29
detected_anomalies_bools = test_mae > train_thsld
30
31
anomalies_detected = 0
32
33
for idx , x in np.ndenumerate( detected_anomalies_bools ):
34
for idy , y in np.ndenumerate(x):
35
if
detected_anomalies_bools [idx][idy] ==
anomalies_bools [idx][idy]:
36
if
detected_anomalies_bools [idx][idy] == true:
37
anomalies_detected
+= 1
38
39
#how many
anomalies
found in anomalous
test data and how many
anomalies in signal
40
anomalies_in_data = np.count_nonzero( anomalies_bools )
41
42
#score
43
try:
44
score = anomalies_detected / anomalies_in_data
45
except:
46
score = 1
47
51
48
return
score
49
50 if __name__ == ""__main__"":
51
logger.debug(""starting
evaluation."")
52
#fetch
model
53
model_path = ""/opt/ml/processing/model/model.tar.gz""
54
55
with
tarfile.open(model_path) as tar:
56
tar.extractall(path=""."")
57
58
logger.debug(""current
dir
content: "")
59
#tensorflow
model
loading
60
model = tensorflow.keras.models.load_model(’./1/ ’)
61
62
#fetch
test data
63
test_data_path = ""/opt/ml/processing/test/ test_data_anomalies .
json""
64
obj_text = codecs.open(test_data_path , ’r’, encoding=’utf -8’).
read ()
65
b_new = json.loads(obj_text)
66
test_data = np.array(b_new)
67
68
train_data_path = ""/opt/ml/processing/train/train_data.json""
69
obj_text = codecs.open(train_data_path , ’r’, encoding=’utf -8’).
read ()
70
b_new = json.loads(obj_text)
71
train_data = np.array(b_new)
72
73
signal_data_path = ""/opt/ml/processing/signal/
test_signal_anomalies .json""
74
obj_text = codecs.open(signal_data_path , ’r’, encoding=’utf -8’)
.read ()
75
b_new = json.loads(obj_text)
76
test_anomaly_signal = np.array(b_new)
77
78
#score
79
score = evaluate_score(model ,train_data ,test_anomaly_signal ,
test_data)
80
81
report_dict = {
82
""metrics"": {
83
""score"": {
84
""value"": score
85
},
86
},
87
}
88
89
output_dir = ""/opt/ml/processing/evaluation""
90
pathlib.path(output_dir).mkdir(parents=true , exist_ok=true)
91
evaluation_path = f""{output_dir }/ evaluation.json""
92
with open(evaluation_path , ""w"") as f:
93
f.write(json.dumps(report_dict))
52
b
pipeline.py
1 import os
2
3 import
boto3
4 import
sagemaker
5 import
sagemaker.session
6
7 from
sagemaker
import
model
8
9 from
sagemaker.estimator
import
estimator
10 from
sagemaker.inputs
import
traininginput
11 from
sagemaker.model_metrics
import (
12
metricssource ,
13
modelmetrics ,
14 )
15 from
sagemaker.tensorflow
import
tensorflow
16 from
sagemaker.processing
import (
17
processinginput ,
18
processingoutput ,
19
scriptprocessor ,
20 )
21 from
sagemaker.sklearn.processing
import
sklearnprocessor
22 from
sagemaker.workflow.conditions
import
conditiongreaterthanorequalto
23 from
sagemaker.workflow.condition_step
import (
24
conditionstep ,
25
jsonget ,
26 )
27 from
sagemaker.workflow.parameters
import (
28
parameterinteger ,
29
parameterstring ,
30 )
31 from
sagemaker.workflow.pipeline
import
pipeline
32 from
sagemaker.workflow.properties
import
propertyfile
33 from
sagemaker.workflow.steps
import (
34
processingstep ,
35
trainingstep ,
36 )
37 from
sagemaker.workflow. step_collections
import
registermodel
38
39
40 base_dir = os.path.dirname(os.path.realpath(__file__))
41
42 def
get_sagemaker_client (region):
43
"""""" gets the
sagemaker
client.
44
45
args:
46
region: the aws region to start the
session
47
default_bucket : the bucket to use for
storing
the
artifacts
48
49
returns:
50
‘sagemaker.session.session
instance
53
51
""""""
52
boto_session = boto3.session(region_name=region)
53
sagemaker_client = boto_session.client(""sagemaker"")
54
return
sagemaker_client
55
56
57 def
get_session(region , default_bucket):
58
"""""" gets the
sagemaker
session
based on the region.
59
60
args:
61
region: the aws region to start the
session
62
default_bucket : the bucket to use for
storing
the
artifacts
63
64
returns:
65
‘sagemaker.session.session
instance
66
""""""
67
68
boto_session = boto3.session(region_name=region)
69
70
sagemaker_client = boto_session.client(""sagemaker"")
71
runtime_client = boto_session.client(""sagemaker -runtime"")
72
return
sagemaker.session.session(
73
boto_session=boto_session ,
74
sagemaker_client =sagemaker_client ,
75
sagemaker_runtime_client =runtime_client ,
76
default_bucket =default_bucket ,
77
)
78
79 def
get_pipeline_custom_tags (new_tags , region ,
sagemaker_project_arn =none):
80
try:
81
sm_client = get_sagemaker_client (region)
82
response = sm_client.list_tags(
83
resourcearn= sagemaker_project_arn )
84
project_tags = response[""tags""]
85
for
project_tag in project_tags:
86
new_tags.append(project_tag)
87
except
exception as e:
88
print(f""error
getting
project
tags: {e}"")
89
return
new_tags
90
91
92 def
get_pipeline(
93
region ,
94
sagemaker_project_arn =none ,
95
role=none ,
96
default_bucket =none ,
97
model_package_group_name ="" autoencodergroup "",
98
pipeline_name="" trainpipefeaturestore "",
99
base_job_prefix ="" trainpipefeaturestore "",
100 ):
101
""""""
102
args:
103
region: aws region to create and run the
pipeline.
54
104
role: iam role to create and run steps and
pipeline.
105
default_bucket : the bucket to use for
storing
the
artifacts
106
107
returns:
108
an instance of a pipeline
109
""""""
110
sagemaker_session = get_session(region , default_bucket )
111
if role is none:
112
role = sagemaker.session. get_execution_role (
sagemaker_session )
113
114
# parameters
for
pipeline
execution
115
processing_instance_count = parameterinteger (
116
name="" processinginstancecount "",
117
default_value =1
118
)
119
processing_instance_type = parameterstring (
120
name="" processinginstancetype "",
121
default_value=""ml.m5.xlarge""
122
)
123
training_instance_type = parameterstring (
124
name="" traininginstancetype "",
125
default_value=""ml.m5.xlarge""
126
)
127
model_approval_status = parameterstring (
128
name="" modelapprovalstatus "",
129
default_value="" pendingmanualapproval ""
130
)
131
132
####
preprocessing
step ####
133
sklearn_processor = sklearnprocessor (
134
framework_version =""0.23 -1"",
135
instance_type=processing_instance_type ,
136
instance_count =processing_instance_count ,
137
base_job_name=""sklearn -anomaly -preprocess"",
138
sagemaker_session =sagemaker_session ,
139
role=role ,
140
)
141
step_process = processingstep (
142
name="" preprocessanomalydata "",
143
processor=sklearn_processor ,
144
outputs =[ processingoutput (output_name=’train_data ’,
145
source=’/opt/ml/processing/train ’),
146
processingoutput (output_name=’test_data ’,
147
source=’/opt/ml/processing/test ’),
148
processingoutput (output_name=’train_data_anom ’,
149
source=’/opt/ml/processing/train_anom
’),
150
processingoutput (output_name=’test_data_anom ’,
151
source=’/opt/ml/processing/test_anom ’
),
152
processingoutput (output_name=’train_signal_anom ’,
153
source=’/opt/ml/processing/
signal_train_anom ’),
55
154
processingoutput (output_name=’test_signal_anom ’,
155
source=’/opt/ml/processing/
signal_test_anom ’)],
156
code=os.path.join(base_dir , ""preprocess.py"")
157
)
158
159
####
training
step ####
160
model_path = f""s3 ://{ sagemaker_session . default_bucket ()}/{
base_job_prefix }/ anomaly -train""
161
162
image_uri = sagemaker.image_uris.retrieve(
163
framework=""tensorflow"",
164
region=region ,
165
version=""2.3"",
166
py_version=""py37"",
167
instance_type=training_instance_type ,
168
image_scope=’training ’
169
)
170
171
tf_estimator = tensorflow(
172
train_instance_count =1,
173
output_path=model_path ,
174
entry_point=os.path.join(base_dir , ""train.py""),
175
sagemaker_session =sagemaker_session ,
176
role=role ,
177
train_instance_type =training_instance_type ,
178
framework_version =’2.3’,
179
py_version=""py37"",
180
)
181
182
step_train = trainingstep(
183
name="" trainanomalymodel "",
184
estimator=tf_estimator ,
185
inputs ={
186
""train"": traininginput(
187
s3_data=step_process.properties.
processingoutputconfig .outputs[
188
""train_data""
189
]. s3output.s3uri ,
190
content_type=""text/json""
191
)
192
},
193
)
194
195
####
processing
step for
evaluation
####
196
script_eval = scriptprocessor (
197
image_uri=image_uri ,
198
command =[""python3""],
199
instance_type=processing_instance_type ,
200
instance_count =1,
201
role=role ,
202
)
203
204
evaluation_report = propertyfile(
56
205
name="" evaluationreport "",
206
output_name=""evaluation"",
207
path=""evaluation.json""
208
)
209
210
step_eval = processingstep(
211
name=""anomalyeval"",
212
processor=script_eval ,
213
inputs =[
214
processinginput (
215
source=step_train.properties.modelartifacts .
s3modelartifacts ,
216
destination=""/opt/ml/processing/model""
217
),
218
processinginput (
219
source=step_process.properties.
processingoutputconfig .outputs[
220
"" test_data_anom""
221
]. s3output.s3uri ,
222
destination=""/opt/ml/processing/test""
223
),
224
processinginput (
225
source=step_process.properties.
processingoutputconfig .outputs[
226
"" test_signal_anom ""
227
]. s3output.s3uri ,
228
destination=""/opt/ml/processing/signal""
229
),
230
processinginput (
231
source=step_process.properties.
processingoutputconfig .outputs[
232
""train_data""
233
]. s3output.s3uri ,
234
destination=""/opt/ml/processing/train""
235
)
236
],
237
outputs =[
238
processingoutput (output_name=""evaluation"", source=""/opt
/ml/processing/evaluation""),
239
],
240
code=os.path.join(base_dir , ""evaluate.py""),
241
property_files =[ evaluation_report ],
242
)
243
244
####
register
model
step that is conditionally
executed
####
245
model = model(image_uri=image_uri , sagemaker_session =
sagemaker_session ,
246
role=role ,model_data=step_train.properties.modelartifacts .
s3modelartifacts ,name=’cool ’)
247
248
model_metrics = modelmetrics(
249
model_statistics =metricssource(
250
s3_uri=""{}/ evaluation.json"".format(
251
step_eval.arguments["" processingoutputconfig ""][""
57
outputs""][0][""s3output""][""s3uri""]
252
),
253
content_type=""application/json""
254
)
255
)
256
step_register = registermodel(
257
name="" registeranomalymodel "",
258
model=model ,
259
content_types =[""text/json""],
260
response_types =[""text/json""],
261
inference_instances =[""ml.t2.medium"", ""ml.m5.large""],
262
transform_instances =[""ml.m5.large""],
263
model_package_group_name =model_package_group_name ,
264
approval_status =model_approval_status ,
265
model_metrics=model_metrics ,
266
)
267
268
####
condition
step for
evaluating
model
quality
and
branching
execution
####
269
cond_lte = conditiongreaterthanorequalto (
270
left=jsonget(
271
step=step_eval ,
272
property_file=evaluation_report ,
273
json_path=""metrics.score.value""
274
),
275
right =0.3,
276
)
277
step_cond = conditionstep(
278
name="" checkanomalyevaluation "",
279
conditions =[ cond_lte],
280
if_steps =[ step_register],
281
else_steps =[],
282
)
283
284
####
pipeline
instance
####
285
pipeline = pipeline(
286
name=pipeline_name ,
287
parameters =[
288
processing_instance_type ,
289
processing_instance_count ,
290
training_instance_type ,
291
model_approval_status
292
],
293
steps =[ step_process , step_train , step_eval , step_cond],
294
sagemaker_session =sagemaker_session ,
295
)
296
return
pipeline
58
c
preprocess.py
1 import
argparse
2 import os
3 import
pathlib
4 import
warnings
5 import
json
6 import
codecs
7
8 import
boto3
9 from
botocore.config
import
config
10 import
pandas as pd
11 import
numpy as np
12 import
random
13
14 from
datetime
import
datetime
15
16 def
anomalygen(anomaly_count ,anomaly_length ,anomaly_value ,lookback ,
n_features , preprocessed_data_raw ):
17
# create
random
anomaly
sequences
18
l_anomaly_start_idxs =[ random.randint (0,len(
preprocessed_data_raw )-anomaly_length *2) for _ in range(
anomaly_count)]
19
20
# sort list
21
l_anomaly_start_idxs .sort ()
22
23
# create
matching
list of zeros
24
anomaly_signal_full =np.zeros( preprocessed_data_raw .shape)
25
26
#populate
list of zeros
with a combination of original
values and random
noise
27
for idx in l_anomaly_start_idxs :
28
anomaly_start_idx =idx
29
anomaly_end_idx = anomaly_start_idx + anomaly_length
30
31
anomaly_train_signal = np.random.randn(anomaly_end_idx -
anomaly_start_idx ,lookback ,n_features)*anomaly_value
32
np.add.at(preprocessed_data_raw ,np.arange(
anomaly_start_idx , anomaly_end_idx ), anomaly_train_signal )
33
np.add.at(anomaly_signal_full ,np.arange(
anomaly_start_idx , anomaly_end_idx ), anomaly_train_signal )
34
35
x_train_anom = preprocessed_data_raw [0: int(len(
preprocessed_data_raw )*0.8)] #80%
36
x_test_anom = preprocessed_data_raw [int(len(
preprocessed_data_raw )*0.8) :] #20%
37
38
anomaly_signal_train = anomaly_signal_full [0: int(len(
anomaly_signal_full )*0.8)] #80%
39
anomaly_signal_test = anomaly_signal_full [int(len(
anomaly_signal_full )*0.8) :] #20%
40
41
return
x_train_anom , x_test_anom , anomaly_signal_train ,
59
anomaly_signal_test
42
43 def
float_to_date(float_input):
44
return
datetime.fromtimestamp(float_input).strftime(’%y-%m
-%dt%h:%m:%sz’)
45
46 if __name__ ==’__main__ ’:
47
#fetch
feature
store
contents
with
athena
48
my_config = config(
49
region_name = ’eu -north -1’,
50
signature_version = ’v4’,
51
retries = {
52
’max_attempts ’: 10,
53
’mode ’: ’standard ’
54
}
55
)
56
57
pathlib.path(f""/opt/ml/processing/data"").mkdir(parents=true ,
exist_ok=true)
58
client = boto3.client(’athena ’, config=my_config)
59
prefix = ’preprocessing -’ + datetime.now().strftime(’%y-%m-%d:%
h-%m’)
60
result_location = ’s3:// sagemaker -eu -north -1 -683608778688/ ’ +
prefix
61
62
querystart = client. start_query_execution (
63
querystring = ’select * from ""chiller -feature -group
-1643633036""
limit
1000 ’,
64
queryexecutioncontext = {
65
’database ’: ’sagemaker_featurestore ’
66
},
67
resultconfiguration = { ’outputlocation ’: result_location }
68
)
69
70
state = ’running ’
71
72
while (state in [’running ’, ’queued ’]):
73
response = client. get_query_execution ( queryexecutionid =
querystart[’queryexecutionid ’])
74
75
if ’queryexecution ’ in response
and \
76
’status ’ in response[’queryexecution ’] and \
77
’state ’ in response[’queryexecution ’][’status ’]:
78
state = response[’queryexecution ’][’status ’][’state ’]
79
80
if state == ’failed ’:
81
time.sleep (1)
82
83
elif
state == ’succeeded ’:
84
#csv to container
85
s3 = boto3.resource(""s3"")
86
path_to_athena_data = prefix + ’/’ + querystart[’
queryexecutionid ’] + ’.csv’
87
s3.bucket(’sagemaker -eu -north -1 -683608778688 ’).
60
download_file(path_to_athena_data , ’/opt/ml/processing/data/data
.csv’)
88
89
#read to df
90
df = pd.read_csv(’/opt/ml/processing/data/data.csv’,low_memory=
false)
91
df[’property_timestamp ’] = df[’property_timestamp ’]. transform(
float_to_date)
92
93
#set
timestamps to index and with
right
type
94
df[’property_timestamp ’] = pd.to_datetime(df[’
property_timestamp ’])
95
df = df.set_index(’property_timestamp ’)
96
97
#features
selected
98
used_cols =[’avglinecurrentrlackt1 ’,’
compr1aavglinecurrentrla_rslt ’, ’unitpowerconsump ’, ’
compr1aline1currentrla ’,
99
’compr1aline2currentrla ’,’compr1aline3currentrla ’,’
current_draw_denom ’, ’current_draw_blanks_denom ’]
100
101
#dataframe
manipulations
102
df = df[used_cols]
103
df = df.fillna (0)
104
df = (df -df.min())/(df.max()-df.min())
105
df = df.resample(’1d’).mean ()
106
df = df.fillna (0)
107
df = df.to_numpy ()
108
109
#static
parameters
110
lookback = 14
111
n_features = len(df [0])
112
anomaly_count = 10
113
anomaly_length = 10
114
anomaly_value = 0.06
115
116
#apply
lookback
117
output = []
118
for i in range(len(df) - lookback + 1):
119
output.append(df[i : (i + lookback)])
120
output = np.stack(output)
121
122
preprocessed_data = output
123
124
#split
data
125
x_train = np.copy( preprocessed_data [0: int(len( preprocessed_data
)*0.8) ]) #80%
126
x_test = np.copy( preprocessed_data [int(len( preprocessed_data )
*0.8) :]) #20%
127
128
x_train_anom , x_test_anom , anomaly_signal_train ,
anomaly_signal_test = anomalygen(anomaly_count ,
129
anomaly_length ,
61
130
anomaly_value ,
131
lookback ,
132
n_features ,
133
preprocessed_data )
134
135
#save the
preprocessed
datas
136
train_features_output_path = os.path.join(’/opt/ml/processing/
train ’, ’train_data.json ’)
137
test_features_output_path = os.path.join(’/opt/ml/processing/
test ’, ’test_data.json ’)
138
train_features_anomalies_output_path = os.path.join(’/opt/ml/
processing/train_anom ’, ’train_data_anomalies .json ’)
139
test_features_anomalies_output_path = os.path.join(’/opt/ml/
processing/test_anom ’, ’test_data_anomalies .json ’)
140
train_signal_anomalies_output_path = os.path.join(’/opt/ml/
processing/ signal_train_anom ’, ’train_signal_anomalies .json ’)
141
test_signal_anomalies_output_path = os.path.join(’/opt/ml/
processing/ signal_test_anom ’, ’test_signal_anomalies .json ’)
142
143
json.dump(x_train.tolist (), codecs.open(
train_features_output_path , ’w’, encoding=’utf -8’), separators =(
’,’,’:’), sort_keys=true , indent =4)
144
json.dump(x_test.tolist (), codecs.open(
test_features_output_path , ’w’, encoding=’utf -8’), separators =(’
,’,’:’), sort_keys=true , indent =4)
145
json.dump(x_train_anom.tolist (), codecs.open(
train_features_anomalies_output_path , ’w’, encoding=’utf -8’),
separators =(’,’,’:’), sort_keys=true , indent =4)
146
json.dump(x_test_anom.tolist (), codecs.open(
test_features_anomalies_output_path , ’w’, encoding=’utf -8’),
separators =(’,’,’:’), sort_keys=true , indent =4)
147
json.dump( anomaly_signal_train .tolist (), codecs.open(
train_signal_anomalies_output_path , ’w’, encoding=’utf -8’),
separators =(’,’,’:’), sort_keys=true , indent =4)
148
json.dump( anomaly_signal_test .tolist (), codecs.open(
test_signal_anomalies_output_path , ’w’, encoding=’utf -8’),
separators =(’,’,’:’), sort_keys=true , indent =4)
62
d
train.py
1 import
argparse
2 import os
3 import
codecs
4 import
json
5 import
numpy as np
6 import
tensorflow
7 from
tensorflow.keras
import layers , models
8
9 if __name__ ==’__main__ ’:
10
parser = argparse.argumentparser ()
11
12
# hyperparameters
sent by the client are passed as command -line
arguments to the script.
13
parser.add_argument(’--epochs ’, type=int , default =350)
14
parser.add_argument(’--batch_size ’, type=int , default =256)
15
parser.add_argument(’--lookback ’, type=float , default =14)
16
parser.add_argument(’--n_features ’, type=float , default =8)
17
18
# input
data and model
directories
19
parser.add_argument(’--model_dir ’, type=str)
20
parser.add_argument(’--train ’, type=str , default=os.environ.get
(’sm_channel_train ’))
21
parser.add_argument(’--test ’, type=str , default=os.environ.get(
’sm_channel_test ’))
22
parser.add_argument(’--sm_model_dir ’, type=str , default=os.
environ.get(’sm_model_dir ’))
23
24
args , _ = parser. parse_known_args ()
25
26
# ... load from args.train and args.test , train a model , write
model to args.model_dir.
27
inp=tensorflow.keras.input(shape =( args.lookback , args.
n_features))
28
29
# encoder
30
x=layers.conv1d(filters =32, kernel_size =5, strides =1,
activation=""relu"", padding=’same ’)(inp)
31
x=layers.dropout (0.2)(x)
32
x=layers.conv1d(filters =64, kernel_size =5, strides =1,
activation=""relu"", padding=’same ’)(x)
33
x=layers.dropout (0.2)(x)
34
x=layers.conv1d(filters =128,
kernel_size =5, strides =1,
activation=""relu"", padding=’same ’)(x)
35
x=layers.dropout (0.2)(x)
36
37
# encoded
representation
38
encoded_shape=x.shape
39
x=layers.flatten ()(x)
40
flattened_shape =x.shape
41
x=layers.dense( flattened_shape [-1])(x)
42
43
# reshaping
63
44
x=layers.reshape (( encoded_shape [1:]))(x)
45
46
# decoder
47
x = layers. conv1dtranspose (64, 5,strides =1, activation=""relu"",
padding=""same"")(x)
48
x=layers.dropout (0.2)(x)
49
x = layers. conv1dtranspose (32, 5,strides =1, activation=""relu"",
padding=""same"")(x)
50
x=layers.dropout (0.2)(x)
51
52
x = layers. conv1dtranspose (args.n_features , 5, activation=""relu
"", padding=""same"")(x)
53
54
model=models.model(inputs=inp , outputs=x)
55
56
model.compile(optimizer=""adam"", loss=""huber_loss"", metrics =[""
accuracy""])
57
58
#do a fit
59
training_data_path = os.path.join(args.train , ’train_data.json ’
)
60
obj_text = codecs.open(training_data_path , ’r’, encoding=’utf -8
’).read ()
61
b_new = json.loads(obj_text)
62
training_data = np.array(b_new)
63
64
model.fit(training_data ,training_data , epochs=args.epochs ,
verbose =0, batch_size=args.batch_size)
65
66
model_data_path = os.path.join(args.sm_model_dir , ’1’)
67
tensorflow.keras.models.save_model(model , model_data_path)
68
#tensorflow.saved_model.save(model , model_data_path )
e
pipeline.py
1 """"""
2
. -registermodel
3
.
4
process -> train
-> evaluate
-> condition .
5
.
6
. -stop
7
8 implements a get_pipeline (** kwargs) method.
9 """"""
10
11 import os
12
13 import
boto3
14 import
sagemaker
15 import
sagemaker.session
16 import
logging
17 from
datetime
import
datetime
64
18
19 from
sagemaker.tensorflow.serving
import
model
20 from
sagemaker.workflow.steps
import
createmodelstep
21
22 from
sagemaker.inputs
import
transforminput
23 from
sagemaker.inputs
import
createmodelinput
24 from
sagemaker.workflow.steps
import
transformstep
25
26 from
sagemaker.processing
import (
27
processinginput ,
28
processingoutput ,
29
scriptprocessor ,
30 )
31 from
sagemaker.sklearn.processing
import
sklearnprocessor
32 from
sagemaker.workflow.condition_step
import (
33
conditionstep ,
34
jsonget ,
35 )
36 from
sagemaker.workflow.parameters
import (
37
parameterinteger ,
38
parameterstring ,
39 )
40 from
sagemaker.workflow.pipeline
import
pipeline
41 from
sagemaker.workflow.steps
import (
42
processingstep
43 )
44
45 base_dir = os.path.dirname(os.path.realpath(__file__))
46 sm_client = boto3.client(""sagemaker"")
47 logger = logging.getlogger(__name__)
48
49 def
get_sagemaker_client (region):
50
"""""" gets the
sagemaker
client.
51
52
args:
53
region: the aws region to start the
session
54
default_bucket : the bucket to use for
storing
the
artifacts
55
56
returns:
57
‘sagemaker.session.session
instance
58
""""""
59
boto_session = boto3.session(region_name=region)
60
sagemaker_client = boto_session.client(""sagemaker"")
61
return
sagemaker_client
62
63
64 def
get_session(region , default_bucket):
65
"""""" gets the
sagemaker
session
based on the region.
66
67
args:
68
region: the aws region to start the
session
69
default_bucket : the bucket to use for
storing
the
artifacts
70
65
71
returns:
72
‘sagemaker.session.session
instance
73
""""""
74
75
boto_session = boto3.session(region_name=region)
76
77
sagemaker_client = boto_session.client(""sagemaker"")
78
runtime_client = boto_session.client(""sagemaker -runtime"")
79
return
sagemaker.session.session(
80
boto_session=boto_session ,
81
sagemaker_client =sagemaker_client ,
82
sagemaker_runtime_client =runtime_client ,
83
default_bucket =default_bucket ,
84
)
85
86 def
get_pipeline_custom_tags (new_tags , region ,
sagemaker_project_arn =none):
87
try:
88
sm_client = get_sagemaker_client (region)
89
response = sm_client.list_tags(
90
resourcearn= sagemaker_project_arn )
91
project_tags = response[""tags""]
92
for
project_tag in project_tags:
93
new_tags.append(project_tag)
94
except
exception as e:
95
print(f""error
getting
project
tags: {e}"")
96
return
new_tags
97
98 def
get_approved_package ( model_package_group_name ):
99
"""""" gets the latest
approved
model
package
for a model
package
group.
100
101
args:
102
model_package_group_name : the model
package
group
name.
103
104
returns:
105
the
sagemaker
model
package
arn.
106
""""""
107
try:
108
# get the latest
approved
model
package
109
response = sm_client. list_model_packages (
110
modelpackagegroupname =model_package_group_name ,
111
modelapprovalstatus =""approved"",
112
sortby=""creationtime"",
113
maxresults =100,
114
)
115
approved_packages = response["" modelpackagesummarylist ""]
116
117
# fetch
more
packages if none
returned
with
continuation
token
118
while len( approved_packages ) == 0 and ""nexttoken"" in
response:
119
logger.debug(""getting
more
packages
for token: {}"".
format(response[""nexttoken""]))
66
120
response = sm_client. list_model_packages (
121
modelpackagegroupname =model_package_group_name ,
122
modelapprovalstatus =""approved"",
123
sortby=""creationtime"",
124
maxresults =100,
125
nexttoken=response[""nexttoken""],
126
)
127
approved_packages .extend(response[""
modelpackagesummarylist ""])
128
129
# return
error if no packages
found
130
if len( approved_packages ) == 0:
131
error_message = (
132
f""no approved
modelpackage
found for
modelpackagegroup : { model_package_group_name }""
133
)
134
logger.error(error_message)
135
raise
exception(error_message)
136
137
# return the pmodel
package
arn
138
model_package_arn = approved_packages [0]["" modelpackagearn ""]
139
logger.info(f""identified
the latest
approved
model
package:
{ model_package_arn }"")
140
141
response = sm_client. describe_model_package (
modelpackagename = model_package_arn )
142
model_location = response[’inferencespecification ’][’
containers ’][0][ ’modeldataurl ’]
143
144
return
model_location
145
except
clienterror as e:
146
error_message = e.response[""error""][""message""]
147
logger.error(error_message)
148
raise
exception(error_message)
149
150 def
get_pipeline(
151
region ,
152
sagemaker_project_arn =none ,
153
role=none ,
154
default_bucket =none ,
155
model_package_group_name ="" anomalypackagegroup "",
156
pipeline_name="" anomalytransformpipeline "",
157
base_job_prefix ="" anomalytransform "",
158 ):
159
""""""
160
args:
161
region: aws region to create and run the
pipeline.
162
role: iam role to create and run steps and
pipeline.
163
default_bucket : the bucket to use for
storing
the
artifacts
164
165
returns:
166
an instance of a pipeline
167
""""""
168
sagemaker_session = get_session(region , default_bucket )
67
169
if role is none:
170
role = sagemaker.session. get_execution_role (
sagemaker_session )
171
172
# parameters
for
pipeline
execution
173
processing_instance_count = parameterinteger (
174
name="" processinginstancecount "",
175
default_value =1
176
)
177
processing_instance_type = parameterstring (
178
name="" processinginstancetype "",
179
default_value=""ml.m5.xlarge""
180
)
181
batch_instance_type = parameterstring (
182
name="" batchinstancetype "",
183
default_value=""ml.m5.xlarge""
184
)
185
186
#preprocess
187
sklearn_processor = sklearnprocessor (
188
framework_version =""0.23 -1"",
189
instance_type=processing_instance_type ,
190
instance_count =processing_instance_count ,
191
base_job_name=""analysis -preprocess"",
192
sagemaker_session =sagemaker_session ,
193
role=role ,
194
)
195
196
step_preprocess = processingstep(
197
name=""analysis -preprocess"",
198
processor=sklearn_processor ,
199
outputs =[ processingoutput (output_name=’data ’,
200
source=’/opt/ml/processing/data ’)],
201
code=os.path.join(base_dir , ""preprocess.py"")
202
)
203
""""""
204
#transform v2
205
image_uri = sagemaker.image_uris.retrieve(
206
framework ="" tensorflow"",
207
region=region ,
208
version =""2.3"" ,
209
py_version ="" py37"",
210
instance_type=processing_instance_type ,
211
image_scope=’training ’
212
)
213
214
script_processor = scriptprocessor (
215
image_uri=image_uri ,
216
command =["" python3 ""],
217
instance_type=processing_instance_type ,
218
instance_count =1,
219
role=role ,
220
)
221
68
222
step_transform = processingstep (
223
name ="" analysis -transform"",
224
processor=script_processor ,
225
code=os.path.join(base_dir , ""transform.py"")
226
)
227
228
""""""
229
#transform
230
image_uri = sagemaker.image_uris.retrieve(
231
framework=""tensorflow"",
232
region=region ,
233
version=""2.3"",
234
py_version=""py37"",
235
instance_type=processing_instance_type ,
236
image_scope=’training ’
237
)
238
239
model = model(model_data= get_approved_package (’autoencodergroup
’),framework_version =’1.13 ’, sagemaker_session =sagemaker_session
,
240
role=role ,)
241
242
output_key = f""{ base_job_prefix }/ analysis -transform -"" +
datetime.now().strftime(’%y-%m-%d:%h-%m’)
243
transformer = model.transformer(instance_count =1, instance_type
=’ml.m5.xlarge ’, output_path=f""s3 ://{ sagemaker_session .
default_bucket ()}"" + ’/’ +output_key)
244
245
step_transform = transformstep(
246
name=""analysis -transform"",
247
transformer=transformer ,
248
inputs=transforminput (data= step_preprocess .properties.
processingoutputconfig .outputs[
249
""data""
250
]. s3output.s3uri , content_type=’application/json ’,
split_type=’line ’)
251
)
252
253
#postprocess
254
image_uri = sagemaker.image_uris.retrieve(
255
framework=""tensorflow"",
256
region=region ,
257
version=""2.3"",
258
py_version=""py37"",
259
instance_type=processing_instance_type ,
260
image_scope=’training ’
261
)
262
263
script_processor = scriptprocessor (
264
image_uri=image_uri ,
265
command =[""python3""],
266
instance_type=processing_instance_type ,
267
instance_count =1,
268
role=role ,
69
269
)
270
271
step_postprocess = processingstep (
272
name=""analysis -postprocess"",
273
processor=script_processor ,
274
code=os.path.join(base_dir , ""postprocess.py""),
275
job_arguments =[""--batch_key"", output_key],
276
inputs =[
277
processinginput (
278
source= step_preprocess .properties.
processingoutputconfig .outputs[
279
""data""
280
]. s3output.s3uri ,
281
destination=""/opt/ml/processing/source""
282
)]
283
)
284
285
#pipeline
286
step_postprocess .add_depends_on ([ step_transform ])
287
288
pipeline = pipeline(
289
name=pipeline_name ,
290
parameters =[
291
processing_instance_type ,
292
processing_instance_count ,
293
batch_instance_type
294
],
295
steps =[ step_preprocess ,step_transform , step_postprocess ],
296
sagemaker_session =sagemaker_session ,
297
)
298
299
return
pipeline
70
f
postprocess.py
1 import
boto3
2 from
botocore.config
import
config
3 import
argparse
4 import
json
5 import
pathlib
6 import
tarfile
7 import
logging
8 import os
9 import
codecs
10
11 import
numpy as np
12 import
pandas as pd
13 from
datetime
import
datetime
14 import
time
15
16 logger = logging.getlogger ()
17 logger.setlevel(logging.info)
18 logger.addhandler(logging.streamhandler ())
19
20 if __name__ == ""__main__"":
21
parser = argparse.argumentparser ()
22
parser.add_argument(""--batch_key"", type=str , required=true)
23
args = parser.parse_args ()
24
25
logger.debug(""starting
postprocessing ."")
26
logger.debug(args.batch_key + ’/data.json.out’)
27
28
#extract
29
logger.debug(""extracting"")
30
s3 = boto3.client(’s3’)
31
s3.download_file(""sagemaker -eu -north -1 -683608778688"",args.
batch_key + ’/data.json.out’,’data.json.out’)
32
33
#transform
34
logger.debug(""transforming"")
35
prediction_data_path = ’data.json.out’
36
obj_text = codecs.open(prediction_data_path , ’r’, encoding=’utf
-8’).read ()
37
b_new_int = json.loads(obj_text)
38
pred_data = np.array(b_new_int[’predictions ’])
39
40
train_data_path = ""/opt/ml/processing/source/data.json""
41
obj_text = codecs.open(train_data_path , ’r’, encoding=’utf -8’).
read ()
42
b_new = json.loads(obj_text)
43
train_data = np.array(b_new)
44
45
mae = np.mean(np.abs(pred_data - train_data), axis =1)
46
used_columns =[’avglinecurrentrlackt1 ’,’
compr1aavglinecurrentrla_rslt ’, ’unitpowerconsump ’, ’
compr1aline1currentrla ’,
47
’compr1aline2currentrla ’,’compr1aline3currentrla ’,’
71
current_draw_denom ’, ’current_draw_blanks_denom ’]
48
df = pd.dataframe(mae , columns = used_columns)
49
df[’day’] = pd.series(range(0,len(df)), dtype=""float64"")
50
df[’id’] = pd.series(np.ones (29) *1703 , dtype=""float64"")
51
52
#load
53
logger.debug(""loading"")
54
feature_group_name =’postprocessing -’ + datetime.now().strftime(
’%y-%m-%d-%h-%m’)
55
my_config = config(
56
region_name = ’eu -north -1’,
57
signature_version = ’v4’,
58
retries = {
59
’max_attempts ’: 10,
60
’mode ’: ’standard ’
61
}
62
)
63
sm_client = boto3.client(’sagemaker ’,config=my_config)
64
featuregroupresponse = sm_client. create_feature_group (
65
featuregroupname =feature_group_name ,
66
recordidentifierfeaturename =’day’,
67
eventtimefeaturename =’id’,
68
featuredefinitions =[
69
{
70
’featurename ’: ’avglinecurrentrlackt1 ’,
71
’featuretype ’: ’fractional ’
72
},
73
{
74
’featurename ’: ’compr1aavglinecurrentrla_rslt ’,
75
’featuretype ’: ’fractional ’
76
},
77
{
78
’featurename ’: ’unitpowerconsump ’,
79
’featuretype ’: ’fractional ’
80
},
81
{
82
’featurename ’: ’compr1aline1currentrla ’,
83
’featuretype ’: ’fractional ’
84
},
85
{
86
’featurename ’: ’compr1aline2currentrla ’,
87
’featuretype ’: ’fractional ’
88
},
89
{
90
’featurename ’: ’compr1aline3currentrla ’,
91
’featuretype ’: ’fractional ’
92
},
93
{
94
’featurename ’: ’current_draw_denom ’,
95
’featuretype ’: ’fractional ’
96
},
97
{
98
’featurename ’: ’current_draw_blanks_denom ’,
99
’featuretype ’: ’fractional ’
72
100
},
101
{
102
’featurename ’: ’id’,
103
’featuretype ’: ’fractional ’
104
},
105
{
106
’featurename ’: ’day’,
107
’featuretype ’: ’fractional ’
108
},
109
],
110
onlinestoreconfig ={
111
’enableonlinestore ’: true
112
},
113
offlinestoreconfig ={
114
’s3storageconfig ’: {
115
’s3uri ’: ""s3:// sagemaker -eu -north -1 -683608778688/ ’
feature_store_trane_chiller ’/"",
116
}
117
},
118
rolearn=’arn:aws:iam ::683608778688: role/service -role/
amazonsagemaker -executionrole -20211116 t152772 ’,
119
description=’store for
predictions ’
120
)
121
122
data = df.to_numpy ()
123
124
fs_client = boto3.client(’sagemaker -featurestore -runtime ’,
config=my_config)
125
126
127
128
state = ’creating ’
129
130
while (state in [’creating ’]):
131
responsefeat = sm_client. describe_feature_group (
featuregroupname = feature_group_name )
132
state = responsefeat[’featuregroupstatus ’]
133
134
if state == ’created ’:
135
for index , record in enumerate(data):
136
output_record = []
137
for index , feature in enumerate(record):
138
dict_cool = {’featurename ’:df.columns[index], ’
valueasstring ’:str(feature)}
139
output_record.append(dict_cool)
140
141
response = fs_client.put_record(
142
featuregroupname =feature_group_name ,
143
record=output_record
144
)
145
146
else:
147
time.sleep (1)
73
g
preprocess.py
1 import
argparse
2 import os
3 import
pathlib
4 import
warnings
5 import
json
6 import
codecs
7
8 import
boto3
9 from
botocore.config
import
config
10 import
pandas as pd
11 import
numpy as np
12 import
random
13
14 from
datetime
import
datetime
15
16 def
float_to_date(float_input):
17
return
datetime.fromtimestamp(float_input).strftime(’%y-%m
-%dt%h:%m:%sz’)
18
19 if __name__ ==’__main__ ’:
20
#fetch
feature
store
contents
with
athena
21
my_config = config(
22
region_name = ’eu -north -1’,
23
signature_version = ’v4’,
24
retries = {
25
’max_attempts ’: 10,
26
’mode ’: ’standard ’
27
}
28
)
29
30
pathlib.path(f""/opt/ml/processing/data"").mkdir(parents=true ,
exist_ok=true)
31
client = boto3.client(’athena ’, config=my_config)
32
prefix = ’preprocessing -’ + datetime.now().strftime(’%y-%m-%d:%
h-%m’)
33
result_location = ’s3:// sagemaker -eu -north -1 -683608778688/ ’ +
prefix
34
35
querystart = client. start_query_execution (
36
querystring = ’select * from ""chiller -feature -group
-1643633036""
limit
1000 ’,
37
queryexecutioncontext = {
38
’database ’: ’sagemaker_featurestore ’
39
},
40
resultconfiguration = { ’outputlocation ’: result_location }
41
)
42
43
state = ’running ’
44
45
while (state in [’running ’, ’queued ’]):
46
response = client. get_query_execution ( queryexecutionid =
querystart[’queryexecutionid ’])
74
47
48
if ’queryexecution ’ in response
and \
49
’status ’ in response[’queryexecution ’] and \
50
’state ’ in response[’queryexecution ’][’status ’]:
51
state = response[’queryexecution ’][’status ’][’state ’]
52
53
if state == ’failed ’:
54
time.sleep (1)
55
56
elif
state == ’succeeded ’:
57
#csv to container
58
s3 = boto3.resource(""s3"")
59
path_to_athena_data = prefix + ’/’ + querystart[’
queryexecutionid ’] + ’.csv’
60
s3.bucket(’sagemaker -eu -north -1 -683608778688 ’).
download_file(path_to_athena_data , ’/opt/ml/processing/data.csv’
)
61
62
#read to df
63
df = pd.read_csv(’/opt/ml/processing/data.csv’,low_memory=false
)
64
df[’property_timestamp ’] = df[’property_timestamp ’]. transform(
float_to_date)
65
66
#set
timestamps to index and with
right
type
67
df[’property_timestamp ’] = pd.to_datetime(df[’
property_timestamp ’])
68
df = df.set_index(’property_timestamp ’)
69
70
#features
selected
71
used_cols =[’avglinecurrentrlackt1 ’,’
compr1aavglinecurrentrla_rslt ’, ’unitpowerconsump ’, ’
compr1aline1currentrla ’,
72
’compr1aline2currentrla ’,’compr1aline3currentrla ’,’
current_draw_denom ’, ’current_draw_blanks_denom ’]
73
74
#dataframe
manipulations
75
df = df[used_cols]
76
df = df.fillna (0)
77
df = (df -df.min())/(df.max()-df.min())
78
df = df.resample(’1d’).mean ()
79
df = df.fillna (0)
80
df = df.to_numpy ()
81
82
#static
parameters
83
lookback = 14
84
n_features = len(df [0])
85
anomaly_count = 10
86
anomaly_length = 10
87
anomaly_value = 0.06
88
89
#apply
lookback
90
output = []
91
for i in range(len(df) - lookback + 1):
75
92
output.append(df[i : (i + lookback)])
93
output = np.stack(output)
94
95
preprocessed_data = output
96
97
#save the
preprocessed
data
98
features_output_path = os.path.join(’/opt/ml/processing/data ’,
’data.json ’)
99
json.dump( preprocessed_data .tolist (), codecs.open(
features_output_path , ’w’, encoding=’utf -8’), separators =(’,’,’:
’), sort_keys=true , indent =4)
"
MScthesis_Yizhen_Zhao.pdf;"
references
[11] databricks. mlﬂow quick start part 2: serving models with microsoft
azure ml. 10
[12] emmanuel raj. edge mlops framework for aiot applications, continuous
delivery for aiot, big data and 5g applications. june 2020. 10
[13] istván pölöskei. mlops approach in the cloud-native data pipeline design.
acta technica jaurinensis, 04 2021. 11
[14] yizhen zhao. mlops and data versioning in machine learning project. 21,
24
[15] yizhen zhao. mlops: data versioning with dvc — part i. 21
[16] yizhen zhao. mlops: deploy custom model with aws sagemaker batch
transform — part ii. 24
[17] mlflow. how runs and artifacts are recorded. 29
[18] azure. track ml models with mlﬂow and azure machine learning. 29
[19] aws. train a model with amazon sagemaker. 37
[20] tian li, anit kumar sahu, ameet s. talwalkar, and virginia smith. fed-
erated learning: challenges, methods, and future directions. ieee signal
processing magazine, 37:50–60, 2020. 38
[21] tian li. federated learning: challenges, methods, and future directions.
38
[22] aws. automatically scale amazon sagemaker models. 38
[23] azure. machine learning operations maturity model. 38
54
"
no.ntnu_inspera_76427839_35170985.pdf;
Poster_Summer_School_20225.pdf;"
references
[1] a. morichetta, v. c. pujol, and s. dustdar, “a roadmap on learning
and reasoning for distributed computing continuum ecosystems,” in 2021
ieee international conference on edge computing (edge), pp. 25–31,
2021.
[2] a. luckow, k. rattan, and s. jha, “pilot-edge: distributed resource man-
agement along the edge-to-cloud continuum,” in 2021 ieee international
parallel and distributed processing symposium workshops (ipdpsw),
pp. 874–878, 2021.
[3] d. rosendo, a. costan, g. antoniu, m. simonin, j.-c. lombardo, a. joly,
and p. valduriez, “reproducible performance optimization of complex
applications on the edge-to-cloud continuum,” in 2021 ieee international
conference on cluster computing (cluster), pp. 23–34, 2021.
[4] l. baresi and d. filgueira mendona, “towards a serverless platform
for edge computing,” in 2019 ieee international conference on fog
computing (icfc), pp. 1–10, 2019.
[5] t. pfandzelter and d. bermbach, “tinyfaas: a lightweight faas platform
for edge environments,” in 2020 ieee international conference on fog
computing (icfc), pp. 17–24, 2020.
[6] m. ciavotta, d. motterlini, m. savi, and a. tundo, “dfaas: decentralized
function-as-a-service for federated edge computing,” in 2021 ieee 10th
international conference on cloud networking (cloudnet), pp. 1–4,
2021.
[7] c. sicari, l. carnevale, a. galletta, and m. villari, “openwolf: a
serverless workﬂow engine for native cloud-edge continuum,” in he 7th
ieee cyber science and technology congress (cyberscitech 2022), 09
2022.
"
screenshot.pdf;
sensors-22-04425-v2.pdf;"
references
1.
adadi, a. a survey on data-efﬁcient algorithms in big data era. j. big data 2021, 8, 1–54. [crossref]
2.
jones, j.; ionita, a.; mihai, i.c. ai and iot mapping and the transition to an interconnected cyber defence and intelligence
capabilities. int. conf. cybersecur. cybercrime 2022, 9, 5–22. [crossref]
3.
romero, o.; wrembel, r.; song, i.y. an alternative view on data processing pipelines from the dolap 2019 perspective. j. inf.
syst. 2020, 92, 101489. [crossref]
4.
alla, s.; adari, s.k. what is mlops? in beginning mlops with mlflow: deploy models in aws sagemaker, google cloud, and
microsoft azure; apress: berkeley, ca, usa, 2021; pp. 79–124. [crossref]
5.
leite, l.; rocha, c.; kon, f.; milojicic, d.; meirelles, p. a survey of devops concepts and challenges. acm comput. surv.
2019, 52. [crossref]
6.
challenges with ml in production. 2022. available online: https://docs.cloudera.com/machine-learning/1.1/product/topics/
ml-challenges-in-prod.html (accessed on 31 may 2022).
7.
díaz-de arcaya, j.; miñón, r.; torre-bastida, a.i.; del ser, j.; almeida, a. padl: a modeling and deployment language for
advanced analytical services. sensors 2020, 20, 6712. [crossref]
8.
wagner, h. deep mining: a rock engineering challenge. rock mech. rock eng. 2019, 52, 1417–1446. [crossref]
9.
li, c.c. principles and methods of rock support for rockburst control. j. rock mech. geotech. eng. 2021, 13, 46–59. [crossref]
10.
rajapakse, r. rock bolts, dowels, and cable bolts. in geotechnical engineering calculations and rules of thumb; rajapakse, r., ed.;
elsevier/butterworth-heinemann: amsterdam, the netherlands, 2008; pp. 303–320. [crossref]
11.
nöger, m.; hartlieb, p.; moser, p.; griesser, t.; ladinig, t.; dendl, d. the potential of a mine-wide digital rock mass condition
monitoring system. in proceedings of the 5th international future mining conference, perth, australia and online, 6–8
december 2021.
12.
singh, a.; singh, u.k.; kumar, d. iot in mining for sensing, monitoring and prediction of underground mines roof support. in
proceedings of the 2018 4th international conference on recent advances in information technology (rait), dhanbad, india,
15–17 march 2018; pp. 1–5. [crossref]
13.
song, g.; li, w.; wang, b.; ho, s.c.m. a review of rock bolt monitoring using smart sensors. sensors 2017, 17, 776. [crossref]
14.
illumineation-projcet. 2022. available online: https://www.illumineation-h2020.eu/ (accessed on 31 may 2022).
15.
pivarski, j.; bennett, c.; grossman, r.l. deploying analytics with the portable format for analytics (pfa). in proceedings of the
22nd acm sigkdd international conference on knowledge discovery and data mining, acm, san francisco, ca, usa, 13–17
august 2016; pp. 579–588.
16.
rahman, a.; mahdavi-hezaveh, r.; williams, l. a systematic mapping study of infrastructure as code research. inf. softw.
technol. 2019, 108, 65–77. [crossref]
17.
chef. 2022. available online: https://www.chef.io (accessed on 31 may 2022).
18.
loope, j. managing infrastructure with puppet: conﬁguration management at scale; o’reilly media, inc.: sebastopol, ca, usa, 2011.
19.
zadka, m. salt stack. in devops in python: infrastructure as python; apress: berkeley, ca, usa, 2019; pp. 121–137. [crossref]
20.
zadka, m. ansible. in devops in python: infrastructure as python; apress: berkeley, ca, usa, 2019; pp. 139–145. [crossref]
21.
chef vs. puppet vs. ansible vs. saltstack: which works best for you? 2022. available online: https://www.edureka.co/blog/
chef-vs-puppet-vs-ansible-vs-saltstack (accessed on 31 may 2022).
22.
terraform. 2022. available online: https://www.terraform.io (accessed on 31 may 2022).
23.
aws cloudformation. 2022. available online: https://aws.amazon.com/es/cloudformation (accessed on 31 may 2022).
24.
openstack heat. 2022. available online: https://docs.openstack.org/heat (accessed on 31 may 2022).
25.
cloudera. 2022. available online: https://www.cloudera.com (accessed on 31 may 2022).
26.
1010data. 2022. available online: https://www.1010data.com (accessed on 31 may 2022).
27.
azure hd insight. 2022. available online: https://azure.microsoft.com/es-es/services/hdinsight (accessed on 31 may 2022).
28.
verma, a.; pedrosa, l.; korupolu, m.; oppenheimer, d.; tune, e.; wilkes, j. large-scale cluster management at google with borg.
in proceedings of the tenth european conference on computer systems, bordeaux, france, 21–24 april 2015; pp. 1–17.
29.
foundation, c.n.c. ofﬁcial kubernetes website. 2022. available online: https://kubernetes.io (accessed on 31 may 2022).
30.
hykes, s. docker swarm engine. 2022. available online: https://docs.docker.com/engine/swarm (accessed on 31 may 2022).
31.
kubeedge. 2022. available online: https://kubeedge.io (accessed on 31 may 2022).
32.
apache airﬂow. 2022. available online: https://airﬂow.apache.org (accessed on 31 may 2022).
33.
guazzelli, a.; zeller, m.; lin, w.-c.; williams, g. pmml: an open standard for sharing models. r j. 2009, 1, 60–65. [crossref]
34.
onnx. 2021. available online: https://onnx.ai/ (accessed on 31 may 2022).
35.
zaharia, m.; chen, a.; davidson, a.; ghodsi, a.; hong, s.a.; konwinski, a.; murching, s.; nykodym, t.; ogilvie, p.; parkhe, m.;
et al. accelerating the machine learning lifecycle with mlﬂow. ieee data eng. bull. 2018, 41, 39–45.
36.
liu, p.; bravo-rocca, g.; guitart, j.; dholakia, a.; ellison, d.; hodak, m. scanﬂow: an end-to-end agent-based autonomic
ml workﬂow manager for clusters. in proceedings of the 22nd international middleware conference: demos and posters,
middleware ‘21, virtual event, 6–10 december 2021; association for computing machinery: new york, ny, usa, 2021; pp. 1–2.
[crossref]
sensors 2022, 22, 4425
29 of 29
37.
crankshaw, d.; wang, x.; zhou, g.; franklin, m.j.; gonzalez, j.e.; stoica, i. clipper: a {low-latency} online prediction serving
system. in proceedings of the 14th usenix symposium on networked systems design and implementation (nsdi 17), boston,
ma, usa, 27–29 march 2017; pp. 613–627.
38.
ml.net. 2022. available online: https://dotnet.microsoft.com/learn/ml-dotnet/ (accessed on 31 may 2022).
39.
lee, y.; scolari, a.; chun, b.g.; weimer, m.; interlandi, m. from the edge to the cloud: model serving in ml. net. ieee data
eng. bull. 2018, 41, 46–53.
40.
zhao, j.; tiplea, t.; mortier, r.; crowcroft, j.; wang, l. data analytics service composition and deployment on edge devices. in
proceedings of the 2018 workshop on big data analytics and machine learning for data communication networks, budapest,
hungary, 20 august 2018; pp. 27–32.
41.
pycaret. 2022. available online: https://pycaret.org (accessed on 31 may 2022).
42.
seldon. 2022. available online: https://www.seldon.io (accessed on 31 may 2022).
43.
talagala, n.; sundararaman, s.; sridhar, v.; arteaga, d.; luo, q.; subramanian, s.; ghanta, s.; khermosh, l.; roselli, d. eco:
harmonizing edge and cloud with ml/dl orchestration. available online: https://www.usenix.org/system/ﬁles/conference/
hotedge18/hotedge18-papers-talagala.pdf (accessed on 31 may 2022).
44.
bhattacharjee, a.; barve, y.; khare, s.; bao, s.; kang, z.; gokhale, a.; damiano, t. stratum: a bigdata-as-a-service for lifecycle
management of iot analytics applications. in proceedings of the 2019 ieee international conference on big data (big data), los
angeles, ca, usa, 9–12 december 2019; pp. 1607–1612.
45.
pytorch, torchserve. 2022. available online: https://pytorch.org/serve/ (accessed on 3 march 2022).
46.
baylor, d.; breck, e.; cheng, h.t.; fiedel, n.; foo, c.y.; haque, z.; haykal, s.; ispir, m.; jain, v.; koc, l.; et al. tfx: a tensorﬂow-
based production-scale machine learning platform. in proceedings of the 23rd acm sigkdd international conference on
knowledge discovery and data mining, halifax, ns, canada, 13–17 august 2017; pp. 1387–1395.
47.
paszke, a.; gross, s.; massa, f.; lerer, a.; bradbury, j.; chanan, g.; killeen, t.; lin, z.; gimelshein, n.; antiga, l.; et al. pytorch:
an imperative style, high-performance deep learning library. in proceedings of the 33rd conference on neural information
processing systems (neurips 2019), vancouver, bc, canada, 8–14 december 2019.
48.
abadi, m.; barham, p.; chen, j.; chen, z.; davis, a.; dean, j.; devin, m.; ghemawat, s.; irving, g.; isard, m.; et al. {tensorflow}:
a system for {large-scale} machine learning. in proceedings of the 12th usenix symposium on operating systems design and
implementation (osdi 16), savannah, ga, usa, 2–4 november 2016; pp. 265–283.
49.
kubeﬂow. 2022. available online: https://www.kubeﬂow.org (accessed on 31 may 2022).
50.
overview of docker compose. 2022. available online: https://docs.docker.com/compose/ (accessed on 31 may 2022).
51.
angular json editor package. 2022.
available online: https://www.npmjs.com/package/ang-jsoneditor (accessed on 31
may 2022).
52.
angular material design stepper component. 2022. available online: https://material.angular.io/components/stepper/overview
(accessed on 31 may 2022).
53.
titus. 2022. available online: https://pypi.org/project/titus2 (accessed on 31 may 2022).
54.
zaharia, m.; xin, r.s.; wendell, p.; das, t.; armbrust, m.; dave, a.; meng, x.; rosen, j.; venkataraman, s.; franklin, m.j.; et al.
apache spark: a uniﬁed engine for big data processing. commun. acm 2016, 59, 56–65. [crossref]
"
Snyder_2019_Literature review as a research methodology_an overview and guidelines.pdf;"
references
antons, d., & breidbach, c. f. (2018). big data, big insights? advancing service in-
novation and design with machine learning. journal of service research, 21, 17–39.
https://doi.org/10.1177/1094670517738373.
baumeister, r. f., & leary, m. r. (1997). writing narrative literature reviews. review of
general psychology, 1, 311–320. https://doi.org/10.1037/1089-2680.1.3.311.
borman, g. d., & dowling, n. m. (2008). teacher attrition and retention: a meta-analytic
and narrative review of the research. review of educational research, 78, 367–409.
https://doi.org/10.3102/0034654308321455.
boyd, b. k., & solarino, a. m. (2016). ownership of corporations: a review, synthesis, and
research agenda. journal of management, 42, 1282–1314. https://doi.org/10.1177/
0149206316633746.
braun, v., & clarke, v. (2006). using thematic analysis in psychology. qualitative research
in psychology, 3, 77–101. https://doi.org/10.1191/1478088706qp063oa.
carlborg, p., kindström, d., & kowalkowski, c. (2014). the evolution of service in-
novation research: a critical review and synthesis. the service industries journal,
34(5), 373–398. https://doi.org/10.1080/02642069.2013.780044.
carrillat, f. a., legoux, r., & hadida, a. l. (2018). debates and assumptions about motion
picture performance: a meta-analysis. journal of the academy of marketing science, 46,
273–299. https://doi.org/10.1007/s11747-017-0561-6.
chang, w., & taylor, s. a. (2016). the eﬀectiveness of customer participation in new
product development: a meta-analysis. journal of marketing, 80, 47–64. https://doi.
org/10.1509/jm.14.0057.
covington, m. v. (2000). goal theory, motivation, and school achievement: an in-
tegrative review. annual review of psychology, 51, 171–200. https://doi.org/10.
1146/annurev.psych.51.1.171.
davis, j., mengersen, k., bennett, s., & mazerolle, l. (2014). viewing systematic reviews
and meta-analysis in social research through diﬀerent lenses. springerplus, 3, 511.
https://doi.org/10.1186/2193-1801-3-511.
dersimonian, r., & laird, n. (1986). meta-analysis in clinical trials. controlled clinical
trials, 7, 177–188. https://doi.org/10.1016/0197-2456(86)90046-2.
edeling, a., & himme, a. (2018). when does market share matter? new empirical gen-
eralizations from a meta-analysis of the market share–performance relationship.
journal of marketing, 82, 1–24. https://doi.org/10.1509/jm.16.0250.
glass, g. v. (1976). primary, secondary, and meta-analysis of research. educational
researcher, 5, 3–8. https://doi.org/10.2307/1174772.
grant, m. j., & booth, a. (2009). a typology of reviews: an analysis of 14 review types
and associated methodologies. health information & libraries journal, 26, 91–108.
https://doi.org/10.1111/j.1471-1842.2009.00848.x.
greenhalgh, t., robert, g., macfarlane, f., bate, p., & kyriakidou, o. (2004). diﬀusion of
innovations in service organizations: systematic review and recommendations.
milbank quarterly, 82, 581–629. https://doi.org/10.1111/j.0887-378x.2004.
00325.x.
gross, j. j. (1998). the emerging ﬁeld of emotion regulation: an integrative review.
review of general psychology, 2, 271–299. https://doi.org/10.1037/1089-2680.2.3.
271.
liberati, a., altman, d. g., tetzlaﬀ, j., mulrow, c., gøtzsche, p. c., ioannidis, j. p. a., ...
moher, d. (2009). the prisma statement for reporting systematic reviews and meta-
analyses of studies that evaluate health care interventions: explanation and ela-
boration. annals of internal medicine, 151, w–65. https://doi.org/10.7326/0003-
4819-151-4-200908180-00136.
macinnis, d. j. (2011). a framework for conceptual contributions in marketing. journal of
marketing, 75, 136–154. https://doi.org/10.1509/jmkg.75.4.136.
mazumdar, t., raj, s. p., & sinha, i. (2005). reference price research: review and pro-
positions. journal of marketing, 69, 84–102. https://doi.org/10.1509/jmkg.2005.69.
4.84.
mccoll-kennedy, j. r., snyder, h., elg, m., witell, l., helkkula, a., hogan, s. j., &
anderson, l. (2017). the changing role of the health care customer: review, synth-
esis and research agenda. journal of service management, 28.
moher, d., liberati, a., tetzlaﬀ, j., & altman, d. g. (2009). preferred reporting items for
systematic reviews and meta-analyses: the prisma statement. annals of internal
medicine, 151, 264–269. https://doi.org/10.7326/0003-4819-151-4-200908180-
00135.
palmatier, r. w., houston, m. b., & hulland, j. (2018). review articles: purpose, process,
and structure. journal of the academy of marketing science, 46, 1–5. https://doi.org/
10.1007/s11747-017-0563-4.
rodell, j. b., breitsohl, h., schröder, m., & keating, d. j. (2016). employee volunteering:
a review and framework for future research. journal of management, 42, 55–84.
https://doi.org/10.1177/0149206315614374.
snyder, h., witell, l., gustafsson, a., fombelle, p., & kristensson, p. (2016). identifying
categories of service innovation: a review and synthesis of the literature. journal of
business research, 69, 2401–2408. https://doi.org/10.1016/j.jbusres.2016.01.009.
torraco, r. j. (2005). writing integrative literature reviews: guidelines and examples.
human resource development review, 4, 356–367. https://doi.org/10.1177/
1534484305278283.
tranﬁeld, d., denyer, d., & smart, p. (2003). towards a methodology for developing
evidence-informed management knowledge by means of systematic review. british
journal of management, 14, 207–222. https://doi.org/10.1111/1467-8551.00375.
verlegh, p. w. j., & steenkamp, j.-b. e. m. (1999). a review and meta-analysis of country-
of-origin research. journal of economic psychology, 20, 521–546. https://doi.org/10.
1016/s0167-4870(99)00023-9.
ward, v., house, a., & hamer, s. (2009). developing a framework for transferring
knowledge into action: a thematic analysis of the literature. journal of health services
research and policy, 14, 156–164. https://doi.org/10.1258/jhsrp.2009.008120.
webster, j., & watson, r. t. (2002). analyzing the past to prepare for the future: writing
a literature review. management information systems quarterly, 26, 3.
whittemore, r., & knaﬂ, k. (2005). the integrative review: updated methodology.
journal of advanced nursing, 52, 546–553. https://doi.org/10.1111/j.1365-2648.
2005.03621.x.
witell, l., snyder, h., gustafsson, a., fombelle, p., & kristensson, p. (2016). deﬁning
service innovation: a review and synthesis. journal of business research, 69,
2863–2872.
wong, g., greenhalgh, t., westhorp, g., buckingham, j., & pawson, r. (2013). rameses
publication standards: meta-narrative reviews. bmc medicine, 11, 20. https://doi.
org/10.1186/1741-7015-11-20.
hannah snyder is an assistant professor at the department of marketing, bi - norwegian
school of business, oslo, norway. her research interest relates to service innovation,
customer creativity, deviant customer behavior, and value co-creation as well as a special
interest in literature review methodology. she has published in the journal of business
research, european journal of marketing, journal of service management and international
journal of nursing studies.
h. snyder
journal of business research 104 (2019) 333–339
339
"
SS08-02-008.pdf;"
references
agre, p. e., and chapman, d. 1987. pengi: an implementa-
tion of a theory of activity. in proc. national conf. on artiﬁcial
intelligence, 268–272. morgan kaufmann.
atkins, e. m.; durfee, e. h.; and shin, k. g. 1996. plan devel-
opment using local probabilistic models. in uai, 49–56.
atkins, e. m.; durfee, e. h.; and shin, k. g. 1997. detecting and
reacting to unplanned-for world states. in proc. national conf. on
artiﬁcial intelligence, 571–576.
glover, f., and laguna, m. 1993. tabu search. in reeves, c.,
ed., modern heuristic techniques for combinatorial problems.
oxford, england: blackwell scientiﬁc publishing.
goldman, r. p.; musliner, d. j.; krebsbach, k. d.; and boddy,
m. s. 1997. dynamic abstraction planning. in proc. national
conf. on artiﬁcial intelligence, 680–686.
goldman, r. p.; musliner, d. j.; and krebsbach, k. d. 2001.
managing online self-adaptation in real-time environments. in
proc. second international workshop on self adaptive software.
goldman, r. p.; musliner, d. j.; and krebsbach, k. d. 2003.
managing online self-adaptation in real-time environments.
in
lecture notes in computer science, volume 2614. springer-
verlag. 6–23.
goldman, r. p.; musliner, d. j.; and pelican, m. j. 2002. ex-
ploiting implicit representations in timed automaton veriﬁcation
for controller synthesis. in proceedings of the 2002 hybrid sys-
tems: computation and control workshop.
levi, s. t.; tripathi, s. k.; carson, s. d.; and agrawala, a. k.
1989. the maruti hard real-time operating system. acm op-
erating system review 23(3).
mcdermott, d. 1999. using regression-match graph to control
search in planning. artiﬁcial intelligence 109(1-2):111–159.
musliner, d. j.; hendler, j. a.; agrawala, a. k.; durfee, e. h.;
strosnider, j. k.; and paul, c. j. 1995. the challenges of real-time
ai. ieee computer 28(1):58–66.
musliner, d. j.; durfee, e. h.; and shin, k. g. 1993. circa: a
cooperative intelligent real-time control architecture. ieee trans.
systems, man, and cybernetics 23(6):1561–1574.
musliner, d. j.; durfee, e. h.; and shin, k. g. 1995. world
modeling for the dynamic construction of real-time control plans.
artiﬁcial intelligence 74(1):83–127.
musliner, d. j.; goldman, r. p.; and krebsbach, k. d. 2003.
deliberation scheduling strategies for adaptive mission planning
in real-time environments. in proc. third international workshop
on self adaptive software.
musliner, d. j.; goldman, r. p.; and pelican, m. j. 2000. using
model checking to guarantee safety in automatically-synthesized
real-time controllers. in proc. ieee int’l conf. on robotics and
automation.
musliner, d. j.
1993.
circa: the cooperative intelligent
real-time control architecture. ph.d. dissertation, university
of michigan, ann arbor. available as university of maryland
computer science technical report cs-tr-3157.
musliner, d. j.
1994a.
scheduling issues arising from auto-
mated real-time system design. technical report cs-tr-3364,
umiacs-tr-94-118, university of maryland department of
computer science.
musliner, d. j. 1994b. using abstraction and nondeterminism to
plan reaction loops. in proc. national conf. on artiﬁcial intelli-
gence, 1036–1041.
musliner, d. j. 2000. imposing real-time constraints on self-
adaptive controller synthesis. in proc. int’l workshop on self-
adaptive software.
musliner, d. j. 2001. imposing real-time constraints on self-
adaptive controller synthesis. in lecture notes in computer sci-
ence, number 1936. springer-verlag.
smith, r. 1977. the contract net: a formalism for the control of
distributed problem solving. in proc. int’l joint conf. on artiﬁcial
intelligence, volume 1, 472.
younes, h. l., and musliner, d. j. 2002. probabilistic plan veriﬁ-
cation through acceptance sampling. in proc. aips-02 workshop
on planning via model checking, 81–88.
younes, h. l. s.; musliner, d. j.; and simmons, r. g. 2003. a
framework for planning in continuous-time stochastic domains.
in proc. int’l conf. on automated planning and scheduling, 195–
204.
yovine, s. 1998. model-checking timed automata. in rozenberg,
g., and vaandrager, f., eds., embedded systems. springer verlag.
"
tesi_enrico_salvucci.pdf;"
reference architecture for mlops
building an eﬀective architecture, according to each diﬀerent needs of a company,
is crucial to get the best from mlops.
due to the plethora of tools, which are continuously arising, it may be very
hard to select exactly the best technologies and infrastructures to use. first of all,
it is necessary to understand the business process, how the models are developed
and how they are put into production; by doing so a clear understanding of what
tools and technologies better ﬁt the business needs can be achieved. the mlops
space is not done yet and it is constantly evolving; it is important to have an
architecture, from a technical point of view, which is very modular, in such a way
that it will be possible to change the building blocks, the tools and the technologies
at any time [37].
chapter 2 oﬀers a large variety of open source tools but, as pointed out,
diﬀerent solutions by the main cloud providers are available.
adopting open
source instruments or building the mlops architecture on a cloud solution is not
a mutually exclusive choice, a mixed approach can be adopted. open source tools
provide transparency and ﬂexibility with respect to the cloud vendor since they
typically are cloud-agnostic. moreover, some tools are considered “standard de
facto” at the moment, some noteworthy examples are docker and kubernetes;
open source tools also allow a modular approach.
on the other hand cloud
solutions provide a full-stack environment, they ensure enterprise support and
typically oﬀer best-known tools with respect to speciﬁc open source instruments.
however, proprietary solutions have diﬀerent drawbacks: “they reduce extendibility
and transparency on a pipeline while enforcing heavy vendor lock-in. further cloud
providers services are often not a feasible solution for companies that work on
regulatory software devices or software with user-privacy concerns; they require an
mlops solution that can be run cloud-agnostic and on-premises machines” [38].
37
38chapter 3. design and implementation of a reference architecture for mlops
existing cloud providers oﬀer machine learning platforms such as ai platform
(google cloud), azureml (microsoft) and sagemaker (aws) to build an mlops
architecture. “the adoption of such machine learning platform depends on the
cloud strategy of the organisation” [29].
when an in-house hosted solution is
preferred, using open source tools is a better choice (as they typically are cloud-
agnostic).
3.1
introducing an mlops architecture in the
business process
the process of introducing mlops and building a successful machine learning
system might be challenging for a company: it requires to change the approach
of developing and deploying the system, it involves a lot of people in diﬀerent
teams (cf. section 1.2.2), it needs to introduce new tools in the whole process and
the tools to be introduced need to integrate with the existing enterprise systems,
platform choices, pipeline strategy, and monitoring applications. mlops should
help the machine learning workﬂow, not inhibit it; thus, designing a good strategy
to bring an mlops approach in the business process is very important and it is
crucial to focus on the change management it requires.
two diﬀerent approaches can be adopted; those procedures may also be in-
terleaved, depending on the experience and the skills of the people involved, the
customer’s requirements and the infrastructure currently in use. the ﬁrst method
is based on the complexity of the tools, the second on the level of automation
instead. regardless of the approach, it is ﬁrst essential to do a diagnosis of the
current practices and processes used by the diﬀerent teams, also by organising
multiple interviews with the key stakeholders from the business, it, data science
and ops teams. the most straightforward method concerns introducing mlops
technologies step-by-step, starting from the simplest tool: for example, mlflow
is very easy to use and it provides diﬀerent beneﬁts, from an mlops point of
view, by only using a python api. the sooner the teams experience the beneﬁts
of mlops best practices, the better; by doing so the people involved in the process
familiarise themselves with the new method. afterwards, the focus, when building
an mlops architecture with this approach, can shift on the tools which provide
the highest value, possibly prioritising reproducibility and automation; by using
kubeﬂow, for example, reproducibility, validation and the focus on the process
can be guaranteed (and, in addition, ci/cd and continuous training can be im-
plemented by adopting, along kubeﬂow itself, few other tools). a ﬁner and neater
approach consists of enhancing the level of automation in the process of building
a machine learning system. as explained in section 1.4.7, google identiﬁes three
3.2. the reference architecture
39
levels of automation [4]: in the ﬁrst (level 0) the whole process is handled manually
(without mlops), level 1 adds the execution of the model training automatically
and, the last stage, consists of introducing a full ci/cd system.
3.2
the reference architecture
figure 3.1: the designed and implemented mlops architecture [7].
this section is intended to show the implementation of the reference mlops
architecture for this thesis and its beneﬁts on the whole machine learning system;
the architecture does not leverage in the considerations exposed in the previous
section but an exploratory approach has been adopted. this implementation is
the result of the internship at data reply; the goal of the company was to explore
the diﬀerent tools in the market and use them to build an mlops architecture (by
using preferably open source software).
40chapter 3. design and implementation of a reference architecture for mlops
figure 3.2: the dag of the training
pipeline on kubeﬂow.
figure 3.3: the dag of the predic-
tion pipeline on kubeﬂow.
the project is built upon a kaggle notebook [39] as use case example; though,
here, the focus is on the whole architecture itself and not on the model only; in the
github repository [7] of the project detailed documentation about the architecture
implementation is provided. the original notebook builds and trains three diﬀerent
models to forecast total german power consumption, on an hourly basis, with a
lead time of 24 hours in the data; in this implementation two of the three models
in the original notebook has been employed (sgd regressor and random forest
regressor) and, instead of the german dataset, the italian one has been used
(available within the same notebook). since the aim of this implementation is to
build an mlops architecture, instead of training a machine learning model, the
dataset has a very simple structure; it is composed of a “start” column, an “end”
column and a “load” column; “start” and “end” respectively contain the start
time and the end time of the measured power consumption, the “load” column
represents the power consumption itself in the range of time between “start” and
“end”.
figure 3.1 shows the whole implemented architecture, which is composed of two
diﬀerent pipelines (a training pipeline and a prediction pipeline); the prediction
pipeline was not in the original notebook and it has been implemented, in this
project, in order to serve the trained models.
implementation of the two pipelines
the components of each pipeline are
orchestrated together; this allows to foreground the whole process instead of fo-
cusing only on the model building. figure 3.2 shows the dag of the training
3.2. the reference architecture
41
pipeline implemented through kubeﬂow, used in this project as pipeline orches-
trator; the green tag, beside each component, represents the component itself has
been run successfully. in order to implement the two pipelines, the original note-
book has been split into multiple docker containers, one for each component of the
pipelines. this approach allows to build a modular architecture and also to reuse
some components in both the pipelines (data extraction and feature engineering
in figure 3.1). the components of the training pipeline include: data ingestion,
data preparation and feature engineering, model training and, the last step, is
aimed to promote the model from “staging” to “production” according to its per-
formances; the promotion of a model has been implemented comparing the results
of the two trained models by using the conditions mechanism of kubeﬂow, which
allows choosing a speciﬁc path in the dag according to speciﬁc circumstances.
the prediction pipeline (figure 3.3), besides the components to extract and pre-
pare the data, includes a component to load a pre-trained model and two diﬀerent
containers to build a batch and a real-time prediction service. kubeﬂow allows to
reuse components implemented by other people: the “remove-header” component
in the prediction pipeline is an example of a “reusable component” and it removes
the header from the dataset.
the implementation of both the pipelines, through diﬀerent docker containers,
guarantees ﬂexibility, reusability, portability, encapsulation and repeatability.
figure 3.4: successful runs of the model training experiments, tracked through
mlflow.
42chapter 3. design and implementation of a reference architecture for mlops
figure 3.5: runs of the training and predictions pipelines in kubeﬂow.
the data ﬂow
within the training pipeline the data, ﬁrst of all, are ingested and
appended to the existing data (in the “data-ingestion” component in figure 3.2),
then the dataset is transformed and prepared in such a way as to be processed
by the model (“data-preparation” component in figure 3.2) by performing feature
engineering; afterwards the data pass to the model training and validation com-
ponent (which, in figure 3.2 could be either the “linear-regression-training” or the
“random-forest-regressor-training” component).
in the prediction pipeline data are ingested and prepared in the same way
as the training pipeline, thereafter they feed either the rest api or the batch
prediction components; the output produced by the batch prediction component
is ﬁnally saved on the prediction bucket.
implementation of models experiment tracking and versioning
the
model training component includes experiment tracking and versioning, imple-
mented by using mlflow. the component, in addition to build and train the
model, also logs the parameters (cf. figure 2.2), the metrics (cf. figure 3.4) and
the artefacts produced by the model itself; through mlflow it also tracks the
model versions (cf. figure 2.3) and stores them in a model registry. mlflow is
also used, in this project, to promote a version of a trained model for a “produc-
tion” usage and to load it in the prediction pipeline (in such a way as to be used
for batch or real-time analysis). figure 3.4 shows some of the successful runs of
3.2. the reference architecture
43
the experiments tracked by mlflow.
by tracking the whole training, in this architecture, all the experiments can be
easily reproduced, versioned, compared with each other and executed in the same,
logged, environment.
figure 3.6: openapi speciﬁcations of the bentoml service.
implementation of automation, continuous integration, continuous
delivery and continuous training
the training and the prediction pipelines
can be executed automatically according to a run schedule. moreover, when the
code in the repository changes (for example as a result of a commit or a merge)
or new data are available, the two pipelines are automatically run. these two fea-
tures, continuous integration and continuous training, enable automation and
they have been implemented through kubeﬂow and, respectively, google cloud
build and google cloud functions. figure 3.5 shows some runs of the training
and the prediction pipelines on kubeﬂow, the green tag in the “status” column
represents the run completed successfully; in the “run name” column the runs
with the commit hash are those triggered by google cloud build and, the runs
with the updated dataset name, those triggered by google cloud functions.
despite seldon core and kfserving are the main tools on the landscape of
open source serving tools, in this project both the batch and the real-time pre-
diction service have been implemented through bentoml. as mentioned in sec-
tion 2.1.6 bentoml is an emerging high-performance open source framework for
44chapter 3. design and implementation of a reference architecture for mlops
figure 3.7: successful response of a prediction through the bentoml rest api.
serving, managing, and deploying machine learning models. this choice is due
to the eﬀectiveness and the easiness of use of bentoml. this tool allows to build
a rest api (by also providing an openapi graphic interface and its speciﬁca-
tions, cf. figure 3.6) and a prediction service in a very simple fashion and with
few lines of code. figure 3.7 shows a successful response of a prediction through
the bentoml rest api.
3.3
deployment of the architecture
the whole project is designed and implemented to run on google cloud platform
and it uses both open source software and tools from the ai platform by google.
since all the three open source tools used in this project (mlflow, kubeﬂow
and bentoml) are platform-agnostic they can be executed on any other cloud
platform.
google cloud platform is the high-performance infrastructure, by google, for
cloud computing, data analytics and machine learning. the technologies adopted
in this project, from google cloud platform, are:
• google cloud storage
• google cloud functions
3.3. deployment of the architecture
45
• google cloud build
• google container registry
• kubeﬂow (which, in google cloud platform, is called ai platform pipelines).
google cloud storage
google cloud storage is a service for storing objects
in google cloud platform. in google cloud storage the data are held by basic
containers, called “buckets”.
google cloud functions
cloud functions is a lightweight solution to create
single-purpose, stand-alone functions that respond to cloud events without the
need to manage a server or runtime environment; namely a tool for creating event-
driven applications according to the “function as a service” (faas) paradigm.
in this project a function is used to provide continuous training: whenever
a new ﬁle is uploaded to the bucket, where the training dataset is stored, the
function triggers a new run of the kubeﬂow (training) pipeline.
in practice,
inside the training pipeline code, a python function is deﬁned: this is the cloud
function to be triggered. this generic function logs relevant data when a ﬁle is
changed, compiles the training kubeﬂow pipeline and runs it.
google cloud build
google cloud build is the serverless ci/cd tool in google
cloud platform.
it allows to import sources from cloud source repositories,
figure 3.8: the sequence of the steps performed in the ci/cd ﬂow.
46chapter 3. design and implementation of a reference architecture for mlops
figure 3.9: runs on google cloud build, triggered by a commit or a merge in the
code repository.
github or bitbucket and, then, build each component according to a build conﬁg-
uration ﬁle (cloudbuild.yaml) and produce artefacts such as docker containers.
in this architecture, ci/cd is implemented through kubeﬂow and google
cloud build together (figure 3.8). each component of the kubeﬂow pipeline is
built by google cloud build, which is triggered by a speciﬁc event on the github
repository (e.g. a commit or a merge). when the event occurs, the kubeﬂow
pipeline is compiled and run. in such a way, whenever a piece of code changes in
the github repository, each component of the kubeﬂow pipeline is built up and
the kubeﬂow pipeline itself is automatically executed.
a very powerful feature, in google cloud build, is substitution. cloud build
allows you to use variables in the conﬁguration ﬁle and deﬁne their actual value be-
figure 3.10: example of substitution variables in google cloud build.
3.4. the training and the prediction pipelines in detail
47
fore each individual run; this characteristic is helpful for variables whose value isn’t
known until build time. these variables include $commit sha, $repo name,
$branch name, $tag name. other non-trigger-based variables are $project id
and $build id. in this project substitutions are used, speciﬁcally, to tag each
kubeﬂow run, triggered by google cloud build, with the corresponding git com-
mit hash. figure 3.10 shows an example of substitutions used in this project;
figure 3.9, instead, shows some successful run on google cloud build, triggered
as a consequence of a commit or a merge in the code repository.
google container registry
google container registry is a container image
registry that runs on google cloud platform; in this project, it is used to push
the docker images of each kubeﬂow component and pull them from the training
or prediction pipeline.
3.4
the training and the prediction pipelines in
detail
figure 3.11: the dag of the train-
ing pipeline on kubeﬂow, triggered
by google cloud functions as conse-
quence of a ﬁle upload (or update),
while running the model traininig
components.
figure 3.12: the dag of the predic-
tion pipeline on kubeﬂow while run-
ning both the load-data and remove-
header components.
48chapter 3. design and implementation of a reference architecture for mlops
listing 3.1: the deﬁnition of the data-ingestion kubeﬂow component
�
1
def __data_ingestion_step(bucket_name):
2
return kfp.dsl.containerop(
3
name=’data_ingestion ’,
4
image=os.environ[’
docker_container_registry_base_url ’] +
5
’/’ +
6
os.environ[’project_name ’] +
7
’/’ +
8
os.environ[’data_ingestion ’] +
9
’:’ +
10
os.environ[’tag’],
11
arguments =[’--bucket_name ’, bucket_name],
12
file_outputs ={’dataset_path ’:
13
’/tmp/dataset.csv’}
14
)

�
�
in the previous section a generic description of the training and the prediction
pipelines was provided; in this section will be explained, in more detail, the exe-
cution of the two pipelines. the ﬁrst one is aimed at build and train two diﬀerent
models; the second one, on the other hand, is aimed at building two diﬀerent
services that handle batch or real-time predictions. figure 3.11 and figure 3.12
shows the two running pipelines.
the training pipeline
the training pipeline (figure 3.2) builds and trains an
sgd regressor and a random forest regressor. the pipeline can be triggered
manually, according to a scheduled run, by google cloud build or by google cloud
function. when new data are available, within the bucket containing the dataset,
the pipeline is automatically triggered by google cloud function; figure 3.13
shows the logs of the function triggered as a consequence of a new ﬁle upload or
update and figure 3.11, instead, the running pipeline triggered by the function
itself. to run the pipeline, through google cloud functions, the latest version of
the docker container of each component is used. when the pipeline is triggered
by google cloud build, as a consequence of a commit or a merge in the code
repository, each component is built up in a new docker container (according to
the new code) and, then, the pipeline starts its run.
the pipeline is orchestrated through kubeﬂow. listing 3.1 shows how a kube-
ﬂow component is deﬁned (the data-ingestion component in the example): the
3.4. the training and the prediction pipelines in detail
49
listing 3.2: the kfp-cli command used to run programmatically the kubeﬂow
pipeline
�
1
kfp --endpoint $_endpoint run submit
2
-e ""${_pipeline_name}""
3
-r ${short_sha}
4
-p $(kfp --endpoint $_endpoint
pipeline
list | grep -w ""${_pipeline_name}""
|
5
grep -e -o -e ""([a-z0 -9]) {8} -([a-z0
-9]) {4} -([a-z0 -9]) {4} -([a-z0 -9])
{4} -([a-z0 -9]) {12}"")

�
�
function returns a containerop, which takes in input the name of the component,
the docker image to be retrieved to run the component and a list of arguments (ac-
cording to the type of argument both a simple parameter or a ﬁle can be passed);
the parameter “ﬁle outputs” deﬁnes the name of the artefacts produced in out-
put by the component itself, which will be passed to the next component of the
pipeline. to run the pipeline through google cloud build the kubeﬂow’s kfp-cli
command is employed in diﬀerent ways: listing 3.2 shows the command to run
the new pipeline on kubeﬂow.
the pipeline starts with the “data-ingestion” component; this component gets
each ﬁle, from the dataset bucket, and appends it to the others. the dataset,
merged in a single csv ﬁle, is passed to the “data-preparation” component which
performs feature engineering: time features (month, weekday and hour), national
holiday features and lag features (load data with a lag value ranging from 24 to 48
hours) are added, then one-hot encoders, on categorical features (time features),
and a standard scaler, on numerical features (lag features), are applied.
the dataset (including all the new features) is passed to both the model training
components: “linear-regression-training” and “random-forest-regressor-training”
in figure 3.2; the ﬁrst component trains an sgd regressor and, the second one,
a random forest regressor. the experiments performed on both the models are
tracked and versioned through mlflow (including the parameters, the metrics
and the artefacts); this approach allows to easily reproduce the experiments and
compare them. the built models are saved on the mlflow model registry, in
such a way that they can be easily loaded in the prediction pipeline. in both, the
model training components model validation is also performed.
the last component of the training pipeline, “promote” in figure 3.2, makes use
of conditions, a mechanism of kubeﬂow that allows choosing a path in the dag
according to speciﬁc circumstances; in the “promote” component a “condition”
50chapter 3. design and implementation of a reference architecture for mlops
figure 3.13: the logs of google cloud functions when a new ﬁle in the bucket is
uploaded (or updated).
compares the metrics of the two, trained, models and tags the best one as available
to be used in production.
the prediction pipeline
the prediction pipeline (figure 3.3) builds two diﬀer-
ent services to handle real-time and batch predictions; the pipeline can be triggered
manually, by a scheduled run or by google cloud build (as a consequence of a
commit or a merge).
thanks to the modular implementation of the training pipeline, the prediction
pipeline can reuse both “data-ingestion” and “data-preparation” components. in
this pipeline kubeﬂow is used as an orchestration tool as in the training pipeline;
this tool also allows to employ components designed and implemented by other
people: this kind of components is called “reusable components”; the “remove-
header” component, which is a “reusable component”, removes the header from
the dataset.
in a parallel path (cf. figure 3.12), the model to be used for the prediction
(the one with the tag “production”) is loaded through mlflow within the “load-
model” component. the prediction pipeline includes a service to perform real-
time predictions and another to perform batch predictions: “scikit-learn-inference-
service” is the component which produces an artefact, deployed by using bentoml,
to handle real-time predictions; “scikit-learn-batch-prediction”, instead, gets the
artefact produced by the “scikit-learn-inference-service” and employs it to perform
3.5. code publication
51
a batch prediction on the transformed dataset.
3.5
code publication
the code of the implementation, of the reference architecture, is available on
github [7] and is published as free software, under gplv3 licence. the repository
aims to provide the implementation of the architecture and a detailed explanation
of how the tools have been employed; the advantages on the machine learning
workﬂow have been also underlined within the project.
the doc directory contains deep documentation of the usage of each tool em-
ployed within the architecture. in the demo folder a concise explanation on how
to run the whole project is provided; in the same directory the dataset used in the
project can be also found. the components directory contains the code of each
component of both the two kubeﬂow pipelines; the folder of each component is
constituted of:
• a dockerﬁle, to build the corresponding docker image.
• a bash script, used to build the docker image of the component and push it
into the container registry.
• the src folder, which contains the code of the component.
• the requirements.txt ﬁle, used to install all the dependencies of the compo-
nent.
the prediction pipeline and training pipeline folders contain the code used to de-
ﬁne the two kubeﬂow pipelines. in both the directories a cloudbuild.yaml ﬁle is
also included: it deﬁnes the steps to be executed by google cloud build to build
all the components, compile the kubeﬂow pipeline and run it. prediction pipeline
training pipeline also contains a dockerﬁle: it is used to build a docker image
with kfp-cli, the kubeﬂow’s command line tool (which is employed in the cloud-
build.yaml ﬁle). the main.py ﬁle contains the implementation of the kubeﬂow
pipeline. furthermore, the .env.yaml ﬁle, used by google cloud function, contains
the environment variables needed to compile the kubeﬂow pipeline.
52chapter 3. design and implementation of a reference architecture for mlops
chapter 4
mlops use cases and scenarios
mlops is a very recent methodology, nevertheless it is rapidly taking hold in
business contexts thanks to the beneﬁts it has on putting a model into production;
as explained in chapter 1, mlops can help to signiﬁcantly reduce the time to
deploy models, allowing more ﬂexibility on updating them. in the following will
be discussed diﬀerent use cases in which mlops is currently used by diﬀerent
companies of diﬀerent business domains; a generic scenario related to the covid-
19 pandemic will be also provided. all the architectures exposed below share a
common trait: they are built up according to the speciﬁc needs of each company.
covid-19 pandemic
modern business applications leverage machine learning
and deep learning models to analyze real-world and large-scale data, to predict,
or to react intelligently to events; however, data change according to the environ-
ment and the events, causing concept drift. as already mentioned, concept drift
is a challenging problem in data science: it may happen due to changes in con-
sumer preferences, technological innovations, catastrophic events, etc. covid-19
pandemic is a very clear example of this problem; “the pandemic disrupted many
supply chains because demand planning models weren’t updated frequently enough
to account for the quickly emerging “new normal” as the pandemic itself began”
[24].
covid-19 caused a huge change in the data used to make predictions. the
unexpected ﬁrst lockdown, which has expanded becoming worldwide, day by day,
deeply impacted the sales of every kind of market and, consequently, on their data.
the covid-19 pandemic caused machine learning models across many industries
to go haywire because of rapidly changing conditions; moreover, due to the long
time to get a model in production, it was diﬃcult to overcome the concept drift
caused by the pandemic itself.
“investing in mlops allows organisations, and their machine learning solu-
tions in production, to be more resilient to external volatile events, like rapid mar-
53
54
chapter 4. mlops use cases and scenarios
ket landscape changes, regulatory changes, and other unforeseen external events
like the covid-19 pandemic” [40]. mlops is primarily aimed at reducing the time
to get a model in production as much as possible. one of the most important
features of mlops, to face concept drift, is continuous training, since it helps to
be fastly resilient to unexpected events which cause a change in the data. mlops
speeds up the development, deployment, and management of models, thus enabling
the creation of applications that can rapidly adapt to changes in the environment.
“using mlops automation, businesses can monitor and detect changes that impact
their ai models, make swift changes to their ai applications and get new solutions
to market faster and in a much more agile way” [41].
astrazeneca
astrazeneca, multinational pharmaceutical company, leverage ma-
chine learning and deep learning techniques to accelerate drugs discovery. its
platform, called augmented drug design, helps chemicals develop drugs faster by
using machine learning and other techniques, alongside the work performed in
chemical laboratories; this approach allows to save both a lot of time and money.
“mlops plays a key part in astrazeneca’s mission to reduce the research phase
of the drug discovery cycle by half, from 24 months to 12 months, by 2025” [37]
says adrian rossall, head of augmented drug design, at the seldon 1.0 launch
event.
mlops helps astrazeneca deploying models faster, monitoring and re-
training them. astrazeneca makes use of scientiﬁcally aware and industry-speciﬁc
tools, for this reason it needed a customizable solution; other requirements to build
an mlops architecture, for the company, were ﬂexibility and scalability. in 2019
astrazeneca started using seldon on top of kubernetes. seldon simpliﬁed and
enhanced model deployment, kubernetes allowed to scale up and down, granted
ﬂexibility and allowed the company to deploy models faster; “mlops really helped
to accelerate the whole pipeline”, says adrian rossal [37].
netﬂix
netﬂix makes extensive use of machine learning across a lot of areas
in their product and deploys thousands of models; this helps to personalize the
experience of the customers and the content necessary to oﬀer them an optimal ex-
perience. the business problem, for netﬂix, is to estimate the size of the audience,
every day, of a show in the months leading up to the show’s launch. this is impor-
tant for a variety of reasons, including prioritization, allocation of resources etc.
both accuracy and timeliness are crucial, because if predictions aren’t accurate
and fast then they’re not useful and, at the same time, they lose the opportunity
to make decisions based on them. a project typically starts with the exploration
of the data and looking for correlations among them; this phase can take between
two to four weeks. the next stage involves building and identifying the candidate
model to solve the business problem; this usually takes about six to eight weeks.
55
then the model needs to be shipped into production, which concerns diﬀerent
tasks and which requires from 12 to 14 weeks [8].
metaﬂow is the open source machine learning infrastructure, originally devel-
oped at netﬂix, to boost the productivity of data scientists. metaﬂow can help for
rapid prototyping and for a fast deployment of the models, reducing the time to get
a model in production to less than 12 weeks [8]. metaﬂow enables collaboration,
oﬀers ﬁrst-class support for prototyping and deployment, allows straightforward
scalability, provides speciﬁc data tooling and is designed to make operational is-
sues easy to diagnose and ﬁx [42]. metaﬂow can be thought of as a wrapper of
well-known tools: xgboost, pytorch, tensorﬂow as machine learning libraries,
pandas for feature engineering, jupyter as a collaborative tool; meson, a work-
ﬂow management tool, to provide job scheduling and task isolation with titus and
apache mesos, spark as query engine and amazon s3 as data lake. figure 4.1
shows the stack of the tools included in metaﬂow.
compared to the architecture described in chapter 3 metaﬂow is a general-
purpose platform; it makes use of airﬂow instead of kubeﬂow and it includes
query engine and data lake tools, missing in the architecture exposed in the pre-
vious chapter. moreover, metaﬂow does not include any tool aimed to track the
experiments for the model training.
figure 4.1: stack of the technologies included in metaﬂow [8].
bank ita´u unibanco
“ita´u unibanco is the largest private sector bank in brazil,
with a mission to put its customers at the centre of everything they do as a key
driver of success” [9]; to deal with this goal the bank built ita´u virtual assistant,
a digital customer service tool that uses natural language processing to under-
stand customer questions and respond in real-time. to help continually improve
and evolve ita´u virtual assistant the bank needed an eﬃcient strategy for the de-
ployment of machine learning models; hence the machine learning team designed
a ci/cd pipeline, based on kubeﬂow, on google cloud platform. “for the ita´u
56
chapter 4. mlops use cases and scenarios
virtual assistant project, two business requirements were essential: the ability to
have multiple models in production (whether using diﬀerent techniques or models
trained using distinct data), and the ability to retrain the production model with
new data” [9].
the architecture was designed by using open source tools, including kubeﬂow,
kubernetes, seldon core, docker, and git. the goal was to have a single overall
solution that could be deployed on google cloud platform or on-premises (for
example origin, the open source version of redhat openshift), according to the
needs and restrictions of each team inside the company.
figure 4.2 shows the
architecture built up by unibanco.
unibanco’s architecture is very similar to the architecture exposed in chapter 3:
it includes a ci/cd pipeline (which, instead of google cloud build, is implemented
through jenkins) and, as the architecture in the previous chapter, it implements
a model training pipeline by using kubeﬂow; seldon core is used for the model
serving instead of bentoml. despite these diﬀerences, the reference architecture
for this thesis would ﬁt unibanco’s requirements.
figure 4.2: ci/cd architecture of itau unibanco [9].
greensteam
greensteam is a company that provides software solutions for
optimizing vessel performance, to save fuel and reduce emissions. even though
greensteam has already built several machine learning products in the past,
which helped some major shipping companies make informed performance op-
timization decisions, in 2019 the need for a renewal of the process of building
machine learning models emerged: “we knew our machine learning operations
needed to grow with the company” [10]. for this reason, the company decided to
start from scratch and rethink its entire machine learning infrastructure.
the ﬁrst step involved switching from jupyter notebooks to python packages,
versioned on git repositories.
subsequently, the company faced reproducibility
57
figure 4.3: mlops tools stack used by greenstream [10].
issues; while the setup of the environments, in the diﬀerent laptops, was similar
it was never the same: greenstream started using conda, but it did not solve the
problem; “docker helped with the problem and it enabled greensteam to have a
uniﬁed setup” [10]. dealing with docker containers also helped greenstream to
build a continuous integration pipeline by using jenkins. through docker the
company paved the way to move from monoliths to microservices, orchestrated
by using argo (open source pipeline orchestrator). greenstream also needed a
custom model serving solution: sagemaker and kubeﬂow have been looked at as
solutions, but the tools did not meet the needs; fastapi turned out to be the
best tool for the company requirements. the last step of rethinking the machine
learning infrastructure, for greenstream, involved tracking the experiments, their
version and their metrics: neptune was used as a tool. figure 4.3 shows the full
technologies stack used by greenstream for its mlops infrastructure.
the architecture exposed in chapter 3 would probably ﬁt greenstream’s re-
quirements. the architecture built up by the company, however, diﬀers in the
employed tools: argo is used instead of kubeﬂow to orchestrate the pipelines,
aws is used as cloud platform, jenkins to implement ci/cd (instead of kube-
ﬂow and google cloud build) and neptune as experiment tracking tool, instead
of mlflow.
uber
uber employs machine learning to make data-driven decisions; it not only
enables services such as ridesharing (destination prediction, driver-rider pairing,
eta prediction, etc) but also ﬁnancial planning and other core business needs.
machine learning solutions are also implemented in some of uber’s other busi-
58
chapter 4. mlops use cases and scenarios
nesses such as ubereats, uberpool, and uber’s self-driving car division [43].
uber faced, in the past, diﬀerent challenges with building and deploying ma-
chine learning models: there were no systems to build reliable, uniform and re-
producible pipelines and there was neither a standard place to store the results of
training experiments and compare them together. “we were starting to see signs
of many of the machine learning ant-patterns documented by scully et al. [1]”
[11].
figure 4.4: architecture of uber michelangelo [11].
michelangelo is uber’s machine learning-as-a-service platform, which enables
internal teams to easily build, deploy and operate machine learning solutions at
scale; it is designed to cover the end-to-end workﬂow: manage data, train, eval-
uate and deploy models, make predictions and monitor the models themselves.
michelangelo is designed to address the gaps that emerged, by standardizing the
workﬂows and tools. figure 4.4 shows both the online and the oﬄine architecture
of michelangelo. the platform consists of multiple open source tools and diﬀer-
ent built in-house components; the primary open source components are hdfs,
spark, samza, cassandra, mllib, xgboost and tensorflow; to manage the re-
sources both yarn and mesos can be used. michelangelo is designed to provide
scalable, reliable, reproducible easy to use tools to address an end-to-end workﬂow.
michelangelo is the ﬁrst architecture that includes a centralized feature store; it
allows teams to create and manage canonical features to be used and shared.
with respect to the architecture exposed in chapter 3 michelangelo is a general-
59
purpose platform that intersects both mlops and dataops (cf. section 1.1.2); it
also does not explicitly include an experiment tracking tool, a pipeline orchestrator
and a ci/cd tool. due to these diﬀerences, the infrastructure presented in the
previous chapter would probably not be suitable for uber’s requirements.
figure 4.5: mlops architecture of h&m [12].
h&m
“we have a saying: ‘we don’t care how good your model is, if it’s not
in production and delivering value then it’s not worth anything’” [37] says errol
koolmeister, head of ai foundation at h&m group. h&m, in its ai department,
has many diﬀerent teams and counts hundreds of models across the entire h&m
value chain. each team solves diﬀerent business problems, potentially using dis-
tinct techniques, however the process is very similar: there is a machine learning
pipeline, a model deploying pipeline and monitoring infrastructure. kevan wang,
ai architect in h&m group, underlines the complexity of mlops and the impor-
tance to ﬁrst think about what kind of problem to address, what kind of process
is involved and which kind of skills a team owns [12].
in december 2020 h&m started building its own and centralized ai platform,
based on mlops principles. figure 4.5 shows the technologies stack for the mlops
architecture of h&m, which is very similar to the reference architecture of this the-
sis, exposed in chapter 3; the main diﬀerence is that h&m’s implementation is
a general-purpose architecture, in which the model training pipeline can be han-
60
chapter 4. mlops use cases and scenarios
dled through databricks, airﬂow or kubeﬂow, depending on the speciﬁc product
lifecycle (instead of kubeﬂow only): “airﬂow and kubeﬂow are very similar but,
if starting from scratch, i’d probably suggest kubeﬂow” [12], says kevan wang.
ci/cd is implemented, by h&m, by using azure native service as opposed to
the reference architecture for this thesis, which adopts kubeﬂow and, serving, is
provided through seldon core instead of bentoml. h&m additionally includes
monitoring and system availability, handled through azure and two open source
tools: graphana and prometheus. regarding the model management h&m makes
the same choice as the architecture in chapter 3, by considering mlflow as the
most mature solution today on the market.
chapter 5
conclusions
mlops is a very recent approach aimed at simplifying the workﬂow and reduc-
ing the time to get models in production. the goal of this thesis was to provide
a deep study of this new methodology. accordant with the analysis of the fea-
tures of mlops, it can be concluded that the machine learning workﬂow can be
signiﬁcantly simpliﬁed and, the process to get a model in production, can be accel-
erated thanks to this approach; this goal can be achieved by adopting techniques as
ci/cd, continuous training and monitoring, automation, ensuring repeatability,
by designing the machine learning system in a modular fashion and by applying
all the other features of mlops explained in chapter 1. this thesis is the result
of the internship at data reply and it has also the objective of providing a deep
study on the plethora of tools to build an mlops architecture; the analysis on the
diﬀerent tools was conducted by focusing on the open source ones, as requested
by the company, in such a way as to be platform-agnostic respect to the cloud
platform used to deploy the architecture. chapter 2 oﬀers an in-depth analysis on
the main open source tools in the market and on their maturity level. according
to this deep exploration of the open source tools, it can be concluded that some
of them can be considered the best choice in the market and “standard-de-facto”
among the mlops technologies. in addition to a deep study on mlops’ features
and a deep analysis on its main technologies, the other main contribution was
to implement an mlops architecture, designed by using some of those tools and
deployed on google cloud platform. due to google cloud platform restrictions
seldon core, the main serving open source tool in the market, cannot be em-
ployed; future works concern implementing serving features by seldon core itself
and including in the reference architecture monitoring tools (for example grafana).
this thesis can be a starting point to explore mlops both theoretically and
practically (by relying on the implemented reference architecture and its code, pub-
lished as free software with gplv3 licence). nevertheless the tools mentioned in
chapter 2 can be considered the best choice in the market, among the mlops
61
62
chapter 5. conclusions
technologies, they are very recent and some of them are still immature. during
the implementation of the reference architecture, i ran into diﬀerent lacuna both
using mlflow and kubeﬂow. the full mlflow api is available only in python
and the java and r api are still in development; other shortcomings i run into,
while using mlflow, were related to the xgboost package (which didn’t work as
expected) and the lack of a method to retrieve the version of a model (information
required by other functions within the library). related to kubeﬂow, while using
the kfp-cli tool (to dynamically submit and run a pipeline), many exceptions were
not handled and it was very diﬃcult to ﬁnd the root of the problems. therefore,
even if both mlflow and kubeﬂow are very eﬀective for generic usage, they re-
quire an improvement for deep and detailed usage. furthermore, as mentioned in
chapter 2, a common trait of the exposed tools is a lack in the documentation:
this needs to be improved in such a way as to make each tool easier to under-
stand. mlops is rapidly evolving, maturing and is a topic with great potential
in the market: it will grow more and more in the near future. “mlops market
is expected to expand to nearly us$4 billion by 2025” [23]. moreover, mlops
will impact the machine learning processes in the same way devops had a wide
impact on software development in the past. for these reasons many companies
are starting to adopting this methodology and it will be charming to have the
opportunity to work dealing with mlops in the future; some use cases of compa-
nies, which recently started employing mlops, are provided in chapter 4. based
on the study conducted with this thesis, companies dealing with machine learn-
ing should consider adopting mlops (but it is crucial doing it according to their
speciﬁc requirements, as suggested in chapter 3).
63
64
chapter 5. conclusions
glossary
a
a/b tests a/b testing is a way to compare the two versions of a variable to ﬁnd
out which performs better in a production environment. 35, 63
c
concept drift concept drift refers to the phenomenon that happens when the
statistical properties of the target variable change. 15, 17, 53, 54, 63
containerization containerization is an increasingly popular solution to deal
with dependencies when deploying a machine learning model. 8, 19, 63
d
dag a dag is a collection of all the tasks to be run, organized in a way that
reﬂects their relationships and dependencies. ix, x, 23, 30, 31, 40, 41, 47, 49,
63
data drift data drift is an unexpected and unplanned change in the distribution
of the data used in a predictive task. data drift happens when statistical
properties of the predictors change. 15, 17, 63
design pattern a design pattern is a solution for a problem which occurs over
and over again, in such way the solution itself can be adopted many times.
22, 26, 63
domain speciﬁc language “dsls are small languages, focused on a particular
aspect of a software system.” [44]. 31, 63
e
ensembled model ensemble modeling is a process where multiple diverse base
models are used to predict an outcome. 13, 63
65
66
glossary
f
feature a feature is a measurable property of a phenomena under observation
and (part of) an input to a machine learning model (e.g. a raw word, a
pixel, a sound wave, an aggregate, a time window, etc.) . 34, 63
feature store a feature store is a new layer of abstraction aimed to reduce
the time that data scientists spend on getting the data into a format they
can use to train models and maximize the amount of time they actually do
data science. though it is a repository of diﬀerent features associated with
business entities that are created and stored in a central location for easier
reuse. 7, 21, 34, 58, 63, 66
function as a service “faas (function-as-a-service) is a type of cloud-computing
service that allows you to execute code in response to events without the com-
plex infrastructure typically associated with building and launching microser-
vices applications. serverless and functions-as-a-service are often conﬂated
with one another but the truth is that faas is actually a subset of serverless.”
[45]. 45, 63
m
metric a number that you care about. may or may not be directly optimized in
a machine-learning system. a metric that your system tries to optimize is
called an objective. 8, 9, 15, 17, 18, 21, 29, 42, 49, 50, 63
model a machine learning model is a mathematical function that relates an
input to an output. to do that mapping machine learning relies on pa-
rameters. it is the representation of what a machine learning system has
learned from the training data, based on statistical theory. 3, 8, 21, 29, 35,
36, 56, 63
model store it is a tool for versioning, exporting, and storing machine learning
models that allows to collaboratively manage the full lifecycle. 27, 28, 63
multi-armed bandits technique (and area of study) to deal with the problem
of deciding how to route requests to competing machine learning model and
determines which model is the best in the shortest amount of time can be
treated. 35, 63
o
glossary
67
openapi “the openapi speciﬁcation (oas) deﬁnes a standard, language-
agnostic interface to restful apis which allows both humans and computers
to discover and understand the capabilities of the service without access to
source code, documentation, or through network traﬃc inspection.” . 44, 63
p
parameter a parameter is a real valued variable that changes during the model
training. a special kind of parameter is an hyper-parameter: it is set before
the training and it does not change (until the next epoch or retraining). 15,
17, 21, 27, 29, 30, 32, 42, 49, 63, 66
s
serving the process of taking some sort of trained machine learning model and
make its predictions available for its users. 28, 29, 35, 36, 56, 57, 60, 63
stale model the model is deﬁned as stale if the trained model does not include
up-to-date data and/or does not satisfy the business impact requirements.
18, 63
t
technical debt technical debt is a term related to immature, incomplete or
inadequate code (due to design deﬁciencies, low quality or other problems
on the software), which will require additional work to be ﬁxed.
it is a
metaphor linked to ﬁnance: having tecnical debts on a software, in economy
terms, would be like paying interest on a loan. further, technical debt, is
related to “deﬁciencies in internal quality that make it harder than it would
ideally be to modify and extend the system [...]” [46]. 3, 12, 13, 63
68
glossary
bibliography
[1] d. sculley, gary holt, daniel golovin, eugene davydov, todd phillips, di-
etmar ebner, vinay chaudhary, michael young, jean-francois crespo, and
dan dennison.
hidden technical debt in machine learning systems.
in
proceedings of the 28th international conference on neural information pro-
cessing systems - volume 2, nips’15, page 2503–2511, cambridge, ma, usa,
2015. mit press.
[2] data
science
project
lifecycle.
https://coursebricks.com/
blog-data-science-project-lifecycle/.
[3] survey - kubeﬂow continues to move into production.
https://blog.
kubeflow.org/kubeflow-continues-to-move-to-production.
[4] google. mlops: continuous delivery and automation pipelines in machine
learning).
https://cloud.google.com/solutions/machine-learning/
mlops-continuous-delivery-and-automation-pipelines-in-machine-learning.
[5] theoﬁlos kakantousis, antonios kouzoupis, fabio buso, gautier berthou,
jim dowling, and seif haridi.
horizontally scalable ml pipelines with a
feature store.
[6] featurestore.org. https://www.featurestore.org/.
[7] enrico salvucci. the reference mlops architecture. https://github.com/
esalvucci/thesis-mlops-reference-architecture.
[8] julie pitt and ashish rastogi. netﬂix presents: a human friendly approach
to mlops — netﬂix. https://www.youtube.com/watch?v=foszuonmlba.
[9] ita´u
unibanco:
how
we
built
a
ci/cd
pipeline
for
ma-
chine
learning
with
online
training
in
kubeﬂow.
https:
//cloud.google.com/blog/products/ai-machine-learning/
\itau-unibanco-how-we-built-a-cicd-pipeline-for-\
machine-learning-with-online-training-in-kubeflow.
69
70
bibliography
[10] mlops
at
greensteam:
shipping
machine
learn-
ing
[case
study].
https://neptune.ai/blog/
mlops-at-greensteam-shipping-machine-learning-case-study.
[11] meet michelangelo: uber’s machine learning platform. https://eng.uber.
com/michelangelo-machine-learning-platform/.
[12] keven wang. apply mlops at scale by h&m. https://databricks.com/
session_eu20/apply-mlops-at-scale.
[13] danilo sato, arif wider, and christoph windheuser. continuous delivery for
machine learning. https://martinfowler.com/articles/cd4ml.html.
[14] srecon19 asia/paciﬁc - what is ml ops solutions and best practices.
https://www.youtube.com/watch?v=algxalx46f8&t=156s.
[15] agile manifesto. http://agilemanifesto.org.
[16] continuous delivery. https://continuousdelivery.com/.
[17] what the ops are you talking about? https://towardsdatascience.com/
what-the-ops-are-you-talking-about-518b1b1a2694.
[18] dataops manifesto. https://www.dataopsmanifesto.org/.
[19] m. treveil, n. omont, c. stenac, k. lefevre, d. phan, j. zentici, a. lavoil-
lotte, m. miyazaki, and l. heidmann. introducing mlops. o’reilly media,
2020.
[20] why
is
there
no
devops
manifesto?
https://devops.com/
no-devops-manifesto/.
[21] damian a tamburri. sustainable mlops: trends and challenges. in 2020
22nd international symposium on symbolic and numeric algorithms for sci-
entiﬁc computing (synasc), page 1. ieee, 2020.
[22] ml
model
management
and
operations
2020
(“mlops”).
https://www.cognilytica.com/2020/03/03/
ml-model-management-and-operations-2020-mlops/.
[23] infographic: the rapid growth of mlops. https://www.cognilytica.com/
2020/04/02/infographic-the-rapid-growth-of-mlops/.
[24] mlops
optimizes
development,
deployment,
and
management.
https://www2.deloitte.com/us/en/insights/focus/tech-trends/
2021/mlops-industrialized-ai.html.
bibliography
71
[25] yue zhou, yue yu, and bo ding. towards mlops: a case study of ml
pipeline platform. in 2020 international conference on artiﬁcial intelligence
and computer engineering (icaice). ieee, 2020.
[26] kaz sato. what is mlops? best practices for devops for ml (cloud next
’18). https://www.youtube.com/watch?v=_jnhxzy1hcw.
[27] rules
of
machine
learning.
https://developers.google.com/
machine-learning/guides/rules-of-ml.
[28] jez humble and david farley. continuous delivery: reliable software releases
through build, test, and deployment automation. pearson education, 2010.
[29] ml
ops.org.
mlops
principles.
https://ml-ops.org/content/
mlops-principles.
[30] valliappa lakshmanan, sara robinson, and michael munn. machine learning
design patterns. ”o’reilly media, inc.”, 2020.
[31] setting up an mlops environment on google cloud.
https://cloud.
google.com/architecture/setting-up-an-mlops-environment.
[32] aws
mlops
framework.
https://aws.amazon.com/solutions/
implementations/aws-mlops-framework/,
https://docs.aws.amazon.
com/sagemaker/latest/dg/sagemaker-projects-why.html.
[33] azure
mlops
framework.
https://docs.microsoft.com/en-en/
azure/architecture/reference-architectures/ai/mlops-python,
https://docs.microsoft.com/en-us/azure/machine-learning/
concept-model-management-and-deployment.
[34] clive cox, dan sun, ellis tarn, animesh singh, and david goodwin. server-
less inferencing on kubernetes. arxiv preprint arxiv:2007.07366, 2020.
[35] sridhar alla and suman kalyan adari. introduction to mlflow. in beginning
mlops with mlflow, page 125. springer, 2021.
[36] kfserving documentation. https://www.kubeflow.org/docs/components/
kfserving/kfserving/.
[37] seldon
deploy
1.0
launch
event.
https://www.seldon.io/
seldon-deploy-1-0-launch-event/.
[38] sasu m¨akinen et al. designing an open-source cloud-native mlops pipeline.
university of helsinki, faculty of science, 2021.
72
bibliography
[39] forecasting
hourly
electricity
consumption
of
ger-
many.
https://www.kaggle.com/francoisraucent/
forecasting-electricity-consumption-of-germany.
[40] how
mlops
helps
keep
machine
learning
solutions
relevant
dur-
ing
challenging
times.
https://medium.com/datasparq-technology/
how-mlops-helps-keep-machine-learning-solutions-relevant-during-challenging-times-8e12a609f1ec.
[41] concept
drift
and
the
impact
of
covid-19
on
data
science.
https://www.iguazio.com/blog/
concept-drift-and-the-impact-of-covid-19-on-data-science/.
[42] metaﬂow
doc.
https://docs.metaflow.org/introduction/
what-is-metaflow.
[43] how these 8 companies implement mlops – in-depth guide. https://
neptune.ai/blog/how-these-8-companies-implement-mlops.
[44] martin fowler. domain-speciﬁc languages. pearson education, 2010.
[45] what is faas (function-as-a-service)? https://www.ibm.com/cloud/learn/
faas.
[46] technical debt. https://martinfowler.com/bliki/technicaldebt.html.
"
The%20AI%20Architecture%20of%20Versu.pdf;"
references
1. blackburn, p., de rijke, m., venema, y.: modal logic. cambridge university press
(2002)
2. brandom, r.: making it explicit. harvard university press (1998)
3. drefyus, h.: being-in-the-world. the mit press (1990)
4. ekman, p.: basic emotions (1990)
5. evans, r.: introducing exclusion logic as a deontic logic. deontic logic in com-
puter science. springer (2010)
6. evans, r.: representing personality traits as conditionals. proceedings of aisb
(artiﬁcial intelligence and the simulation of behaviour) pp. 64-82 (2008)
7. mateas, m.: interactive drama, art, and artiﬁcial intelligence. ph.d. thesis. school
of computer science, carnegie mellon university (2002)
8. mateas, m., stern, a.: writing fa¸cade: a case study in procedural authorship. in
second person ed. harrigan and fruin (2007)
9. knuth, d.: digital searching. the art of computer programming volume 3: sorting
and searching (2nd ed.). addison-wesley (1997)
10. lewis, d.: scorekeeping in a language game. journal of philosophical logic p.379
(1979)
11. maes, p.: how to do the right thing. connection science journal (1989)
12. mccarthy, j., hayes, p.: some philosophical problems from the standpoint of
artiﬁcial intelligence. machine intelligence 4 (1969)
13. mccoy, j., mateas, m., wardrip-fruin, n.: comme il faut: a system for simulat-
ing social games between autonomous characters. proceedings of the 8th digital
art and culture conference, pp.1-8 (2009)
14. mcdermott, d.: pddl - the planning domain deﬁnition language (version 1.2).
yale center for computational vision and control (1998)
15. meehan, j.r.: tale-spin, an interactive program that writes stories. proceedings
of the fifth ijcai (1977)
16. moses, y., tenenholtz, m.: on computational aspects of artiﬁcial social systems.
proc 11th dai workshop pp. 108 - 131 (1992)
17. nau, d., cao, y., lotem, a., and muoz-avila, h.: ”shop: simple hierarchical
ordered planner.” ijcai-99, pp. 968-973 (1999)
18. nelson,
g.,
short,
e.:
the
inform
7
manual.
http://inform7.com/learn/man/index.html
19. orkin, j.: three states and a plan: the ai of fear.
20. rawls, j.: two concepts of rules. philosophical review lxiv pp. 3 - 32 (1955)
21. riedl, m., bulitko, v.: interactive narrative: an intelligent systems approach. ai
magazine vol. 34(1) (2013)
22. roberts, d. l., and isbell, c. l.: a survey and qualitative analysis of recent ad-
vances in drama management. international transactions on systems science and
applications, special issue on agent based systems for human learning, pp.179 -
204 (2008)
23. rosenblatt, j.: maximising expected utility for behaviour arbitration. australian
joint conference on artiﬁcial intelligence (1996)
24. sacks, h.: lectures on conversation. kluwer (1989)
25. sacks, h., schlegoﬀ, e., jeﬀerson, g.: a simplest systematics for the organization
of turn-taking for conversation. language, vol.50 (1974)
26. salen, k., zimmerman, e.: rules of play. mit press (2003)
the ai architecture of versu
37
27. schank, r., abelson, r.: scripts, plans, goals and understanding. artiﬁcial intel-
ligence series (1977)
28. schank, r., abelson, r.: scripts, plans, and knowledge. ijcai (1975)
29. schatzki, t, r.: social practices: a wittgensteinian approach to human activity
and the social. cambridge university press (1996)
30. short, e.: npc conversation systems. if theory reader. transcript on press.
pp.331 - 359 (2011)
31. somogyi, z., henderson, f., conway, t.: the execution algorithm of mercury: an
eﬃcient purely declarative logic programming language. journal of logic program-
ming, vol.29 (1996)
32. strachey, c.: fundamental concepts in programming languages. higher-order
and symbolic computation 13: 11-49 (2000)
33. tomasello, m: origins of human communication. mit press (2008)
34. wardrip-fruin, n.: expressive processing. mit press (2007)
"
White_Paper_BWL_DL_Leyer_Vol3_Nr6_DE.pdf;
WS5Paper10.pdf;
XuRunyu_Summer2020.pdf;
